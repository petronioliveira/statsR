[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestatística Usando o R",
    "section": "",
    "text": "Prefácio",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "index.html#bem-vindos-ao-bioestatística-usando-o-r",
    "href": "index.html#bem-vindos-ao-bioestatística-usando-o-r",
    "title": "Bioestatística Usando o R",
    "section": "Bem-vindos ao Bioestatística usando o R",
    "text": "Bem-vindos ao Bioestatística usando o R\n\n\n\n\n\n\nDica\n\n\n\nBioestatística Usando o R é uma jornada completa pela estatística aplicada à saúde, com foco prático no uso do R e RStudio. Esta segunda edição foi revisada, reorganizada em partes temáticas e enriquecida com novos recursos visuais e pedagógicos.\n\n\nEste livro foi escrito para estudantes, pesquisadores e profissionais da área biomédica que desejam compreender e aplicar os conceitos estatísticos com o suporte de uma das ferramentas mais poderosas e acessíveis: o R. Bioestatística Usando o R tem a ambição de ser um pouco mais tolerável e amistoso, a fim de estimular o estudo da Bioestatística e, assim, facilitar a crítica da literatura científica e , desta forma, ajudar a evitar a intoxicação causada pela pseudociência.\nVocê encontrará aqui:\n\nFundamentos teóricos da bioestatística\nAplicações práticas com a linguagem R\nVisualizações elegantes com ggplot2\nTestes estatísticos, regressões, ANOVA e muito mais\nAplicações em epidemiologia e saúde pública\n\n\nSobre o Autor\nPetrônio Fagundes de Oliveira Filho nasceu em 4 de outubro de 1947, em Porto Alegre, Rio Grande do Sul, Brasil. Cursou o Ensino Médio no Colégio do Rosário, em Porto Alegre. Graduou-se em Medicina pela Universidade de Caxias do Sul (UCS) em 1973, realizou residência em Pediatria no Hospital da Criança Conceição (Porto Alegre) em 1975, e concluiu o mestrado em Saúde Pública Materno-Infantil pela Universidade de São Paulo em 1998.\n\n\n\n\n\n\n\n\n\n\n\nEm 1980, obteve o Título de Especialista em Pediatria (TEP) e, em 2009, o título de especialista em Estatística Aplicada pela UCS. Atuou como pediatra no INAMPS até 2002 e mantém consultório privado até hoje. Aposentou-se como professor da Universidade de Caxias do Sul em 2019, após uma trajetória iniciada em 1975. Na UCS, lecionou nas áreas de Pediatria, Epidemiologia e Bioestatística, foi coordenador do Serviço e da Residência Médica em Pediatria, chefe de Departamento, coordenador do curso de Medicina e diretor de Ensino do Hospital Geral de Caxias do Sul — hospital de ensino da universidade. Também integrou o Comitê de Ética em Pesquisa da UCS, vinculado à CONEP (Conselho Nacional de Ética em Pesquisa).\nPor mais de 20 anos, fez parte do Núcleo de Consultoria e Epidemiologia do Centro de Ciências da Saúde da UCS. É autor de dois livros: Epidemiologia e Bioestatística: Fundamentos para a leitura crítica (Editora Rubio, 2015/2018; 2ª edição em 2022) e SPSS – Análise de Dados Biomédicos, em coautoria com Valter Motta (MedBook, 2009). Participou ainda de dezenas de publicações científicas e capítulos de livros.\nDesde 1976, é casado com Lena Maria Cantergiani Fagundes de Oliveira. Tem duas filhas, Nathalia e Andressa, e dois netos encantadores e inteligentes: Gabriel e Felix. Ah, e havia também Floquinho — um cão da raça Shih Tzu, branco com manchas marrom claro e pretas — que acompanhava seus estudos e análises estatísticas. Floquinho latia toda vez que ouvia o nome de Ronald Fisher. Faleceu em 2024, aos 17 anos, deixando um grande vazio e muitas lembranças.\n\nemail: petronioliveira@gmail.com\nWhatsApp: +55 54 9997 15499",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Importância da Bioestatística\nOs indivíduos variam em relação as suas características biológicas, psicológicas e sociais na saúde e na doença. Esta variabilidade gera uma grande quantidade de incertezas.\nA Bioestatística, estatística aplicada às ciências biológicas e da saúde, é a ferramenta utilizada pelos pesquisadores para trabalhar com essas incertezas advindas da variabilidade. Várias definições foram escritas para a estatística, uma delas é a seguinte (Armitage, Berry, e Matthews 2008):\nA Bioestatística lida com a variabilidade humana utilizando técnicas estatísticas quantitativas (Massad et al. 2004) que ajudam a diminuir a ignorância em relação a esta diversidade. A compreensão da variabilidade humana torna a medicina mais ciência, diminuindo as incertezas, na tentativa de verificar se os resultados encontrados de fato existem ou são apenas obra do acaso.\nNa década de 1990, houve um acesso maior aos computadores. Os profissionais da saúde não estatísticos passaram a ter mais interesse no campo da bioestatística. Isto gerou uma onda que facilitou o aparecimento de novas ferramentas estatísticas de ponta. Apesar disso, o conhecimento da Bioestatística permanece restrito aos especialistas na área.\nNos últimos anos, os pacotes de softwares foram aprimorados, tornando-se mais amigáveis e diminuindo significativamente o pânico ao se defrontar com uma série de números uma vez que a maioria deles exige apenas conhecimento básico de matemática.\nPara a tomada de decisão em saúde é fundamental o acúmulo de conhecimento adquirido através da prática clínica, geradora da experiência do profissional, do intercâmbio com os pares e da análise adequada das evidências científicas publicadas em periódicos de qualidade. Para atingir este objetivo, é fundamental o conhecimento de bioestatística, incluindo aqui que o pensamento que deve nortear os profissionais da saúde ao lidar com o ser humano é o pensamento probabilístico.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#importância-da-bioestatística",
    "href": "intro.html#importância-da-bioestatística",
    "title": "1  Introdução",
    "section": "",
    "text": "Estatística é a disciplina interessada com o tratamento dos dados numéricos obtidos a partir de grupos de indivíduos",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-historia",
    "href": "intro.html#sec-historia",
    "title": "1  Introdução",
    "section": "1.2 Pílulas históricas da Estatística",
    "text": "1.2 Pílulas históricas da Estatística\n\n A história deve começar em algum lugar, mas a história não tem começo Kendall (1960) \n\nEntretanto, é natural, que se trace as raízes voltando ao passado, tanto quanto possível. Alguns referem-se à curiosidade em relação ao registro de dados à dinastia Shank, na China, possivelmente no século XIII a.c, com a realização de censos populacionais. Há relatos bíblicos de possíveis censos realizados por Moisés (1491 a.C.) e por Davi (1017 a.C.).\nOs romanos e os gregos já realizavam censos por volta do século VIII a IV a.C. Em 578-534 a.C., o imperador Servo Túlio mandou realizar um censo de população masculina adulta e suas propriedades que serviu para estabelecer o recrutamento para o exército, para o exercício dos direitos políticos e para o pagamento de impostos. Os romanos fizeram 72 censos entre 555 a.C. e 72 d.C. A punição para quem não respondia, geralmente era a morte! Na Idade Média, na Europa, existem registros de diversos censos: durante o domínio muçulmano, na Península Ibérica, nos séculos VII a XV; no reinado de Carlos Magno (712-814) e ainda o maior registro estatístico feito na época, o Domesday Book (Figura 1.1), realizado na Inglaterra, por Guilherme I (Kendall 1960) , o Conquistador, onde registravam nascimentos, mortes, batismos e casamentos. Houve, também, recenseamentos nas repúblicas italianas no século XII ao XIII («Breve História dos Censos» 2014).\n\n\n\n\n\n\n\n\nFigura 1.1: Domesday Book\n\n\n\n\n\nJohn Graunt (24/04/1620 - 18/04/1674) foi um cientista britânico a quem se deve vários estudos demográficos ingleses. Foi o precursor da construção de Tábuas de Mortalidade. Realizou estudos com William Petty (1623 - 1687), economista britânico que propôs a aritmética política.\nEm 1791, Sir John Sinclair (1754 - 1835) concebeu um plano de uma pesquisa empírica na Escócia para fornecer informações estatísticas. Foi a primeira vez que o termo estatística foi usado em inglês.\nGirolamo Cardano (24/09/1501 - 21/09/1576) foi um médico, matemático, físico e filósofo italiano. É tido como o primeiro a introduzir ideias gerais da teoria das equações algébricas e as primeiras regras da probabilidade, descritas no livro Liber de Ludo Aleae, publicado em 1663. Descreveu pela primeira vez a clínica da febre tifoide. Foi amigo de Leonardo da Vinci.\nPierre-Simon Laplace, Marquês Laplace (23/03/1749 - 05/03/1927) foi um matemático, astrônomo e físico francês. Embora conduzisse pesquisas substanciais sobre física, outro tema principal dos esforços de sua vida foi a teoria das probabilidades. Em seu Essai philosophique sur les probabilités, Laplace projetou um sistema matemático de raciocínio indutivo baseado em probabilidades, que hoje coincidem com as ideias bayesianas.\nAntoine Gombaud, conhecido como Chevalier de Méré (1607 - 1684) foi um nobre e jogador. Como não tinha mais sucesso nos jogos de azar, buscou ajuda de Blaise Pascal (19/06/1623 – 19/08/1662), matemático, físico francês, que se correspondeu com Pierre Fermat (matemático e cientista francês), nascendo desta colaboração a teoria matemática das probabilidades (1812). Blaise Pascal foi mais tarde chamado de o Pai da Teoria das Probabilidades.\nA moderna teoria das probabilidades foi atribuída a Abraham De Moivre (25/05/1667 – 27/11/1754), matemático francês, que adquiriu fama por seus estudos na trigonometria, teoria das probabilidades e pela equação da curva normal. Em 1742, Thomas Bayes (1701 – 07/04/1761, matemático e pastor presbiteriano, inglês, desenvolveu o Teorema de Bayes que descreve a probabilidade de um evento ocorrer, baseado em um conhecimento a priori.\nAdrien-Marie Legendre (18/09/1752 - 10/01/1833) foi um matemático francês. Em 1783, tornou-se membro adjunto da Academie des Sciences, instituição que esteve na vanguarda dos desenvolvimentos científicos dos séculos XVII e XVIII. Fez importantes contribuições à estatística, à teoria dos números e à álgebra abstrata.\nJohann Carl Friedrich Gauss (30/04/1777 - 23/02/1855) foi um matemático, astrônomo e físico alemão (Figura 1.2) que contribuiu em diversas áreas das ciências como teoria dos números, estatística, geometria diferencial, eletrostática, astronomia e ótica. Muitos referem-se a ele como o Príncipe da Matemática, o mais notável dos matemáticos. Descobriu o método dos mínimos quadrados e a lei de Gauss da distribuição normal de erros e sua curva em formato de sino, hoje tão familiar para todos que trabalham com estatística.\n\n\n\n\n\n\n\n\nFigura 1.2: Johann Carl Friedrich Gauss\n\n\n\n\n\nLambert Adolphe Jacques Quételet (22/02/1796 - 17/02/1874) foi um astrônomo, matemático, demógrafo e estatístico francês. Seu trabalho se concentrou em estatística social, criando regras de determinação de propensão ao crime\nFrancis Galton (16/02/1822 – 17/01/1911) foi um antropólogo, matemático e estatístico inglês. Entre muitos artigos e livros, criou o conceito estatístico de correlação e da regressão à média. Ele foi o primeiro a aplicar métodos estatísticos para o estudo das diferenças e herança humanas de inteligência. Criou o conceito de eugenia e afirmava que era possível a melhoria da espécie por seleção artificial. Acreditava que a raça humana poderia ser melhorada caso fossem evitados relacionamentos indesejáveis. Isto acompanhava o pensamento burguês europeu da época. Criou a psicometria, onde desenvolveu testes de inteligência para selecionar homens e mulheres brilhantes. Esta teoria teve papel importante na formação do fascismo e nazismo (Salgado-Neto e Salgado 2011).\nWilliam Farr (30/11/1807 - 14/04/1883) foi um médico sanitarista e estatístico inglês, nascido na vila de Kenley, Shropshire. Foi o primeiro investigador a examinar séries temporais de morbimortalidade para longos períodos e, assim, considerado o criador da Estatística da Saúde Pública Moderna. Seus relatórios foram fundamentais para o desencadeamento das reformas sanitárias britânicas, em meados e final do século XIX (Stolley e Lasky 2000b).\nFlorence Nightingale (12/05/1820 – 13/08/1910) foi uma enfermeira (Figura 1.3) que ficou famosa por ser pioneira no tratamento de feridos, durante a Guerra da Criméia (Editors, sem data). Ficou conhecida na história pelo apelido de “A dama da lâmpada”, pelo fato de servir-se de uma lamparina para auxiliar no cuidado aos feridos durante a noite. Também contribuiu no campo da Estatística, sendo pioneira na utilização de métodos de representação visual de informações, como por exemplo gráfico de setores (habitualmente conhecido como gráfico do tipo “pizza”)\n\n\n\n\n\n\n\n\nFigura 1.3: Florence Nightingale\n\n\n\n\n\nJohn Snow (York, 15/03/1813 - Londres, 15/03/1858) foi um médico inglês (Figura 1.4)), considerado pai da Epidemiologia Moderna. Recebeu, em 1853, o título de Sir após ter anestesiado a rainha Vitória no parto sem dor de seu oitavo filho, Leopoldo de Albany. Este fato ajudou a divulgar a técnica entre os médicos da época. Demonstrou que a cólera era causada pelo consumo de águas contaminadas com matérias fecais, ao comprovar que os casos dessa doença se agrupavam em determinados locais da cidade de Londres, em 1854, onde havia fontes dessas águas (Stolley e Lasky 2000b).\n\n\n\n\n\n\n\n\nFigura 1.4: John Snow\n\n\n\n\n\nKarl Pearson (27/03/1857 - 27/04/1936) foi um importante estatístico inglês, fundador do Departamento de Estatística Aplicada da University College London em 1911. Juntamente com Weldon e Galton fundou, em 1901, a revista Biometrika com o objetivo era desenvolver as teorias estatísticas, editada até os dias de hoje. O trabalho de Pearson como estatístico fundamentou muitos métodos estatísticos de uso comum, nos dias atuais: regressão linear e o coeficiente de correlação, teste do qui-quadrado de Pearson, classificação das distribuições (Moore 2000).\nCharles Edward Spearman (10/09/1863 - 17/09/1945) foi um psicólogo inglês conhecido pelo seu trabalho na área da estatística, como um pioneiro da análise fatorial e pelo coeficiente de correlação de postos de Spearman. Ele também fez bons trabalhos de modelos da inteligência humana.\nWilliam Sealy Gosset (13/07/1876 - 16/10/1937) foi um químico e estatístico inglês (Figura 1.5)). Em 1907, enquanto trabalhava químico da cervejaria experimental de Arthur Guinness & Son, criou a distribuição t que usou para identificar a melhor variedade de cevada, trabalhando com pequenas amostras. A cervejaria Guinness tinha uma política que proibia que seus empregados publicassem suas descobertas em seu próprio nome. Ele, então, usou o pseudônimo “Student” e o teste é chamado “t de Student” em sua homenagem (Salsburg 2009).\n\n\n\n\n\n\n\n\nFigura 1.5: William Sealy Gosset\n\n\n\n\n\nRonald Aylmer Fisher (17/02/1890 - 29/07/1962) foi um estatístico, biólogo e geneticista inglês. Em 1919, Fisher se envolveu com pesquisa agrícola no centro de experimentos de Rothamsted Research, em Harpenden, Inglaterra, e desenvolveu novas metodologias e teoria no ramo de experimentos (Hald 2007). Durante sua vida, Fisher (Figura 1.6) escreveu 7 livros e publicou cerca de 400 artigos acadêmicos em estatística e genética . Em um dos seus livros, The design of Experiments (1935), Fisher relata um experimento que surgiu de uma pergunta curiosa: o gosto do chá muda de acordo com a ordem em que as ervas e o leite são colocados? Essa simples questão resultou em um estudo pioneiro na área e serviu de sustentação para análise da aleatorização de dados experimentais (Salsburg 2009). Ronald A. Fisher foi descrito (Kruskal 1980) como “um gênio que criou praticamente sozinho os fundamentos para o moderno pensamento estatístico”. Era muito temperamental. Seus atritos com outros estatísticos ficaram famosos, entre eles encontra-se ninguém menos do que Karl Pearson, outro notável estatístico.\n\n\n\n\n\n\n\n\nFigura 1.6: Ronald A. Fisher\n\n\n\n\n\nAustin Bradford Hill (08/07/1897 - 18 /04/1991) foi um epidemiologista e estatístico inglês (Figura 1.7)), pioneiro no estudo do acaso nos ensaios clínicos e, juntamente com Richard Doll, foi o primeiro a demonstrar a ligação entre o uso do cigarro e o câncer de pulmão. Hill é amplamente conhecido pelos Critérios de Hill, conjunto de critérios para a determinação de uma associação causal (Stolley e Lasky 2000a).\n\n\n\n\n\n\n\n\nFigura 1.7: Bradford Hill\n\n\n\n\n\nJohn Wilder Tukey (16/06/1915 - 26/07/2000) foi um estatístico norte-americano. Desenvolveu uma filosofia para a análise de dados que mudou a maneira de pensar dos estatísticos, sugerindo que se faça uma visualização dos dados, interpretando o formato, centro, dispersão, presença de valores atípicos, sumarizar numericamente e por fim escolher um modelo matemático. Foi o criador do boxplot e introduziu a palavra “bit” como uma contração do termo binary digit.\nDouglas G. Altman (12 /07/1948 - 03/06/2018) foi um estatístico inglês (Figura 1.8)), conhecido por seu trabalho em melhorar a confiabilidade dos artigos de pesquisa médica (Matthews, Chalmers, e Rothwell 2018) e por artigos altamente citados sobre metodologia estatística. Ele foi professor de estatística em medicina na Universidade de Oxford. Há praticamente 30 anos, Altman (Altman 1994) escreveu um artigo sobre problema da qualidade da pesquisa em medicina que causou um grande impacto e permanece válido até hoje. Nesta publicação ele afirma:\n\n A má qualidade de muitas pesquisas médicas é amplamente reconhecida, mas, de forma perturbadora, os líderes da profissão médica parecem apenas minimamente preocupados com o problema e não fazem nenhum esforço aparente para encontrar uma solução.\n\n\n\n\n\n\n\n\n\nFigura 1.8: Douglas G. Altman",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#história-resumida-do-r",
    "href": "intro.html#história-resumida-do-r",
    "title": "1  Introdução",
    "section": "1.3 História resumida do R",
    "text": "1.3 História resumida do R\nO R é uma linguagem e um ambiente de desenvolvimento voltado fundamentalmente para a computação estatística. Foi inspirado em duas linguagens: S (John Chambers, do Bell Labs) que forneceu a sintaxe e Scheme (Hal Abelson e Gerald Sussman) implementou e forneceu a semântica.\nO nome R provém em parte das iniciais dos criadores, George Ross Ihaka e Robert Gentleman (Figura 1.9), e também de um jogo figurado com a linguagem S. Em 29 de Fevereiro de 2000, o software foi considerado com funcionalidades e estável o suficiente para a versão 1.0.\nO R é um projeto GNU 1. Software Livre significa que os usuários têm liberdade para executar, copiar, distribuir, estudar, alterar e melhorar o software. Foi desenvolvido em um esforço colaborativo de pessoas em vários locais do mundo (R Core Team 2022b).\nO projeto R fornece uma grande variedade de técnicas estatísticas e gráficas. É uma linguagem e um ambiente similar ao S. A linguagem do S que também é uma linguagem de computador voltada para cálculos estatísticos. Um dos pontos fortes de R é a facilidade com que produções gráficas de qualidade podem ser produzidas. O R é também altamente expansível com o uso dos pacotes, que são bibliotecas para sub-rotinas específicas ou áreas de estudo específicas. Um conjunto de pacotes é incluído com a instalação de R e muito outros estão disponíveis na rede de distribuição do R - Comprehensive R Archive Network (CRAN) (R Core Team 2022a).\n\n\n\n\n\n\n\n\nFigura 1.9: Robert Gentlemen (E) e George Ross (D)\n\n\n\n\n\nA linguagem R é largamente usada entre estatísticos e analistas de dados para desenvolver softwares de estatística e análise de dados. Pesquisas e levantamentos com profissionais da área da saúde mostram que a popularidade do R aumentou substancialmente nos últimos anos (Whitney et al. 2020).\n\n\n\n\n\n\nAltman, Douglas G. 1994. «The scandal of poor medical research». Bmj. British Medical Journal Publishing Group.\n\n\nArmitage, Peter, Geoffrey Berry, e John Nigel Scott Matthews. 2008. Statistical methods in medical research. John Wiley & Sons.\n\n\n«Breve História dos Censos». 2014. Instituto Nacional de Estatistica. Statistics Portugal. https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria.\n\n\nEditors, History.com. sem data. «Florence Nightingale». https://www.history.com/topics/womens-history/florence-nightingale-1.\n\n\nHald, Anders. 2007. «Biography of Fisher». Em A History of Parametric Statistics Inference from Bernoulli to Fisher,1713-1935, 159–63. John Wiley & Sons.\n\n\nKendall, Maurice George. 1960. «Studies in the history of probability and statistics. Where shall the history of statistics begin?» Biometrika 47 (3/4). JSTOR: 447–49.\n\n\nKruskal, William. 1980. «The Significance of Fisher: A Review of R.A. Fisher: The Life of a Scientist». Journal of the American Statistical Association 75 (372). Taylor & Francis: 1019–30. doi:10.1080/01621459.1980.10477590.\n\n\nMassad, Eduardo, Paulo Sérgio Panse Silveira, Renèe Xavier de Menezes, e Neli Regina Siqueira Ortega. 2004. Métodos Quantitativos em Medicina. São Paulo, SP: Editora Manole.\n\n\nMatthews, Robert, Iain Chalmers, e Peter Rothwell. 2018. «Douglas G Altman: statistician, researcher, and driving force behind global initiatives to improve the reliability of health research». British Medical Journal Publishing Group.\n\n\nMoore, David S. 2000. «Topics in Inferency». Em The basic practice of statistics, 417. W.H. Freeman.\n\n\nR Core Team. 2022a. «The R Project for Statistical Computing | CRAN Mirrors». Disponível em: https://cran.r-project.org/mirrors.html.\n\n\n———. 2022b. «The R Project for Statistical Computing | What is R?» Disponível em: https://www.r-project.org/about.html.\n\n\nSalgado-Neto, Geraldo, e Aquilea Salgado. 2011. «Sir Francis Galton e os extremos superiores da curva normal». Revista de Ciências Humanas 45 (1): 223–39.\n\n\nSalsburg, David. 2009. «Uma senhora toma chá...» Em Uma senhora toma chá..., 17–23. Zahar.\n\n\nStolley, Paul D, e Tamar Lasky. 2000a. «Lung Cancer: New Methods of Studying Disease». Em Investigating Disease Patterns, 51–79. Scientific American Library.\n\n\n———. 2000b. «The Beginnings of Epidemiology». Em Investigating Disease Patterns, 23–49. Scientific American Library.\n\n\nWhitney, Lance et al. 2020. «R programming language continues to grow in popularity». TechRepublic. https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introdução",
    "section": "",
    "text": "Esta sigla está associada ao animal gnu africano, símbolo de software de distribuição livre, quer dizer is Not Unix, sigla recursiva muito comum entre nerds!↩︎",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "02-dados.html",
    "href": "02-dados.html",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "2.1 Variáveis e Dados\nAs pesquisas manuseiam dados referentes às variáveis que estão sendo estudadas. Variável é toda característica ou condição de interesse que pode de ser mensurada ou observada em cada elemento de uma amostra ou população. Como o próprio nome diz, seus valores são passíveis de variar de um indivíduo a outro ou no mesmo indivíduo. Em contraste com a variável, o valor de uma constante é fixo. As variáveis podem ter valores numéricos ou não numéricos. O resultado da mensuração ou observação de uma variável é denominado dado.\nA Tabela 2.1 mostra um conjunto de variáveis e suas medidas (dados) de um grupo de pacientes internados em uma determinada UTI. O termo medida deve ser entendido num sentido amplo, pois não é possível “medir” o sexo (observação) ou o estado geral (critérios) de alguém, ao contrário do peso e da pressão arterial que podem ser mensurados com instrumentos.\nTabela 2.1: Variáveis e dados\n\n\n\nIdNomeIdadeSexoPASPADEstado Geral1João45masculino14090bom2Maria32feminino11070regular3Pedro27masculino12080grave4Teresa18feminino10060bom",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#população-e-amostra",
    "href": "02-dados.html#população-e-amostra",
    "title": "2  Natureza dos Dados",
    "section": "2.2 População e Amostra",
    "text": "2.2 População e Amostra\nNa pesquisa em saúde, a não ser quando se realiza um censo, coleta-se dados de um subconjunto de indivíduos denominado de amostra, pertencente a um grupo maior, conhecido como população. A população de interesse é, geralmente, chamada de população-alvo. A amostra, para ser representativa da população, deve ter as mesmas características desta. A partir da análise dos dados encontrados na amostra, deduz-se sobre a população. Este processo é denominado de inferência estatística. O interesse na amostra não está propriamente nela, mas na informação que ela fornece ao investigador sobre a população de onde ela provém. A amostra fornece estimativas (estatísticas) da população (Figura 2.1).\n\n População ou população-alvo consiste em todos os elementos (indivíduos, itens, objetos) cujas características estão sendo estudadas.\nAmostra é a parte, subconjunto, da população selecionada para estudo. \n\nEm decorrência do acaso, diferentes amostras de uma mesma população fornecem resultados diferentes. Este fato deve ser levado em consideração ao usar uma amostra para fazer inferência sobre uma população. Este fenômeno é denominado de variação amostral ou erro amostral (Consulte também o Capítulo 11) e é a essência da estatística. O grau de certeza na inferência estatística depende da representatividade da amostra.\nO processo de obtenção da amostra é chamado de amostragem. Mesmo que este processo seja adequado, a amostra nunca será uma cópia perfeita da população de onde ela foi extraída. Desta forma, em qualquer conclusão baseada em dados de uma amostra, sempre haverá o erro amostral. Este erro deve ser tratado estatisticamente tendo em mente a teoria da amostragem, baseada em probabilidades.\n\n\n\n\n\n\n\n\nFigura 2.1: População, amostra e inferência estatística",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#estatística-e-parâmetro",
    "href": "02-dados.html#estatística-e-parâmetro",
    "title": "2  Natureza dos Dados",
    "section": "2.3 Estatística e Parâmetro",
    "text": "2.3 Estatística e Parâmetro\nEstatística é uma característica que resume os dados de uma amostra e o parâmetro é uma característica estabelecida da população. Os valores dos parâmetros são normalmente desconhecidos, porque, na maioria das vezes, é inviável medir uma população inteira. A estatística é um valor aproximado, uma estimativa, do parâmetro. As estatísticas são representadas por letras romanas1 e os parâmetros por letras gregas. Por exemplo, a media da população é representada por \\(\\mu\\) e a média da amostra por \\(\\bar{x}\\); o desvio padrão da população é denotado \\(\\sigma\\) e o desvio padrão da amostra por s.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#escalas-de-medição",
    "href": "02-dados.html#escalas-de-medição",
    "title": "2  Natureza dos Dados",
    "section": "2.4 Escalas de medição",
    "text": "2.4 Escalas de medição\nEm um estudo científico, há necessidade de registrar os dados para que eles representem acuradamente as variáveis observadas. Este registro de valores necessita de escalas de medição. Mensuração ou medição é o processo de atribuir números ou rótulos a objetos, pessoas, estados ou eventos de acordo com regras específicas para representar quantidades ou qualidades dos dados. Para a mensuração das variáveis são usadas as escalas nominal, ordinal, intervalar e de razão (Oliveira Filho 2022).\n\n2.4.1 Escala Nominal\nAs escalas nominais são meramente classificativas, permitindo descrever as variáveis ou designar os sujeitos, sem recurso à quantificação. É o nível mais elementar de representação. São usados nomes, números ou outros símbolos para designar a variável. Os números, quando usados, representam códigos e como tal não permitem operações matemáticas. As variáveis nominais não podem ser ordenadas. Podem apenas ser comparadas utilizando as relações de igualdade ou de diferença, através de contagens. Os números atribuídos às variáveis servem como identificação, ou para associá-la a uma dada categoria. As categorias de uma escala nominal são exaustivas e mutuamente exclusivas. Quando existem duas categorias, a variável é dita dicotômica e com três ou mais categorias, politômicas.\nOs nomes e símbolos que designam as categorias podem ser intercambiáveis sem alterar a informação essencial.\nExemplos: Tipos sanguíneos: A, B, AB, O; variáveis dicotômicas: morto/vivo, homem/mulher, sim/não; cor dos olhos, etc.\n\n\n2.4.2 Escala Ordinal\nAs variáveis são medidas em uma escala ordinal quando ocorre uma ordem, crescente ou decrescente, inerente entre as categorias, estabelecida sob determinado critério. A diferença entre as categorias não é necessariamente igual e nem sempre mensuráveis. Geralmente, designam-se os valores de uma escala ordinal em termos de numerais ou postos (ranks), sendo estes apenas modos diferentes de expressar o mesmo tipo de dados. Também não faz sentido realizar operações matemática com variáveis ordinais. Pode-se continuar a usar contagem.\nExemplos: classe social (baixa, média, alta); estado geral do paciente: bom, regular, mau; estágios do câncer: 0, 1, 2, 3 e 4; escore de Apgar: 0, 1, 2… 10.\n\n\n2.4.3 Escala Intervalar\nUma escala intervalar contém todas as características das escalas ordinais com a diferença de que se conhece as distâncias entre quaisquer números. Em outras palavras, existe um espectro ordenado com intervalos quantificáveis. Este tipo de escala permite que se verifique a ordem e a diferença entre as variáveis, porém não tem um zero verdadeiro, o zero é arbitrário.\nO exemplo clássico é a mensuração da temperatura, usando as escalas de: Celsius ou Fahrenheit. Aqui é legítimo ordenar, fazer soma ou médias. No entanto, 0ºC não significa ausência de temperatura, portanto a operação divisão não é possível. Uma temperatura de 40ºC não é o dobro de 20ºC. Se 40ºC e 20ºC forem transformados para a escala Fahrenheit, passarão, respectivamente, para 104ºF e 68ºF e, sem dúvida, 104 não é o dobro de 68!\n\n\n2.4.4 Escala de Razão\nHá um espectro ordenado com intervalos quantificáveis como na escala intervalar. Entretanto, as medidas iniciam a partir de um zero verdadeiro e a escala tem intervalos iguais, permitindo as comparações de magnitude entre os valores. Refletem a quantidade real de uma variável, permitindo qualquer operação matemática.\nOs dados tanto na escala intervalar como na de razão, podem ser contínuos ou discretos. Dados contínuos necessitam de instrumentos para a sua mensuração e assumem qualquer valor em um certo intervalo. Por exemplo, o tempo para terminar qualquer tarefa pode assumir qualquer valor, 10 min, 20 min, 35 min, etc., de acordo com o tipo de tarefa. Outros exemplos: peso, dosagem de colesterol, glicemia.\nDados discretos possuem valores iguais a números inteiros, não existindo valores intermediários. A mensuração é feita através da contagem. Por exemplo: número de filhos, número de fraturas, número de pessoas.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#sec-tipovariavel",
    "href": "02-dados.html#sec-tipovariavel",
    "title": "2  Natureza dos Dados",
    "section": "2.5 Tipos de Variáveis",
    "text": "2.5 Tipos de Variáveis\nA primeira etapa na descrição e análise dos dados é classificar as variáveis, pois a apresentação dos dados e os métodos estatísticos variam de acordo com os seus tipos. As variáveis, primariamente, podem ser divididas em dois tipos: numéricas ou quantitativas e categóricas ou qualitativas (Kirkwood e Sterne 2003).\n\n2.5.1 Variáveis Numéricas\nAs variáveis numéricas são classificadas em dois tipos de acordo com a escala de mensuração: continuas e discretas.\nAs variáveis contínuas são aquelas cujos dados foram mensurados em uma escala intervalar ou de razão, podendo assumir, como visto, qualquer valor dentro de um intervalo de números reais, dependendo da precisão do instrumento de medição. O tratamento estatístico tanto para variável intervalar como de a razão é o mesmo. A diferença entre elas está na presença do zero absoluto. As variáveis numéricas contínuas têm unidade de medida. Por exemplo, um menino de 4 anos tem 104 cm.\nUma variável numérica é considerada discreta quando é apenas possível quantificar os resultados possíveis através do processo de contagem. Também têm unidade de medida – número de elementos. Por exemplo, o número de fraturas, o número de acidentes, etc.\n\n\n2.5.2 Variáveis Categóricas\nAs variáveis categóricas ou qualitativas são de dois tipos: nominal e ordinal, de acordo com a escala de mensuração. Um tipo particularmente comum é uma variável binária (ou variável dicotômica), que tem apenas dois valores possíveis. Por exemplo, o sexo é masculino ou feminino. Este tipo de variável é bastante utilizado na área da saúde, em Epidemiologia. As variáveis nominais não têm quaisquer unidades de medida e a nominação das categorias é completamente arbitrária e pertencer a uma categoria não significa ter maior importância do que pertencer à outra. Uma variável ordinal tem uma ordem inerente ou hierarquia entre as categorias. Do mesmo modo que as variáveis nominais, as variáveis ordinais não têm unidades de medida. Entretanto, a ordenação das categorias não é arbitrária. Assim, é possível ordená-las de modo lógico. Um exemplo comum de uma variável categórica ordinal é a classe social, que tem um ordenamento natural da maioria dos mais desfavorecidos para os mais ricos. As escalas, como a escore de Apgar e a escala de coma de Glasgow (Sternbach 2000), também são variáveis ordinais. Mesmo que pareçam numéricas, elas apenas mostram uma ordem no estado dos pacientes. O escore de Apgar (Pediatrics e Obstetricians 2006) é uma escala, desenvolvida para a avaliação clínica do recém-nascido imediatamente após o nascimento. Originalmente, a escala foi usada para avaliar a adaptação imediata do recém-nascido à vida extrauterina. A pontuação pode variar de zero a 10. Uma pontuação igual ou maior do que oito, indica um recém-nascido normal. Uma pontuação de sete ou menos pode significar depressão do sistema nervoso e abaixo de quatro, depressão grave.\nAs variáveis ordinais, da mesma forma que as nominais, não são números reais e não convém aplicar as regras da aritmética básica para estes tipos de dados. Este fato gera uma limitação na análise dos dados.\n\n\n2.5.3 Como identificar o tipo da variável?\nA maneira mais fácil de dizer se os dados são numéricos é verificar se eles têm unidades ligadas a eles, tais como: g, mm, ºC, ml, número de úlceras de pressão, número de mortes e assim por diante. Se não, podem ser ordinais ou nominais – ordinais se os valores podem ser colocados em ordem. A Figura 2.2 é uma ajuda para o reconhecimento do tipo de variável (Bowers 2008).\n\n\n\n\n\n\n\n\nFigura 2.2: Caminho para identificar o tipo de variável\n\n\n\n\n\n\n\n2.5.4 Variáveis Dependentes e Independentes\nDe um modo geral as pesquisas são realizadas para testar as hipóteses dos pesquisadores e, para isso, eles medem variáveis com a finalidade de compará-las. A maioria das hipóteses podem ser expressas por duas variáveis: uma variável explicativa ou preditora e uma variável desfecho (Kirkwood e Sterne 2003).\nA variável preditora ou explanatória é a que se acredita ser a causa e também é conhecida como variável independente, porque o seu valor não depende de outras variáveis. Em Epidemiologia, é com frequência referida como exposição ou fator de risco.\nA variável desfecho é aquela que é o efeito, consequência ou resultado da ação de outra variável, por isso, também chamada de variável dependente. Em um estudo que tenta verificar se o tabagismo, durante a gestação, pode interferir no peso do recém-nascido, tem o fumo (variável categórica) como variável preditora (exposição ou fator de risco) e o peso do recém-nascido (variável numérica contínua) como variável desfecho\nNa maioria dos estudos, são utilizadas amostras que fornecem estimativas que, para serem representativas da população, devem ser probabilísticas. Ou seja, a amostra deve ser recrutada de forma aleatória, permitindo que cada um dos membros da população tenha a mesma probabilidade de ser incluído na amostra. Além disso, uma amostra deve ter um tamanho adequado para permitir inferências válidas.\n\n\n\n\n\n\nBowers, David. 2008. «First things first-the nature of data». Em Medical Statistics from Scratch, Second Edition, 3–13. John Wiley; Sons.\n\n\nKirkwood, Betty R, e Jonathan AC Sterne. 2003. «Defining the Data». Em Essential Medical Statistics, Second Edition, 9–14. Blackwell Science Company.\n\n\nOliveira Filho, Petronio Fagundes de. 2022. «Natureza dos Dados». Em Epidemiologia e Bioestatística–Fundamentos para a Leitura Crítica, 2ª edição, 3–6. Editora Rubio.\n\n\nPediatrics, American Academy of, e American College of Obstetricians. 2006. «The apgar score». Pediatrics 117 (4). American Academy of Pediatrics: 1444–47.\n\n\nSternbach, George L. 2000. «The Glasgow coma scale». The Journal of emergency medicine 19 (1). Elsevier: 67–71.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#footnotes",
    "href": "02-dados.html#footnotes",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "Também podem ser representadas pela letra grega correspondente ao respectivo parâmetro com um acento circunflexo, por exemplo, a média amostral é \\(\\hat{\\mu}\\), dita (mü chapéu).↩︎",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html",
    "href": "03-producaoDados.html",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "3.1 Processo de Pesquisa\nA pesquisa é um processo de construção do conhecimento. O objetivo deste processo é gerar um novo conhecimento e/ou confirmar ou refutar algum conhecimento prévio. A pesquisa é um processo de aprendizagem tanto do pesquisador quanto da sociedade que se beneficiará deste novo conhecimento. Para ser chamada de científica, a pesquisa deve obedecer aos princípios consagrados pela ciência (Ribeiro Mendes 2012).\nA pesquisa nasce de uma dúvida do pesquisador, de algum questionamento que ele considerou interessante sobre o mundo, ou seja, de algo que se costuma chamar de pergunta ou questão da pesquisa. Existem vários motivos que geram questões de pesquisa:",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-pesquisa",
    "href": "03-producaoDados.html#processo-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "Avaliação crítica de pesquisas realizadas por outros pesquisadores.\nCondução de uma pesquisa primária com a finalidade de responder uma questão (ou questões), gerando um novo conhecimento ou ampliação do conhecimento existente.\nPara obter habilidades de pesquisa ou experiência, com frequência como parte de um programa educacional.\nTestar a viabilidade de um projeto ou técnica de pesquisa.\n\n\n3.1.1 Questão de Pesquisa\nA pesquisa visa estabelecer novos conhecimentos em torno de um tema específico. O tema de pesquisa pode surgir do próprio interesse ou experiência do pesquisador, ou partir da encomenda de alguma instituição financiadora. Algumas vezes, a pesquisa se origina de outros estudos realizados pelo próprio pesquisador ou outros pesquisadores.\nÀ medida que a ideia da pesquisa cresce, o pesquisador estabelece uma pergunta de pesquisa específica ou um conjunto de questões que ele deseja responder. Algumas vezes, o tema da pesquisa é tão amplo que o pesquisador tem que ter cuidado para não se perder do seu objetivo. Este objetivo é que vai guiá-lo no estabelecimento da pergunta ou perguntas a serem respondidas no estudo. Estes questionamentos são conhecidos como questão de pesquisa ou pergunta de partida.\nO foco da questão de pesquisa pode ser na descrição de um fenômeno clínico. Neste caso a pergunta é dita descritiva, por exemplo, pesquisa de prevalência de uma enfermidade, proporção de utilização de um serviço de saúde, características de um teste, etc. Quando a pergunta busca a explicação para um fenômeno, ela é dita analítica, por exemplo, comparação entre dois fenômenos. Em geral, perguntas analíticas são mais interessantes. Entretanto, as perguntas descritivas são fundamentais no início de um estudo analítico.\nUma boa pergunta de pesquisa deve ter as seguintes características (Hulley et al. 2015):\n\nFactível: o pesquisador deve conhecer desde o início os limites e problemas práticos que podem interferir na pesquisa. A viabilidade está relacionada com o tamanho amostral, com o domínio técnico adequado, com o tempo e custos envolvidos e com um foco dirigido estritamente aos objetivos mais importantes.\nInteressante: a questão de pesquisa deve despertar o interesse não apenas do pesquisador, mas também de seus pares e agentes financiadores.\nNova: a pesquisa deve ser inovadora, original, em algum sentido, para que o estudo seja uma contribuição ao conhecimento ou amplie um conhecimento existente;\nÉtica: se o estudo impõe riscos físicos ou invasão de privacidade ou não traz nenhuma informação nova, o pesquisador deve suspendê-lo. É importante discutir previamente com pesquisadores mais experientes ou com algum representante do Comitê de Ética em Pesquisa da instituição.\nRelevante: nenhuma das características da questão de pesquisa é mais importante do que a sua relevância. Para isto basta pensar nos benefícios que os resultados da pesquisa trarão à Medicina atual.\n\nOu seja, antes de dedicar tempo e esforço para escrever um projeto de pesquisa deve-se avaliar se a questão de pesquisa é FINER (Factível, Interessante, Nova, Ética e Relevante).\n\n\n3.1.2 Hipótese de Pesquisa\nUma vez estabelecida a(s) pergunta(s) de pesquisa adequada(s), os pesquisadores formulam hipóteses para serem testadas. Enquanto a pergunta de pesquisa possa ser um pouco vaga em sua natureza como: “existe uma relação entre o tipo psicológico e a capacidade de parar de usar drogas?” Uma hipótese de pesquisa, necessita ser precisa. Há necessidade de especificar qual o tipo psicológico está relacionado à habilidade de parar de usar drogas.\nA precisão da hipótese é fundamental em um projeto de pesquisa, pois ela determinará o delineamento de pesquisa a ser seguido pelo pesquisador e as técnicas estatísticas apropriadas para a análise dos dados. A fonte e o tipo de dados são determinados pela característica do delineamento recomendado pela hipótese de pesquisa.\nO objetivo da pesquisa, usando o método científico, é refutar ou não as hipóteses de pesquisa. Se a hipótese do pesquisador não for rejeitada, houve a geração de um novo conhecimento.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-amostragem",
    "href": "03-producaoDados.html#processo-de-amostragem",
    "title": "3  Produção dos Dados",
    "section": "3.2 Processo de Amostragem",
    "text": "3.2 Processo de Amostragem\nApós o estabelecimento das hipóteses a serem testadas, há necessidade de coletar os dados. Uma vez que é praticamente impossível analisar toda a população que constitui a população-alvo, extrai-se uma amostra desta população. Este processo é denominado de amostragem (McCombes 2019).\nUma amostra deve ser representativa da população, ou seja, deve ter características semelhantes às da população e ser fidedigna. A fidedignidade está relacionada à precisão dos dados que sofrem influência dos instrumentos de aferição, questionários não validados e falhas humanas. Uma amostra inadequada ameaça a validade da pesquisa. Os dados coletados de maneira não aleatória são chamados de evidência anedótica. O nível de confiança nos resultados de uma pesquisa está diretamente relacionado à qualidade da amostra. A amostra deve ser representativa.\nUma amostra deve conter apenas dados úteis que permitam a resposta da pergunta de pesquisa, evitando desperdício e fuga dos objetivos traçados. A aleatoriedade provoca uma diferença entre o resultado da amostra e o verdadeiro valor da população que é denominada erro amostral. Não importa quão bem a amostra seja coletada, os erros amostrais irão sempre ocorrer. Entretanto, não existe técnica estatística que salve amostras coletadas incorretamente, tendenciosas!\n\n3.2.1 Amostras probabilística\nPara evitar vieses, erros sistemáticos, que favorecem determinados desfechos, o ideal é coletar uma amostra probabilística. A amostra probabilística adota o princípio da equiprobabilidade, isto é, “todos os sujeitos da população têm a mesma probabilidade de fazerem parte da amostra”. Esta probabilidade é conhecida e diferente de zero. As amostras probabilísticas têm o potencial de ser possível a generalização para a população; ser imparcial e com menor erro amostral.\nAmostra aleatória simples: é a mais utilizada pois garante representatividade da amostra junto à população. A amostra aleatória simples não emprega nenhum critério particular para a definição da amostra. O mecanismo mais comum de obter este tipo de amostra é por um simples sorteio, em geral, usando programas de computador.\nAmostra aleatória estratificada: quando a população é constituída por subpopulações ou estratos e é razoável supor que a variável de interesse apresenta comportamento diferente nos diferentes estratos, pode-se usar este tipo de amostragem. Neste caso, a amostra deve ter a mesma estratificação da população para ser representativa. Um exemplo comum de estratificação é o nível socioeconômico. A partir do momento que os estratos estão definidos se procede uma amostra aleatória simples de cada estrato.\nAmostra aleatória sistemática: as unidades amostrais são selecionadas a partir de um esquema rígido preestabelecido de sistematização que tem o propósito de abranger toda a população-alvo. Para isso, ordena-se os indivíduos da população (por exemplo, um grande arquivo com 20000 fichas) e calcula-se uma constante conveniente, \\(c = N/n\\), onde \\(N\\) é tamanho da população e \\(n\\) é o tamanho da amostra. Se \\(n = 500\\), a constante será \\(40\\), ou seja, será selecionado aleatoriamente o primeiro membro da amostra (\\(k\\)), de maneira que \\(k\\) seja menor do que a constante e maior do que \\(1\\). A partir daí os sucessivos membros serão: \\(k + c\\) ; \\(k + 2c\\) ; \\(k + 3c\\) ; … até atingir \\(n\\).\nAmostra aleatória por conglomerados (clusters): este tipo de amostra é utilizada quando dentro da população são identificados agrupamentos (clusters) naturais, por exemplo, espaços, vilas, etc. Neste tipo de amostragem o elemento focal não é o sujeito, mas o cluster. Identificados estes, sorteiam-se os conglomerados e se analisa todos os indivíduos dos conglomerados sorteados.\n\n\n3.2.2 Amostras não probabilísticas\nNa amostragem não aleatória ou intencionada há uma escolha deliberada da amostra, subordinada a objetivos específicos do pesquisador. Não há garantia de representatividade da população. É importante averiguar, neste tipo de amostragem, a presença de conflitos de interesse.\nAmostra de conveniência: é uma técnica comum onde é selecionada uma mostra que esteja acessível. Em outras palavras, os indivíduos são recrutados porque eles estão prontamente disponíveis. Neste tipo de amostra há incapacidade de fazer afirmações gerais com rigor estatístico sobre a população.\nAmostra por cotas: é uma versão não probabilística da amostra estratificada. Tem três etapas:\n\nSegmentação, onde se divide em grupos, por exemplo, sexo, classe social, região, etc.;\nDefinição do tamanho das cotas;\nSeleção por meio de amostras de conveniência.\n\nAmostra de resposta voluntária: o pesquisador solicita aos membros de uma população-alvo para que eles participem da amostra e as pessoas decidem se entram ou não. Esses tipos de amostras são enviesados porque as pessoas podem ter interesses particulares ou opiniões negativas e tendem a querer participar.\n\n\n3.2.3 Tamanho amostral\nA determinação do tamanho de uma amostra é de suma importância, pois amostras desnecessariamente grandes acarretam desperdício de tempo e de dinheiro e amostras muito pequenas podem levar a resultados não confiáveis, ameaçando a validade da pesquisa.\nNão existe um número estabelecido para o tamanho da amostra. Há uma solução para cada caso. O tamanho da amostra depende (Callegari-Jacques 2003):\n\ndo tipo de problema;\ndo tipo de variável;\nda magnitude do erro estatístico aceito pelo pesquisador;\nda diferença minimamente importante entre os grupos;\nda probabilidade de que a amostra identifique uma diferença verdadeira: Poder estatístico;\ndo tempo, dinheiro e pessoal disponível, bem como da dificuldade em se obterem dados e da complexidade da pesquisa.\n\nO tamanho amostral mínimo é determinado por fórmulas estatísticas complexas. Os cálculos são muito pesados, mas agora, felizmente, existem programas de computador disponíveis que realizam este trabalho, por exemplo o G-Power3 (Faul et al. 2007). Além disso, é possível acessar um site que fornece informações e ferramentas para o cálculo amostral em pesquisas da área da saúde 1. Existem tabelas extensas para calcular o número de participantes (Cohen 1988) para um determinado nível de poder (e vice-versa).",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "href": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "3.3 Principais Delineamentos de Pesquisa",
    "text": "3.3 Principais Delineamentos de Pesquisa\nEm geral, a pesquisa clínica, é dividida em dois tipos de investigação. O primeiro é aquele em que o observador apenas observa o doente, as características da sua doença e sua evolução, sem atuar de modo a modificar qualquer aspecto que esteja estudando. Trata-se de estudo observacional.\nO segundo corresponde aos estudos experimentais, onde o pesquisador não se limita a observar, mas promove uma intervenção com o objetivo de conhecer os efeitos dessa sobre os participantes da pesquisa. A intervenção pode ser a prescrição de um medicamento, uma dieta, atividade física ou repouso, ou simplesmente, o estabelecimento de um programa de atenção à saúde.\nOs estudos podem ser também classificados em primários ou secundários ou integrativos (Grimes e Schulz 2002a). Estudos primários correspondem a pesquisas originais que constituem a maioria das publicações encontradas nas revistas médicas. Estudos secundários são aqueles que procuram sumarizar e extrair conclusões de estudos primários\n\nEstudos Primários\n\nEstudos Observacionais\n\nRelato de Caso e Série de Casos\nEstudo Transversal\nEstudo Caso-controle\nEstudo de Coorte\n\nEstudos Experimentais\n\nExperimento laboratorial\nEnsaio Clínico\n\n\nEstudos Secundários\n\nRevisões não sistemáticas\nRevisões Sistemáticas\nDirerizes (Guidelines)\nAnálise de decisão\nAnálise Econômica\n\n\n\n3.3.1 Elementos básicos de um delineamento de pesquisa\nOs estudos contêm três elementos básicos:\n\nVariáveis componentes: Nas investigações das relações entre as variáveis identificam-se pelo menos duas variáveis nos estudos epidemiológicos.\n\nDesfecho: Aquilo que vai acontecer durante uma investigação na mensuração da condição de saúde-doença. Sinônimo: variável dependente.\nExposição: O fator que precede o desfecho. Sinônimos: fator em estudo, variável preditora, variável independente.\n\nTemporalidade: Quanto ao tempo os estudos podem ser contemporâneos, retrospectivos e prospectivos, de acordo como os dados são obtidos em relação ao momento atual.\nEnfoque: Um estudo pode ter vários enfoques. Na maioria deles, na área médica, eles relacionam-se à prevenção, ao diagnóstico, à terapêutica e ao prognóstico.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#estudos-observacionais",
    "href": "03-producaoDados.html#estudos-observacionais",
    "title": "3  Produção dos Dados",
    "section": "3.4 Estudos Observacionais",
    "text": "3.4 Estudos Observacionais\n\n3.4.1 Relato de Caso ou Série de casos\nNo relato de caso, descrevem-se casos raros, eventos não comuns ou inesperados, doenças desconhecidas ou raras. Um evento notável deve ser identificado. Um relato de caso tem a descrição de até dez casos. Acima deste número tem-se uma série de casos (Fletcher, Fletcher, e Fletcher 2014a).\nMetodologicamente, faz-se um relato descritivo simples de características interessantes observadas em um paciente ou grupo de pacientes. Os indivíduos são acompanhados em um espaço de tempo curto e não possuem participantes-controles. A coleta dos dados é, na maioria das vezes, retrospectiva.\nUma série de casos não é planejada e não envolve quaisquer hipóteses investigativas. Pode ser empregada como precursor de outros estudos.\n\n\n3.4.2 Estudos Transversais ou Seccionais\nOs estudos transversais são também conhecidos como estudos seccionais. Este tipo de estudo fornece a informação sobre a prevalência, ou seja, a proporção dos indivíduos que tem a doença ou condição clínica em um determinado momento. Por este motivo são também conhecidos como estudos de prevalência (Grimes e Schulz 2002c).\nObservam dados coletados em um grupo de indivíduos em um único momento, sem um período de seguimento. O desfecho e exposição são avaliados no mesmo momento no tempo. Os dados são coletados apenas uma vez para cada indivíduo, podendo ser em dias diferentes em diferentes sujeitos. As informações são, em geral, obtidas em um curto espaço de tempo.\nÉ um estudo estático, representa a “fotografia” de um momento. Entretanto, se as variáveis preditora e de desfecho são definidas apenas com base nas hipóteses causa-efeito do investigador e não no delineamento do estudo, é possível também examinar associações.\nOs estudos de corte transversal, de um modo geral, são desenhados para determinar “O que está acontecendo?”. São usados para:\n\nDeterminar a prevalência de uma doença, como a prevalência de HIV em gestantes.\nPesquisar atitudes ou opiniões em relação a um determinado assunto (pesquisa de satisfação)\nVerificar interrelações entre variáveis, como observação das características de fumantes pesados em relação ao sexo, idade, etc.\nEnquetes\n\n\n3.4.2.1 Cuidados na interpretação de dados de estudos transversais\n\nEfeito temporal\n\nComo os dados (exposição e desfecho) são coletados no mesmo momento, fica difícil estabelecer qualquer relação temporal entre eles (dilema ovo/galinha). Por exemplo, não é possível estabelecer uma relação de causalidade entre hipertensão e doença cardíaca se os dados são coletados de forma a ficar impossível saber que surgiu em primeiro lugar.\n\nEstudos transversais repetidos\n\nOs estudos transversais, algumas vezes, são repetidos em outro momento ou em outros locais com a finalidade de verificar variabilidade nos achados. Por exemplo, medir a prevalência de uma doença em momentos diferentes ou em diferentes locais. Os indivíduos serão um pouco diferentes, devendo-se interpretar as diferenças destes resultados com cautela.\n\nEstudos transversais que parecem longitudinais\n\nUma armadilha comum é confundir um estudo seccional com um longitudinal porque os dados foram coletados através do tempo até completar o tamanho amostral previsto. O importante é que os dados (variável preditora e desfecho) foram coletados somente uma vez para cada indivíduo e no mesmo momento. Isto gera uma interpretação errônea se analisarmos como um estudo longitudinal.\n\n\n3.4.2.2 Análise dos Estudos Transversais\nQuando se compara a prevalência de doença em expostos e não expostos, a medida de associação usada é a Razão de Prevalência Pontual (RPP).\n\n\n\n3.4.3 Estudos Caso-Controle\nPara examinar a possível associação de uma exposição a uma determinada doença, identifica-se um grupo de doentes (casos) e, com a finalidade de comparação, um grupo de pessoas sem a doença (controles) e determina-se a chance (odds) de exposição e não exposição entre casos e entre controles.\nOs estudos caso-controle, portanto, partem da presença ou ausência de um desfecho e após olham para trás no tempo (retrospectivamente) para detectar possíveis fatores de risco (Figura 3.1)) (Fletcher, Fletcher, e Fletcher 2014b). Analisam o que aconteceu e são usados para investigar fatores de risco de doenças raras onde um estudo prospectivo seria muito longo para identificar uma quantidade suficiente de casos.\nÉ útil também para investigar surtos agudos (infecção alimentar) para identificar se existe ou não associação entre a exposição e o desfecho investigado. Com frequência, os estudos caso-controle são o primeiro passo na busca de uma etiologia quando há suspeita de que alguma de várias exposições esteja associada a uma determinada doença.\n\n\n\n\n\n\n\n\nFigura 3.1: Desenho de um estudo caso controle.\n\n\n\n\n\n\n3.4.3.1 Seleção dos casos\nOs casos podem ser selecionados de várias fontes, incluindo indivíduos hospitalizados, de consultórios ou clínicas, principalmente quando registros adequados são mantidos.\nMuitos problemas podem ocorrer na seleção de casos, neste tipo de estudo. Se os casos forem selecionados de um único hospital, quaisquer fatores de risco identificados podem ser apenas daquele hospital, em decorrência do padrão de referência e nível de atendimento (um hospital terciário que apenas atende um determinado convênio, por exemplo, o Sistema Único de Saúde). Por isso, devem ser utilizados casos procedentes de vários hospitais da comunidade, pois aí os casos pertenceriam a diferentes grupos sociais e diferentes graus de gravidade da doença.\nCasos incidentes ou prevalentes\nOs casos usados nos estudos caso-controle podem ser casos incidentes (recém-diagnosticados) ou casos prevalentes da doença (pessoas que apresentaram a doença em algum período).\nO problema do uso de casos incidentes é que há necessidade de se esperar que novos casos sejam diagnosticados e isto pode requerer muito tempo. Enquanto os casos prevalentes já estão disponíveis havendo um maior número disponível para o estudo. Em ambos os modelos existem problemas, pois nos casos prevalentes algumas pessoas podem morrer logo após o diagnóstico e estarem pouco representadas no estudo. Por outro lado, nos casos incidentes, serão excluídos os pacientes que morreram antes do diagnóstico ser feito. Não existe uma solução fácil para este problema, mas é importante lembrar-se destas questões ao interpretar os resultados e tirar conclusões do estudo.\n\n\n3.4.3.2 Seleção dos controles\nDa mesma forma do que nos estudos experimentais, a escolha dos controles afeta a comparação com os casos (Grimes e Schulz 2005). A escolha dos controles inclui:\n\nPacientes do mesmo hospital, mas com condições ou doenças não relacionadas;\nPacientes pareados um a um em relação a fatores prognósticos, tais como sexo e idade;\nUma amostra aleatória originária da mesma população de onde provêm os casos.\n\nSem dúvida, o melhor grupo controle é a terceira opção, mas esta é raramente possível. Por este motivo, alguns estudos caso-controle incluem mais de um grupo controle para tornar o estudo mais robusto\nControles pareados\nO emparelhamento é definido como processo de seleção dos controles para que sejam semelhantes aos casos em algumas características como, por exemplo, idade, gênero, raça, condição socioeconômica e ocupação.\nControles emparelhados são bastante comuns. O autor deve ter o cuidado de especificar cuidadosamente o modo como houve o pareamento. Por exemplo, “emparelhado por idade dentro de dois anos” mostra a amplitude do pareamento. É difícil realizar o emparelhamento para muitos fatores, pois um pareamento seguro não existe. Em um delineamento pareado, a análise estatística deve levar em conta o emparelhamento e os fatores usados por ele. Onde um indivíduo em um par tiver um dado perdido, ambos devem ser omitidos da análise estatística.\n\n\n3.4.3.3 Estudos caso-controle aninhados\nUm delineamento do tipo caso-controle aninhado é um estudo de caso-controle ’’aninhado” em um estudo de coorte (Ernster 1994). É um excelente desenho para variáveis preditoras que são caras para medir e que podem ser avaliadas no final do estudo em indivíduos que desenvolvem o resultado durante o estudo (casos) e em uma amostra daqueles que não o fazem (controles).\nO investigador começa com uma coorte adequada (Figura 3.2) (Newman et al. 2015) com casos suficientes ao final do acompanhamento para fornecer poder adequado para responder à pergunta de pesquisa. No final do estudo, aplica critérios que definem o resultado de interesse para identificar todos aqueles que desenvolveram o resultado (casos). Em seguida, seleciona uma amostra aleatória dos indivíduos que não desenvolveram o resultado (controles).\nA principal razão para usar delineamentos caso-controle aninhado é reduzir o trabalho e o custo na coleta de dados. A principal desvantagem desse projeto é que muitas questões e circunstâncias da pesquisa não são passíveis de armazenamento para posterior análise.\n\n\n\n\n\n\n\n\nFigura 3.2: Desenho de um estudo caso-controle aninhado.\n\n\n\n\n\n\n\n3.4.3.4 Estudo caso-controle de base populacional\nSão os estudos caso-controle onde os casos e controles são uma amostra completa ou probabilística de uma população definida.\n\n\n3.4.3.5 Limitações dos estudos caso-controle\nVárias limitações podem afetar os estudos caso-controle:\n\nA escolha do grupo controle afeta as comparações entre casos e controles;\nOs dados da exposição ao fator de risco são coletados retrospectivamente e dependem da memória dos participantes, registros médicos e, portanto, podem ser incompletos, sem acurácia ou enviesados (viés de memória);\nSe o processo que conduz à identificação dos casos está relacionado a um possível fator de risco, a interpretação dos resultados será difícil (viés averiguação).\n\nPor exemplo: suponha que os casos sejam mulheres jovens com hipertensão selecionadas de uma clínica de contracepção. Nesta situação, um possível fator de risco, o anticoncepcional oral (ACO), estará vinculado à seleção dos casos e, desta forma, o uso de ACO será mais comum entre os casos do que entre os controles populacionais.\n\n\n\n\n3.4.3.6 Análise dos Estudos Caso-controle\nA principal estratégia de análise é o cálculo da odds ratio (Razão de Chances), que pode ser interpretado como uma estimativa do Risco Relativo.\nO Risco Relativo somente pode ser calculado quando é possível o cálculo da incidência (ver Seção 22.3.2). Nos estudos caso-controle, isso não é possível, pois aqui o estudo começa com casos e controles em vez de indivíduos expostos e não expostos ao fator de risco. Desta maneira, se comparam as odds (chance) de uma exposição passada a um fator de risco suspeitado em indivíduos doentes e em controles não doentes. Esta relação é denominada de odds ratio (ver Seção 22.3.1).\n\n\n\n3.4.4 Estudos de Coorte\nOs estudos de coorte são considerados o padrão-ouro dos estudos observacionais. Seu nome se originou das coortes dos soldados romanos, cada uma delas constituída por 480 a 600 legionários. As coortes romanas eram distintas entre si e tinham sua identidade determinada por, ao menos, uma característica comum entre os indivíduos de cada grupo. Podia ser por características estratégicas no campo de batalha, por uma cor presente na indumentária, ou outras. Em Epidemiologia, o termo coorte permaneceu com significado semelhante.\nEm um estudo de coorte, um grupo de pacientes sadios (coorte), expostos ou não a um suspeitado fator de risco, é seguido através do tempo para determinar a incidência da doença em questão em cada um dos grupos (Grimes e Schulz 2002b).\nNeste modelo de estudo, a característica comum aos dois grupos é a exposição. Tem-se uma coorte de expostos e uma coorte de não expostos que são acompanhadas por um período de tempo que permita o aparecimento do desfecho. No final do estudo, compara-se a incidência do desfecho (doença) entre os expostos com a incidência do desfecho entre os não expostos. Se existe uma associação positiva entre a exposição e o desfecho, se espera que a incidência do desfecho entre os expostos seja maior do que a incidência de desfecho entre não expostos.\nUm esquema simplificado de um estudo de coorte é mostrado na Figura 3.3.\n\n\n\n\n\n\n\n\nFigura 3.3: Desenho de um estudo de coorte sobre risco.\n\n\n\n\n\nObservar que como se identifica novos casos (incidência) à medida que eles ocorrem, é possível determinar uma relação temporal entre a exposição e a doença, isto é, se a exposição precedeu o início da doença. Isto é fundamental para estabelecer uma relação causal entre a exposição e a doença.\nOs estudos de coorte têm semelhança com os ensaios clínicos randomizados. Ambos os estudos comparam grupos expostos a grupos não expostos. Não havendo possibilidade de realizar a randomização, por exemplo, por motivos éticos quando a exposição é sabidamente prejudicial, é indicado um estudo de coorte. A diferença fundamental, portanto, é a ausência de randomização nos estudos de coorte.\nExistem duas maneiras básicas para formar os grupos:\n\nSeleciona-se a população-alvo baseado no fato dos indivíduos estarem expostos ou não ao fator em estudo (Figura 3.3);\nOu seleciona-se a população-alvo antes que qualquer um dos seus membros se torne exposto, ou antes, que a exposição seja identificada (Figura 3.4). Um exemplo típico deste modelo é o clássico Estudo de Framingham (Kannel e McGee 1979).\n\n\n\n\n\n\n\n\n\nFigura 3.4: Desenho de uma coorte com grupos expostos e não expostos. Celentano e Szklo (2019).\n\n\n\n\n\n\n3.4.4.1 Tipos de estudo de coorte\nDe acordo com as características do seguimento, as coortes podem ser:\n\nEstudo de Coorte Prospectivo (Coorte Concorrente ou Longitudinal), onde os grupos são montados no presente, coletados os dados basais deles e continua-se a coletar dados com o passar do tempo até a doença se desenvolver ou não.\nEstudo de Coorte Retrospectivo ou Histórico (Coorte não concorrente), onde a exposição é avaliada em dados passados e o desfecho (doença ou não) é verificado no momento do início do estudo. O problema aqui é que a averiguação da exposição depende dos registros pregressos.\nEstudo de Coorte Misto (Prospectivo e Retrospectivo), onde a exposição é verificada em registros objetivos no passado (como em uma coorte histórica) e o seguimento e a medida do desfecho se fazem no futuro.\n\n\n\n3.4.4.2 Vieses em estudos de coorte\nOs potenciais vieses nos estudos de coorte são os seguintes:\n\nViés de confusão – é a grande ameaça dos estudos observacionais. O confundimento causa um erro sistemático na inferência, podendo aumentar ou diminuir uma associação observada entre exposição e doença. Uma variável funciona como fator de confusão quando ela está associada com a exposição e ao mesmo tempo com a doença. Ela não deve fazer parte da cadeia causal da exposição à doença. Por exemplo, num estudo sobre fatores de risco, uma associação entre o hábito de beber café e a doença coronária é detectada. Porém, se não for considerado o fato de que os fumantes bebem mais café do que os não-fumantes, pode-se chegar à errônea conclusão de que o café é um fator de risco independente para doença coronária, o que não corresponde à realidade. Neste caso, o café é um fator de confusão e não um fator causal independente para a doença coronária (Coutinho 1998).\nViés na avaliação dos desfechos – este viés pode ocorrer quando o pesquisador que avalia o desfecho também sabe sobre o status de exposição dos sujeitos da pesquisa. Evita-se este problema “cegando” a pessoa que faz a avaliação da doença.\nViés de informação – ocorrem principalmente em estudos históricos onde as informações dependem de registros passados e podem ser diferentes entre as pessoas expostas e não expostas.\nViés de não resposta e perdas de acompanhamento – a não participação e as perdas podem introduzir um grande viés, alterando o cálculo da incidência nos expostos e entre os não expostos.\nViés de análise – se os estatísticos tiverem alguma hipótese em relação aos dados que estão analisando, eles podem introduzir vieses em suas análises.\n\n\n\n3.4.4.3 Análise dos estudos de coorte\nPara verificar se existe associação entre certo desfecho (doença) e uma determinada exposição calcula-se o Risco Relativo (RR). Este é definido como a razão entre a incidência (risco) em expostos e a incidência (risco) em não expostos (ver Seção 22.3.2).\n\n\n3.4.4.4 Vantagens e desvantagens dos estudos de coorte\n\nVantagens\n\nAdequado para exposições raras\nBom poder para testar hipóteses\nImportante em estudos etiológicos e prognósticos\nSalienta os múltiplos desfechos de uma exposição\n\nDesvantagens\n\nInadequado em desfechos raros\nPerdas no seguimento levam a viés de seleção\nDemorado/elevado custo",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#sec-trials",
    "href": "03-producaoDados.html#sec-trials",
    "title": "3  Produção dos Dados",
    "section": "3.5 Ensaios Clínicos",
    "text": "3.5 Ensaios Clínicos\nExperimentos são estudos nos quais o pesquisador manipula a variável preditora (intervenção) e observa o efeito no desfecho que está sendo avaliado ao longo do tempo. A abordagem experimental, especificamente, o ensaio clínico randomizado controlado é a ferramenta de escolha para comparar terapêuticas ou intervenções.\nOs estudos experimentais podem também comparar os cuidados prestados por serviços de saúde, programas de educação em saúde e estratégias administrativas. Os estudos experimentais realizados com seres humanos são denominados de ensaios clínicos.\nNos ensaios clínicos não controlados os indivíduos servem como seus próprios controles (antes-e-depois). Os resultados destes estudos estão sujeitos vários problemas:\n\nMelhora previsível. Paciente melhora espontaneamente e não pelo tratamento.\nFlutuação na gravidade da doença.\nEfeito Hawthorne: o indivíduo melhora pela atenção e não pela terapêutica (McCambridge, Witton, e Elbourne 2014).\nRegressão à média: uma limitação importante surge quando se quer avaliar a evolução de um grupo que tenha sido selecionado por estar no extremo de uma distribuição sem que haja um grupo controle. Empiricamente, observa-se que indivíduos que se encontrem num determinado momento, em um dos extremos de uma distribuição, tendem a estarem menos distantes da média em um momento posterior, sem que qualquer intervenção tenha sido desenvolvida. Este fenômeno é conhecido como efeito de regressão à média. Por exemplo: uma pessoa com uma doença crônica tem dias piores e outros melhores. Se ela é medicada com gotas homeopáticas ou faz uso de florais nos dias em que se sente excepcionalmente mal vai notar que é frequente uma melhora, seguindo estes “tratamentos”. Não que eles funcionem, mas pela regressão à média (Bland e Altman 1994).\n\n\n3.5.1 Características de um ensaio clínico\nUm ensaio clínico deve ter algumas características fundamentais (Figura 3.5) (Fletcher, Fletcher, e Fletcher 2014c):\n\nOs indivíduos devem ser designados por randomização para os grupos de comparação.\n\nA randomização é a melhor abordagem no delineamento de um ensaio clínico Kabisch et al. (2011).\nRandomizar significa sortear (por meio de computadores, tábua de números aleatórios) os indivíduos para decidir a alocação dos mesmos em um dos grupos de estudo. O elemento decisivo da randomização é a imprevisibilidade da próxima alocação.\n\nO pesquisador compara o grupo de estudo com um grupo controle apropriado.\nO investigador manipula a variável independente (preditora).\n\n\n\n\n\n\n\n\n\nFigura 3.5: Estrutura de um ensaio clínico randomizado.\n\n\n\n\n\n\n\n3.5.2 Elementos básicos de um ensaio clínico\n\nSeleção dos participantes\n\nOs pesquisadores devem determinar e explicar detalhadamente os critérios de inclusão e de exclusão:\n\nObjetivos dos critérios de inclusão e exclusão\n\nRestringir a heterogeneidade da amostra\nDiminuir o número de variáveis independentes\nFazer com que exista uma chance maior de que as diferenças nos desfechos estejam relacionadas aos tratamentos\nMelhorar a validade interna, ou seja, o grau em que os resultados do estudo são consistentes para aquela amostra particular de indivíduos. Esta validade depende basicamente do rigor metodológico usado para delinear o ensaio clínico, podendo ser ameaçada por dois tipos de erros: sistemático ou aleatório.\n\nTornar a generalização (validade externa) mais precisa. Entretanto deve-se ter cuidado com critérios de inclusão e exclusão muito rígidos, pois podem diminuir esta capacidade de generalização\n\n\nO grau de detalhamento deve ser suficientemente preciso para permitir que outros reproduzam o estudo. O tamanho da amostra deve ser claramente determinado pelo poder do teste estatístico. Poder é a habilidade de o teste estatístico detectar diferenças entre os grupos, dado que tais diferenças existam na população em estudo. Lembrar que resultados não significativos podem ser apenas uma evidência para um inadequado tamanho amostral.\nO grupo controle deve ser selecionado utilizando-se os mesmos critérios do grupo experimental. Prestar atenção em possíveis armadilhas que podem gerar vieses:\n\nUso de grupo controle histórico (não concorrente);\nGrupo controle selecionado de outros locais (outras clínicas, outros hospitais).\n\nO grupo controle adequado é um grupo controle concorrente, tratado no mesmo momento e no mesmo local do grupo experimental. O característico é o grupo controle não receber tratamento. Mais comumente recebem um placebo, indistinguível do tratamento experimental, mas sem componente ativo. Mesmo assim, pode haver melhora dos participantes do grupo controle (Efeito Placebo ) (Elander e Hermerén 1995). Quando não for ético suspender o tratamento e administrar placebo, o grupo controle pode ser constituído por indivíduos que recebem o tratamento padrão.\n\nAlocação\n\nA alocação deve ser aleatória. A randomização é a principal técnica para reduzir o viés, criando grupos homogêneos. Como foi visto, é uma das características fundamentais dos ensaios clínicos. O poder da randomização depende da ocultação da sequência de alocação.\nA randomização pode ser:\n\nCompleta: os indivíduos que obedecem ao critério de inclusão e exclusão são randomizados de modo que todos têm a mesma probabilidade de pertencer a cada um dos grupos. Isto maximiza o poder. Pode ser feita por blocos para assegurar a igualdade numérica dos grupos (estudos multicêntricos).\nEstratificada: os participantes são estratificados de acordo com possíveis variáveis de confusão (gravidade da doença, idade, sexo, etc.) e a randomização é realizada dentro de cada estrato.\nRandomização e alocação desigual: os sujeitos têm uma maior probabilidade de ser randomizados em um grupo (em geral, grupo experimental) do que o outro (comparação). Este tipo de estudo tem menor poder.\n\n\nCondução/Seguimento/Avaliação\n\nEm um ensaio clínico deve estar assegurado de que o estudo tenha um tempo de seguimento adequado, pois nem todos os indivíduos participam conforme o plano original. Podem ocorrer perdas de alguns pacientes durante o acompanhamento, seja porque com o tempo se constata que eles não têm a doença em estudo ou porque não aderiram ao tratamento ou intervenção e abandonaram o estudo. Quanto maior o número de pacientes perdidos e menos informações sobre eles, menos confiança pode ser colocada nos resultados do estudo. De um modo geral, não se deve tolerar perdas que sejam maiores que a incidência do desfecho no estudo. Uma regra simples é que perdas menores que 5% produzem pouco viés e perdas maiores que 20% são uma ameaça importante à validade do estudo. As perdas entre 5 e 20% devem ser avaliadas com cuidado, se possível utilizando-se uma análise de sensibilidade (pior cenário), principalmente se as perdas forem diferentes nos grupos pelo maior risco de viés.\nNeste tipo de análise, nos estudos com resultado positivo, todos os pacientes perdidos no grupo experimental, inicialmente, são considerados como tendo o desfecho. Posteriormente, analisa-se como se nenhum dos indivíduos perdidos no grupo controle atingiu o desfecho. Se o resultado permanecer positivo, as perdas não afetaram a validade do estudo. Estudos sem relato adequado ou nenhum relato de perdas ou exclusões devem ser avaliados com muito cuidado.\nOutro aspecto importante, no seguimento dos sujeitos da pesquisa, é o tratamento igual de todos os grupos. Para garantir este princípio, utiliza-se da técnica de cegamento ou mascaramento (Schulz e Grimes 2002). Esta técnica impede que os participantes da pesquisa (pesquisadores, avaliadores e participantes) tomem conhecimento de qual grupo de tratamento o participante se encontra. Este conhecimento antecipado pode influenciar as expectativas, as opiniões e as crenças em relação aos resultados do estudo. O cegamento tem como principal finalidade a eliminação do viés de aferição, além de melhorar a adesão ao tratamento, reduzir as perdas de seguimento e diminuir o viés causado por co-intervenções (assistência suplementar maior para um dos grupos).\nQuando o cegamento ocorre nos pacientes e nos pesquisadores, diz-se que o estudo é duplo-cego. Se ele também incluir os avaliadores do estudo, ele é triplo cego. Um ensaio clínico em que não há cegamento é dito aberto (open label, no caso de estudos com fármacos).\nA avaliação dos desfechos também pode afetar os resultados. É importante garantir-se que aqueles que registram os desfechos estejam cegados em relação a que grupo o sujeito da pesquisa pertence. Os autores devem estabelecer regras cuidadosas para decidir se um desfecho ocorreu ou não e despender esforços iguais para identificar desfechos para todos os pacientes no estudo.\n\nIntenção de tratar\n\nOs pesquisadores violam a randomização se omitirem da análise os pacientes que não receberam a intervenção designada ou, pior ainda, contarem eventos que ocorreram nos sujeitos não aderentes que foram designados para a intervenção contra o grupo controle. Os sujeitos de uma pesquisa, para evitar tal viés, devem ser analisados dentro do grupo para o qual eles foram alocados pela randomização (Montori e Guyatt 2001). Este princípio é denominado intenção de tratar.\n\nAnálise da magnitude do efeito\n\nCalcula-se uma série de estimativas quantitativas para analisar a magnitude do efeito da intervenção em um ensaio clínico. Entre elas, destacam-se o Risco Relativo, Redução Relativa do Risco, Número Necessário para Tratar que serão estudados no capítulo @ref(sec-cap18).\nOutro método para avaliar resultados de um ensaio clínico para dados de tempo até o evento é a análise de sobrevida. Esta fornece informação sobre a rapidez com que os eventos ocorrem. A curva de sobrevida pode utilizar dados de pacientes acompanhados por diferentes períodos de tempo.\n\n\n3.5.3 Ensaios clínicos de equivalência e não inferioridade\nEnsaios clínicos controlados com placebo são ideais para avaliar a eficácia de um tratamento. Eles permitem o controle do efeito placebo e são mais eficientes, exigindo um menor número de pacientes para detectar um efeito do tratamento. Um ensaio clínico placebo controlado é eticamente justificado se não existe tratamento padrão, se o tratamento padrão não se mostrou eficaz, não há riscos associados com o retardo no tratamento e se a possiblidade de se retirar do estudo está incluída no protocolo. Sempre que possível e justificado, os ensaios clínicos placebo controlados devem ser a primeira escolha para avaliação de um tratamento.\nDado que um grande número de tratamentos eficazes comprovados está disponível, ensaios clínicos controlados por placebo são, muitas vezes, antiéticos. Nestas situações, ensaios clínicos com controle ativo são geralmente apropriados.\nSe o objetivo do ensaio clínico é testar se um novo tratamento é similar em eficácia a um tratamento já existente, ele é denominado de Estudo de Equivalência. O Ensaio Clínico é delineado de maneira que possa demonstrar que, dentro limites aceitáveis, os dois tratamentos são igualmente eficazes. Existe equivalência quando a diferença observada entre os dois tratamentos for menor que a máxima diferença aceitável, determinada previamente. Estes limites devem ser clinicamente apropriados. Se condição em investigação for muito grave, os limites para a equivalência devem ser estreitados. Quanto menor forem os limites de equivalência, maior o tamanho amostral. Este delineamento é útil se o novo tratamento trouxer benefícios, tais como menores efeitos colaterais, facilidade no uso e ser mais barato.\nEm muitos estudos com controle ativo, os pesquisadores desejam comprovar que o tratamento em estudo, no mínimo, não é substancialmente pior que o tratamento controle. Estes estudos são chamados de Estudos de Não Inferioridade. Um aspecto importante do delineamento e da interpretação desses estudos é a determinação da margem de não inferioridade. Os estudos de não inferioridade devem demonstrar, pelo menos, que o tratamento em estudo tem alguma eficácia, não inferior ao tratamento padrão. A análise dos estudos de não inferioridade é, por natureza, unidirecional.\nQuando um ensaio clínico busca evidenciar que um tratamento é melhor do que outro ele é denominado Estudos de Superioridade. Quando o ensaio clínico é delineado, ele deve ter uma hipótese bilateral e o tamanho da amostra definido de maneira que haja alto poder estatístico para detectar uma diferença clinicamente significativa entre os dois tratamentos. Os ensaios clínicos clássicos têm esta característica. Entretanto, nos dias atuais, este desenho de estudo pode não ser eticamente possível, uma vez que é pouco provável que não exista um tratamento com algum benefício comprovado. A comparação, portanto, deverá ser feita com o tratamento já existente, provando que o tratamento em estudo é similar ou, pelo menos, não seja inferior (Christensen 2007).\n\n\n3.5.4 Outros tipos de ensaios clínicos\n\n3.5.4.1 Ensaio clínico com delineamento cruzado\nNo delineamento cruzado (crossover design), os sujeitos da pesquisa são randomizados para um grupo e depois mudados para o outro grupo (Figura 3.6). Cada sujeito serve como seu próprio controle, diminuindo a variabilidade intragrupo, aumentando o poder e consequentemente, reduzindo o erro \\(\\beta\\) (erro que ocorre quando a análise estatística dos dados não consegue rejeitar uma hipótese, no caso desta hipótese ser falsa). É um tipo de delineamento bastante atrativo e útil (Health Improvement e Disparities 2020).\nA maior desvantagem é o efeito residual (carryover), por isso os estudos cruzados devem ter um período de washout, período sem nenhum tratamento. Este período de tempo deve ser suficiente para a eliminação da droga para se ter certeza de que nenhum efeito da terapia permaneceu. Também pode haver um viés de acordo com a ordem de administração das terapias, pois os pacientes podem reagir de modo diferente como resultado do entusiasmo no início do tratamento que pode diminuir com o tempo.\n\n\n\n\n\n\n\n\nFigura 3.6: Ensaio clínico randomizado com delineamento cruzado.\n\n\n\n\n\n\n\n3.5.4.2 Delineamento Fatorial\nUma variação interessante de ensaio clínico é o delineamento fatorial. Este tipo de estudo permite que sejam testadas duas drogas em apenas um estudo, assumindo que os desfechos antecipados para as duas são diferentes e que seus modos de ação são independentes. Este desenho de estudo gera economia.\nUm exemplo de delineamento fatorial é observado no Physician’s Health Study onde usando um delineamento fatorial 2 x 2 foi testada a aspirina para a prevenção primária de doença cardiovascular (Physicians’ Health Study Research Group* 1989), e betacaroteno para a prevenção primária de câncer.\nNo estudo da prevenção primária do câncer, os autores concluíram, após 12 anos de suplementação de betacaroteno, que o mesmo não produziu nem benefícios e nem prejuízos em termos de incidência de câncer (Hennekens, Buring, et al. 1996).\n\n\n\n3.5.5 Fases de um ensaio clínico\nPara a realização de um ensaio clínico, a intervenção deve passar por várias fases (Stanley 2007).\n\n3.5.5.1 Fase Não Clínica\nAntes de começar a testar novos tratamentos em seres humanos, os cientistas testam as substâncias em laboratórios (in vitro) e em animais de experimentação. O objetivo principal desta fase é verificar como esta substância se comporta em um organismo. Assim, após esta fase se pode verificar se o medicamento é seguro para ser testado em seres humanos. Todo este processo é regido por leis da bioética em pesquisa em animais.\n\n\n3.5.5.2 Fase Clínica\nA fase clínica é a fase de testes em seres humanos. Esta etapa é constituída por quatro fases consecutivas e somente depois de finalizadas todas as fases, a droga poderá ser autorizada para comercialização e disponibilizada para uso em seres humanos. As sucessivas fases dentro da fase clínica são:\n\nFase I - Um estudo de fase I testa a droga pela primeira vez. O objetivo principal é avaliar a segurança do produto investigado. Nesta fase, o medicamento é testado em pequenos grupos (10 – 30 pessoas), geralmente, de voluntários sadios. Podemos ter exceções se estivermos avaliando medicamentos para câncer ou portadores de HIV-AIDS. Se a droga se mostrar segura, é possível ir para a Fase II.\nFase II - Nesta fase, o número de pacientes é maior (70 - 100). O objetivo é avaliar a eficácia da medicação, isto é, se ela funciona para tratar determinada doença, e também conseguir informações mais detalhadas sobre a segurança (toxicidade). Somente se os resultados forem bons é que o medicamento será estudado como um estudo clínico fase III.\nFase III - Nesta fase, o novo tratamento é comparado com o tratamento padrão existente. São os ensaios clínicos. O número de pacientes aumenta e depende da hipótese (em geral, 100 a 1.000). Devem de preferência utilizar desfechos clínicos, grupo controle, além de serem randomizados e duplo-cegos.\nFase IV - Estes estudos são realizados para se confirmar que os resultados obtidos na fase III são aplicáveis a grande parte dos doentes. Nesta fase, o medicamento já foi aprovado para ser comercializado. A vantagem dos estudos fase IV é que eles permitem acompanhar os efeitos dos medicamentos em longo prazo. É uma fase de vigilância pós-comercialização.\n\n\n\n\n\n\n\nBland, J Martin, e Douglas G Altman. 1994. «Statistic Notes: Regression towards the mean». BMJ 308 (6942). British Medical Journal Publishing Group: 1499.\n\n\nCallegari-Jacques, Sidia M. 2003. «Amostras». Em Bioestatistica: principios e aplicações, 146–47. Artmed Editora.\n\n\nCelentano, David D, e Moyses Szklo. 2019. «Cohort Studies». Em Gordis Epidemiology, 6th Edition, 179. Elsevier.\n\n\nChristensen, Erik. 2007. «Methodology of superiority vs. equivalence trials and non-inferiority trials». Journal of hepatology 46 (5). Elsevier: 947–54.\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral sciences. Lawrence Erlbaum Associates.\n\n\nCoutinho, Mario. 1998. «Principios de epidemiologia clínica aplicada a cardiologia». Arquivos Brasileiros de Cardiologia 71. SciELO Brasil: 109–16.\n\n\nElander, Gunnel, e Göran Hermerén. 1995. «Placebo effect and randomized clinical trials». Theoretical Medicine 16 (2). Springer: 171–82.\n\n\nErnster, Virginia L. 1994. «Nested case-control studies». Preventive Medicine 23 (5). Elsevier: 587–90.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, e Axel Buchner. 2007. «G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences». Behavior research methods 39 (2). Springer: 175–91.\n\n\nFletcher, Robert H, Suzanne W Fletcher, e Grant S Fletcher. 2014a. «Prognóstico». Em Epidemiologia Clínica: Elementos Essenciais, 108–9. Artmed Editora.\n\n\n———. 2014b. «Risco: da doença à exposição». Em Epidemiologia Clínica: Elementos Essenciais, 88. Artmed Editora.\n\n\n———. 2014c. «Tratamento». Em Epidemiologia Clínica: Elementos Essenciais, 143. Artmed Editora.\n\n\nGrimes, David A, e Kenneth F Schulz. 2002a. «An overview of clinical research: the lay of the land». The lancet 359 (9300). Elsevier: 57–61.\n\n\n———. 2002b. «Cohort studies: marching towards outcomes». The Lancet 359 (9303). Elsevier: 341–45.\n\n\n———. 2002c. «Descriptive studies: what they can and cannot do». The Lancet 359 (9301). Elsevier: 145–49.\n\n\n———. 2005. «Compared to what? Finding controls for case-control studies». The Lancet 365 (9468). Elsevier: 1429–33.\n\n\nHealth Improvement, Office for, e Disparities. 2020. «Crossover randomised controlled trial: comparative studies». Office for Health Improvement and Disparities. UK Health improvement. https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies.\n\n\nHennekens, Charles H, Julie E Buring, et al. 1996. «Lack of effect of long-term supplementation with beta carotene on the incidence of malignant neoplasms and cardiovascular disease». New England Journal of Medicine 334 (18). Mass Medical Soc: 1145–49.\n\n\nHulley, Stephen B, Steven R Cummings, Warren S Browner, Deborah G Grady, e Thomas B Newman. 2015. «Elaborando a questão de pesquisa e desenvolvendo o plano de estudo». Em Delineando a pesquisa clinica, Quarta Edição, 15–24. Artmed Editora.\n\n\nKabisch, Maria, Christian Ruckes, Monika Seibert-Grafe, e Maria Blettner. 2011. «Randomized controlled trials: part 17 of a series on evaluation of scientific publications». Deutsches Ärzteblatt International 108 (39). Deutscher Arzte-Verlag GmbH: 663.\n\n\nKannel, William B, e Daniel L McGee. 1979. «Diabetes and cardiovascular risk factors: the Framingham study.» Circulation 59 (1). Am Heart Assoc: 8–13.\n\n\nMcCambridge, Jim, John Witton, e Diana R Elbourne. 2014. «Systematic review of the Hawthorne effect: new concepts are needed to study research participation effects». Journal of Clinical Epidemiology 67 (3). Elsevier: 267–77.\n\n\nMcCombes, Shona. 2019. «Sampling Methods». https://www.scribbr.com/methodology/sampling-methods/. scribbr.com Team. https://www.scribbr.com/.\n\n\nMontori, Victor M, e Gordon H Guyatt. 2001. «Intention-to-treat principle». CMAJ 165 (10). Can Med Assoc: 1339–41.\n\n\nNewman, Thomas B, Warren S Browner, Steven R Cummings, e Stephen B Hulley. 2015. «Delineando estudos de caso-controle». Em Delineando a pesquisa clinica, Quarta Edição, 111. Artmed Editora.\n\n\nPhysicians’ Health Study Research Group*, Steering Committee of the. 1989. «Final report on the aspirin component of the ongoing Physicians’ Health Study». New England Journal of Medicine 321 (3). Mass Medical Soc: 129–35.\n\n\nRibeiro Mendes, Fabio. 2012. «O que é um trabalho científico». Em Iniciacão Cientifica, 17–26. Autonomia Editora.\n\n\nSchulz, Kenneth F, e David A Grimes. 2002. «Blinding in randomised trials: hiding who got what». The Lancet 359 (9307). Elsevier: 696–700.\n\n\nStanley, Kenneth. 2007. «Design of randomized controlled trials». Circulation 115 (9). Am Heart Assoc: 1164–69.",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#footnotes",
    "href": "03-producaoDados.html#footnotes",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "http://calculoamostral.bauru.usp.br/calculoamostral/index.php↩︎",
    "crumbs": [
      "Parte I - Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html",
    "href": "04-introducaoR.html",
    "title": "4  Introdução ao uso do R",
    "section": "",
    "text": "4.1 Instalação do R básico\nPara usar o R, há necessidade de carregar o programa básico que contém a sua linguagem de programação. O sistema é formado por um programa básico, Graphical User Interface (R-Gui) e muitos pacotes com procedimentos adicionais.\nO site oficial do R fornece as versões atualizadas do software e informações sobre este sofisticado projeto de computação estatística.\nPara baixar o R, usa-se um “CRAN Mirror”, clicando em CRAN (Comprehensive R Archive Network) na margem esquerda, abaixo de Download. O CRAN é central no uso do R: é o local de onde se carrega o software e todos os pacotes necessários para instalar e para expandir o R.\nEm vez de ter um único local, o CRAN é “espelhado” em diferentes locais do mundo. “Espelhado” significa simplesmente que existem versões idênticas do CRAN distribuídas por todo o mundo. É possível baixar o R diretamente da nuvem ou escolher uma origem mais próxima do seu local de atuação. No Brasil, encontram-se várias opções, como a Universidade Federal do Paraná, Fundação Oswaldo Cruz, RJ, Universidade de São Paulo, São Paulo e Universidade de São Paulo, Piracicaba\nApós escolher uma das alternativas acima (pode ser qualquer uma delas) surgirá a página The Comprehensive R Archive Network com as opções para escolher o sistema operacional. Escolha o sitema de acordo com o seu computador (Windows, macOS ou Linux). Ao clicar em uma dessas opções, se o sistema operacional escolhido é o Windows, aparecerá a página R for Windows. Nesta, deve-se clicar em base. No caso de outros sistemas operacionais, seguir as orientações mostradas no site do R.\nClicando em base, haverá um redirecionamento para a a página onde aparece a versão do R para o Windows mais atual. Clique no link que diz Download R-…for Window para baixar o instalador em um diretório do computador, em geral Downloads.\nPara instalar o programa básico, basta executar o instalador R-…-win.exe baixado no diretório. Ao fazer isso, aparece na tela do computador,no canto esquerdo, em baixo, o arquivo salvo. Execute este arquivo com um clique sobre ele. Aparecerá u,a janela perguntando “Deseja permitir que este aplicativo faça alterações no seu dispositivo?”. Clique em Sim. A seguir o instalador pedirá para escolher o Idioma. Selecione Português Brasileiro.\nEm sequência aparecerão informações sobre o diretório no qual o R será instalado em seu computador. Recomenda-se aceitar a configuração padrão sugerida pelo instalador do software.\nA próxima janela pedirá para personalizar os componentes que serão instalados. Recomenda-se usar as configurações sugeridas pelo instalador que irá reconhecer automaticamente a arquitetura do seu sistema Windows (32 e/ou 64 bits).\nA partir daqui, siga as recomendações padrão propostas pelo instalador até completar a instalação, clicando em Concluir.\nO R não precisa ser iniciado, pois o software que será usado, neste livro, é o RStudio. Este, para ser executado, necessita ter o R instalado no computador. Ou seja, o R é o programa “cérebro” necessário para as análises de dados que serão realizadas. Ele precisa estar instalado para permitir o funcionamento do RStudio.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#ambiente-de-desenvolvimento",
    "href": "04-introducaoR.html#ambiente-de-desenvolvimento",
    "title": "4  Introdução ao uso do R",
    "section": "4.2 Ambiente de desenvolvimento",
    "text": "4.2 Ambiente de desenvolvimento\n\n4.2.1 RStudio\nUm ambiente de desenvolvimento integrado (IDE - Integrated Development Environment) é uma ferramenta que facilita a escrita, execução e depuração de código.\nO RStudio é o ambiente de desenvolvimento integrado mais utilizado com o R. Ele serve para facilitar a escrita, execução e depuração de código R, bem como para gerenciar projetos, visualizar dados e criar gráficos. O RStudio é um membro ativo da comunidade R. Foi fundado em 2009 por Joseph J. Allaire, engenheiro de software americano. O RStudio, inspirado pelas inovações dos usuários de R em ciência, educação e indústria, desenvolveu ferramentas gratuitas e abertas para facilitar o uso do R. O RStudio é escrito em linguagem C++ e foi inicialmente focado apenas na linguagem R. Com o tempo o desenvolveu suporte para Python e VSCode. Em 2022, para acompanhar essa mudança, foi anunciada a mudança do nome da empresa que o desenvolve para Posit e, recentemente, introduzido um novo IDE, denominado Positron (Blog 2024), um projeto inicial, com um ambiente semelhante ao RStudio e que continua em desenvolvimento. Talvez, no futuro, possa substituir o RStudio. Por enquanto, isso será difícil , pois o Positron não tem todas as funcionalidades do RStudio (Software 2025).\n\n4.2.1.1 Instalação do R Studio\nPara instalar o RStudio , acessar o site e clicar em Download para obter a versão desejada. Recomenda-se a versão RStudio Desktop – Open Source License que é gratuita. Esta versão entrega as ferramentas integradas para o R.\nA seguir, aparecerão os instaladores disponíveis, conforme a plataforma suportada pelo seu computador. As mais utilizadas são Windows e Mac OS X. Neste livro, como base, serão mostrados os passos para a plataforma Windows1 .\nEm sequência, executar o instalador baixado RStudio-2025.05.1-513.exe 2e seguir as suas instruções.\n\n\n4.2.1.2 Iniciando o RStudio\nPara iniciar o RStudio basta clicar no ícone indicativo (Figura 4.1) que se encontra no menu Iniciar do Windows.\n\n\n\n\n\n\n\n\nFigura 4.1: Ícone do RStudio\n\n\n\n\n\nO RStudio abre como mostrado na Figura 4.2. O RStudio é uma interface mais funcional e amigável para o R. Contém um conjunto de ferramentas integradas projetadas para ajudá-lo a ser mais produtivo com o R.\n\n\n\n\n\n\n\n\nFigura 4.2: Tela inicial do RStudio\n\n\n\n\n\nInclui o Console , editor que suporta execução direta de códigos e uma variedade de ferramentas robustas para plotagem, exibição de histórico, depuração e gerenciamento de seu espaço de trabalho incluídos em uma interface que está, inicialmente, dividida em 3 paineis:\n\nConsole\nEnvironment, History, Connections, Tutorial\nFiles, Plots, Packages, Help\n\nConsole e R Script\nDo lado esquerdo fica o Console (Figura 4.2), em vermelho), onde os comandos podem ser digitados e aparecem os resultados da execução dos comandos. Ao abrir o RStudio , vê-se no Console uma série de informações sobre o R, como versão em uso e, por último, o diretório onde está armazenado o espaço de trabalho (workspace). Estas informações podem ser facilmente apagadas, clicando na barra de ferramentas, no menu Edit, e após em Clear Console ou, usando as teclas Ctrl+L.\nO Console é a principal parte do R. Aqui é onde o R realmente executa o comando. No início do Console, existe um caractere (&gt;). Este é um prompt que informa que o R está pronto para receber um novo código. Pode-se digitar o código diretamente no Console após o prompt e obter uma resposta imediata. Por exemplo, se for digitado 1 + 1 e pressionado Enter, o R imediatamente gera uma saída de 2 (Figura 4.3).\n\n\n\n\n\n\n\n\nFigura 4.3: Console do RStudio\n\n\n\n\n\nRecomenda-se que a maior parte dos comandos sejam digitados no bloco de notas do RStudio , o R Script. Reservar o Console apenas para depurar ou fazer análises e cálculos rápidos. A razão para isso é simples: se o comando for digitado diretamente no Console, ele não será salvo e se for cometido um erro na digitação, haverá necessidade de digitar tudo novamente. Portanto, é melhor escrever os comandos no R Script e, quando estiver pronto para executar, enviar para o Console.\nO R Script é o quarto painel do RStudio e seu bloco de notas. Ele é criado através do menu File &gt; New File &gt; R Script ou clicando no botão verde com o sinal (+), na barra de ferramentas de acesso rápido, na parte superior à esquerda. Ao criar um novo R Script será aberto o painel do bloco de notas (Figura 4.4), em verde).\n\n\n\n\n\n\n\n\nFigura 4.4: Bloco de Notas do RStudio: R Script\n\n\n\n\n\nUm diferencial do RStudio é que os comandos são autocompletáveis. Basta começar a escrever o comando, inserindo 3 ou mais caracteres, por exemplo, summ referente a função summary (), usada para sumarizar um conjunto de dados, e surge um menu de opções, facilitando a digitação (Figura 4.5).\n\n\n\n\n\n\n\n\nFigura 4.5: Menu autocompletável\n\n\n\n\n\nApós digitar no Console, para que seja executado o comando há necessidade de clicar na tecla Enter; no RScript, clicar em Run, acima, na barra, no lado direito, ou usar o atalho Ctrl + Enter. Textos podem ser copiados e colados no script e linhas em branco podem ser inseridas. Além disso, no final da sua sessão, é possível salvar o arquivo, que poderá ser recarregado no futuro, se precisar refazer a análise.\nOs scripts do RStudio são apenas arquivos de texto com a extensão (.R). Quando se cria um R Script, aparece como Sem título (Untitled). Antes de começar a digitar um novo script no RStudio, recomenda-se salvar o atual com um novo nome de arquivo. Dessa forma, se algo no computador falhar durante o trabalho, o código ficará protegido.\nAo digitar o código em um script, o R não executa o código enquanto se digita. Para que o R realmente avalie o código digitado, há necessidade de primeiro enviar o código para o Console, clicando no botão Run ou usando a tecla de atalho Crtl + Enter. Cada linha é marcada no início por um número em sequência.\nAlém da digitação de comandos, o R Script permite fazer comentários onde tudo que for escrito, após o símbolo \\(\\#\\), não é considerado, é apenas uma explicação, um esclarecimento. Os comentários são literais, escritos diretamente para explicar o comando executado. São repetidos na saída do Console sem aparecer nos resultados.\nAmbiente, História, Conexão e Tutorial\nNo lado superior direito há um painel com quatro abas (Figura 4.2), em azul):\n\nAmbiente (Environment) - onde ficam armazenados os objetos criados, as bases de dados importadas, etc., na sessão ativa. É possível visualizar informações como o número de observações e linhas dos bancos de dados ativos. A guia também tem algumas ações clicáveis, como Import Dataset, que permite importar arquivos csv, Excel, SPSS, etc.\nHistória (History) - onde fica o histórico dos comandos executados no Console. Estes comandos podem ser pesquisados nesta guia. Os comandos são exibidos em ordem (mais recentes na parte inferior) e agrupados por bloco de tempo.\nConexões (Connections) - mostra todas as conexões feitas com fontes de dados suportadas e permite saber quais conexões estão ativas no momento. O RStudio suporta múltiplas conexões de banco de dados simultâneas.\nTutorial - a partir da versão 1.3, o R Script ganhou um painel Tutorial dedicado, usado para executar tutoriais que ajudarão você a aprender e dominar a linguagem de programação R. Na primeira vez que se abre o programa, clicando nesta aba, o RStudio solicita que seja instalado o pacote learnr (Figura 4.6)). Isto permite acesso a vários tutoriais úteis que merecem ser explorados\n\n\n\n\n\n\n\n\n\nFigura 4.6: Tutoriais do RStudio\n\n\n\n\n\nArquivos, Gráficos, Pacotes, Ajuda e Apresentação\nNo lado direito, abaixo, existem outras abas muito úteis (Figura 4.2), em amarelo):\n\nArquivos (Files) - esta guia dá acesso ao diretório onde se encontram os seus arquivos. Um bom recurso do painel Files é que se pode usá-lo para definir seu diretório de trabalho. Para isso, clique em More e depois em Set As Working Directory.\nGráficos (Plots) - local onde ficam os gráficos gerados. Existem botões para abrir o gráfico em uma janela separada e exportar o gráfico como um .pdf ou .jpeg.\nPacotes (Packages) - mostra uma lista de todos os pacotes R instalados no seu computador e indica se eles estão atualmente carregados ou não. Pacotes que estão sendo executados na sessão atual, estão marcados, enquanto aqueles que estão instalados, mas ainda inativos, estão desmarcados.\nAjuda (Help) - menu de ajuda para as funções R. Você pode digitar o nome de uma função na janela de pesquisa (por exemplo, histogram ou usar o ?hist), no Console ou no R Script, para procurar ajuda sobre uma função (Figura 4.7)). A Ajuda no R Studio pode também ser acessada no menu Help da barra de ferramentas onde existem várias opções. Para complementar, alguns livros são muito uteis, como o R Cookbook (Chang 2021) ou Using R* for introductory statistics* (Verzani 2004). No entanto, na maioria das vezes a forma mais prática de conseguir ajuda com uma dúvida específica é a busca em fóruns na internet, como o Stack Overflow: https://stackoverflow.com/.\nApresentação (Presentation) – é visualizador de apresentações. Nas últimas versões do Rstudio, é possível com o Quarto, editar um código em R Markdown para construir uma apresentação. Não faz parte do objetivo deste livro desenvolver este assunto. É possível encontrar um tutorial em https://quarto.org/docs/get-started/hello/rstudio.html.\n\n\n\n\n\n\n\n\n\nFigura 4.7: Ajuda do RStudio\n\n\n\n\n\n\n\n\n4.2.2 Pacotes\nPara que o R possa interagir com o usuário, realizar análises estatísticas e gerar gráficos, a instalação de pacotes é essencial.\nUm pacote é um conjunto de funções, dados e documentação que amplia os recursos do R base. O uso de pacotes é fundamental para explorar todo o potencial da ferramenta, sendo sua instalação orientada pelas demandas específicas de cada projeto. Ao instalar o R básico, diversos pacotes já vêm incluídos, permitindo uma ampla gama de análises. No entanto, à medida que o uso do R se aprofunda, torna-se necessário instalar pacotes adicionais desenvolvidos pela comunidade, que oferecem funcionalidades extras por meio de novas funções e comandos.\n\n4.2.2.1 Repositório de pacotes\nQuando se identifica a necessidade de um novo pacote, é fundamental saber onde ele se encontra. O principal repositório de pacotes é o CRAN (Comprehensible R Archive Network), já comentado anteriormente. Para acessar este repositório, use o link e escolha um espelho (0-Cloud ou o mais próximo geograficamente). Depois que o pacote for instalado, ele será mantido em sua biblioteca (library) R associada à sua versão principal atual do R. Haverá necessidade de atualizar e reinstalar os pacotes sempre que atualizar uma versão principal do R.\nEstando na página do CRAN, no menu, à esquerda, clique em Packages . Isto o colocará na página dos Contributed Packages, onde a maioria dos pacotes podem ser encontrados em Table of available packages, sorted by name . Também é possível clicar em CRAN Task Views , onde estão os pacotes separados por tópicos.\n\n\n4.2.2.2 Instalação de um novo pacote\nInstalar um pacote significa simplesmente baixar o código do pacote em um computador pessoal. Existem duas maneiras principais de instalar novos pacotes. O método mais comum é baixá-los do CRAN, usando a função install.packages(). Dentro dos parênteses, como argumento, coloca-se entre aspas (duplas ou simples) o nome do pacote. Como visto, deve-se, de preferência, digitar o comando no R Script. Por exemplo, para instalar o pacote ggplot2, usado para trabalhar gráficos, se procede da seguite maneira:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nPara carregar o pacote, isto é, para fazer com que suas funções se tornem ativas para uso na na sessão, deve-se usar a função library(), como mostrado no comando acima.\nSe o RStudio for fechado e reaberto, o pacote deverá ser novamente ativado. Observe que a função library() não requer que o nome do pacote seja digitado entre aspas. Isto acontece porque antes de o pacote ser instalado o R não o reconhece , portanto, há necessidade de indicar o nome (caracteres), para que o R procure na internet o que deve deve baixar. Já, depois de instalado, o pacote é um objeto conhecido pelo R, logo as aspas não são mais necessárias.\nUma outra maneira de instalar pacotes no R, é usar o botão Install, localizado na aba Packages, no painel inferior, à direita. Clicando em Install, abre-se a caixa de diálogo da Figura 4.8. Digitar em Packages o nome do pacote (ggplot2) e o RStudio completará com opções para achar o pacote. Clicar em ggplot2 e verifique se Install dependencies foi selecionado. A seguir clicar em Install e aguardar aparecer no Console a mensagem que o pacote foi instalado com sucesso.\n\n\n\n\n\n\n\n\nFigura 4.8: Instalação do pacote ‘ggplot2’ usando a caixa de diálogo ‘Install Packages’\n\n\n\n\n\n\n\n4.2.2.3 Atualização dos pacotes\nPeriodicamente, há necessidade de atualizar os pacotes instalados. Essa necessidade advém do fato que, com o tempo, os autores de pacotes lançarão novas versões com correções de defeitos e novos recursos e, geralmente, é uma boa ideia manter-se atualizado. Para realizar a atualização, use a função update.packages(), colocando o nome do pacote entre aspas, por exemplo, update.packages(\"ggplot2).\n\n\n4.2.2.4 Instalando e carregando mais de um pacote\nUma das funções, atualmente, mais usadas para executar essa ação é fornecida pelo pacote pacman (Rinker e Kurkiewicz 2018).\nÉ interessante configurar o pacman para que ele esteja sempre disponível assim que o R é aberto. Isso é feito editando o arquivo chamado .Rprofile, que é carregado automaticamente toda vez que você inicia uma sessão do R. Para configurar o pacman no .Rprofile, abra-o no RStudio:\n\nfile.edit(\"~/.Rprofile\")\n\nAdicione o seguinte código:\n\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n  install.packages(\"pacman\")\n}\nlibrary(pacman)\n\nEsse trecho garante que:\n• Se o pacman não estiver instalado, ele será instalado automaticamente.\n• Depois, ele será carregado para uso imediato.\nApós esse procedimento, salve o .Rprofile e feche o arquivo. Reinicie o R para que a configuração entre em vigor.\nO pacman instala e carrega um ou mais pacotes, através da sua função p_load() da seguinte maneira, não havendo necessidade de escrever o nome dos pacotes entre aspas:\n\npacman::p_load(readxl, dplyr, ggplot2, car)\n\nAlém da função p_load(), o pacote pacman tem outas funções, entre elas a função p_update() que atualiza o pacote e , se usada sem especificar o pacote , atualiza todos. Para saber mais sobre o pacote pacman, use a ajuda.\n\n\n4.2.2.5 Citação de pacotes em publicações\nNo R existe um comando que mostra como citar o R ou um de seus pacotes. Basta digitar a função citation() no Console ou no R Script e observar a saída. Para um pacote específico, basta colocar o nome do pacote entre aspas, na função.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation (\"ggplot2\")\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }\n\n\n\n\n\n4.2.3 Diretório de Trabalho\nO diretório de trabalho (Working Directory) é uma pasta onde o R lê e salva arquivos. Deve-se criar um diretório de trabalho para a sessão . Para isso, no RStudio siga o caminho: Session &gt; Set Working Directory &gt; Choose Directory ou use o atalho Ctrl + Shift + H e escolha o diretório desejado ou crie um novo.\nAo finalizar, aparecerá no Console (Figura 4.9):\n\n\n\n\n\n\n\n\nFigura 4.9: Diretório de trabalho\n\n\n\n\n\nNote que o R usou a função setwd() que significa “definir diretório de trabalho”. Também é possível usar esta função diretamente no R Script ou no Console, digitando conforme o caminho do diretório.\nPara saber qual é o diretório de trabalho que está sendo usado pelo R, pode-se executar a função getwd(). A saída no Console mostrará o diretório de trabalho usado, portanto é recomendado que se faça isso no início da sessão para verificar se há ou não necessidade de modificar o diretório.\n\n\n4.2.4 Projeto\nUma funcionalidade importante do RStudio é a possibilidade de se criar projetos. Um projeto nada mais é do que uma pasta no seu computador. Nessa pasta, estarão todos os arquivos que serão usados ou criados na sua análise.\nA principal razão de se utilizar projetos é simplesmente organização. Com eles, fica muito mais fácil importar conjunto de dados para dentro do R, criar análises reprodutíveis e compartilhar o trabalho realizado.\nAo se começar uma nova análise, é interessante criar um Novo Projeto. Para isso, clicar File &gt; New Project ou clicar no menu que está na parte superior, à direita, Project (none) &gt; New Project…. Abrirá a janela da Figura 4.10).\n\n\n\n\n\n\n\n\nFigura 4.10: Assistente de novo projeto\n\n\n\n\n\nClique em New Directory para criar um novo diretório. Por exemplo, para as aulas de Bioestatística, pode-se criar um diretório com o nome de bioestatistica ou qualquer outro nome3.\nQuaisquer documentos Excel ou arquivos de texto associados podem ser salvos nesta nova pasta e facilmente acessados, indo ao menu Project (none) &gt; Open Project…. A partir daí, é possível realizar análises de dados ou produzir visualizações com seus dados importados.\nQuando um projeto estiver aberto no RStudio, o seu nome aparecerá no canto superior direito da tela.\nNa aba Files, aparecerão todos os arquivos contidos no projeto. Quando se clica no nome do projeto, abre um menu que torna muito fácil a navegação pelos projetos existentes. Basta clicar em qualquer um deles para trocar de projeto, isto é, deixar de trabalhar em uma análise e começar a trabalhar em outra.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#princípios-básicos-de-uso-do-r",
    "href": "04-introducaoR.html#princípios-básicos-de-uso-do-r",
    "title": "4  Introdução ao uso do R",
    "section": "4.3 Princípios básicos de uso do R",
    "text": "4.3 Princípios básicos de uso do R\n\n4.3.1 R como calculadora\nO R pode ser usado como uma calculadora desde as mais simples até as mais complexas. Para isso, basta digitar as equações no Console ou no R Script, usando os operadores:\n\n4.3.1.1 Operadores\nOperadores são usados para realizar operações com variáveis e valores.\nOperadores aritméticos\nNo R, você pode usar operadores aritméticos para realizar operações matemáticas comuns.\n\n 10 + 5        # Adição\n\n[1] 15\n\n 10 - 5        # Subtração\n\n[1] 5\n\n 10 * 5        # Multiplicação\n\n[1] 50\n\n 10 / 5        # Divisão\n\n[1] 2\n\n 10 ^ 5        # Potência\n\n[1] 1e+05\n\n 10 %% 3       # Divisão modular (divisão com resto)\n\n[1] 1\n\n 10 %/% 3      # Divisão inteiro\n\n[1] 3\n\n\nObserve que o R repete a operação e coloca em baixo o resultado precedido por [1]. O resultado da operação de exponenciação é exibido como notação científica, onde \\(e+05\\) significa \\(10^5\\).\nOperadores de atribuição\nOperadores de atribuição são usados para atribuir valores a variáveis, como será visto na Seção 4.3.2, adiante.\nOperadores de comparação\nSão usados para comparar dois valores.\n\n# Igualdade\n3 == 3\n\n[1] TRUE\n\n3 == 4\n\n[1] FALSE\n\n# Não igual (diferente)\n3 != 4\n\n[1] TRUE\n\n# Maior\n6 &gt; 3\n\n[1] TRUE\n\n# Menor\n3 &lt; 4\n\n[1] TRUE\n\n# Maior ou igual\n5 &gt;= 3\n\n[1] TRUE\n\n# Menor ou igual  \n3 &lt;= 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nAtenção\n\n\n\nNa linguagem R, o sinal de igualdade é escrito com duplo \\(=\\).\n\n\nOperadores lógicos\nOperadores lógicos são usados para combinar declarações condicionais:\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros \n6 == 6 & 7 == 8\n\n[1] FALSE\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros\n2 * 3 && 1 * 6\n\n[1] TRUE\n\n# Conjunção lógica OU, retorna TRUE se um dos elementos é verdadeiro\n(2 * 2) | sqrt(16)\n\n[1] TRUE\n\n6 == 6 | 7 == 8 \n\n[1] TRUE\n\n# Conjunção lógica NÃO, retorna FALSE se o  elemento é verdadeiro\n!6==6\n\n[1] FALSE\n\n!2==4\n\n[1] TRUE\n\n# Operador lógico que verifica se um elemento pertence a um conjunto (%in%)\n\n pares &lt;- c(0, 2, 4, 6, 8, 10)\n 5 %in% pares\n\n[1] FALSE\n\n\nOutros operadores\n\n# Logarítmo natural (base e)\nlog (10) \n\n[1] 2.302585\n\n# Logarítmo base 10\nlog10 (10)       \n\n[1] 1\n\n# Raiz quadrada\nsqrt (81)\n\n[1] 9\n\n# Resultado absoluto\nabs (3 - 6)\n\n[1] 3\n\n\n\n\n\n4.3.2 Objetos\nO R permite salvar valores dentro de um objeto. Os objetos são criados utilizando o operador de atribuição (&lt;-). Para digitar este operador, basta teclar o sinal menor que (&lt;), seguido de hífen (-) , sem espaços. Existe um atalho que é pressionar (Alt) \\(+\\) (-). O símbolo \\(=\\) pode ser usado no lugar de &lt;-.\nObjeto é um pequeno espaço na memória do computador onde o R armazenará um valor ou o resultado de um comando, utilizando um nome arbitrariamente definido. Tudo criado pelo R pode se constituir em um objeto, por exemplo: uma variável, uma operação aritmética, um gráfico, uma matriz ou um modelo estatístico. Através de um objeto torna-se simples acessar os dados armazenados na memória. Ao criar um objeto, se faz uma declaração. Isto significa que se está afirmando que uma determinada operação aritmética irá, agora, tornar-se um objeto que irá armazenar um determinado valor. As declarações são feitas uma em cada linha do R Script.\nOs objetos devem receber um nome e é obrigatório que ele comece por uma letra (ou um ponto) e não é permitido o uso do hífen. Pode-se usar o ponto ou underlines para separar palavras. Deve ser evitado o uso de nomes que sejam de objetos do sistema, ou outros objetos já criados, funções ou constantes. Por exemplo, não deve ser utilizado: c, q, r, s, t, C, D, F, I, T, diff, exp, log, mean, pi, range, rank, var, NA, NaN, NULL, FALSE, TRUE, break, else, if, break, function, in, while que devem ser reservados, pois têm significados especiais.\nQuando se usa um objeto com o nome pi, ele assumirá outro valor diferente de 3,141593. Preservando este nome, toda vez que usarmos a palavra pi, o R assume o valor pré-estabelecido. Além disso, o R faz a diferença entre letras maiúsculas e minúsculas. Ou seja, soma é um objeto diferente de Soma e ambos são diferentes de SOMA.\nPara exibir o conteúdo de um objeto, basta digitar seu nome no R Script ou no Console e executar. Em análises mais extensas, verificar se já há um objeto com o mesmo nome, pois seus valores serão substituídos ao executar o novo objeto. Para saber se já existe um objeto com o nome definido, digite as primeiras letras do objeto criado e o R Studio listará, usando a sua função de autocompletar, tudo que começar com essas letras no arquivo. Assim ficará fácil verificar se já existe um objeto com o nome desejado.\nNo comando abaixo, é criado um objeto que receberá a soma de dez números, utilizando a função sum(). O objeto foi denominado de soma. Para exibir o valor contido no objeto soma, é necessário digitar soma no R Script ou Console e executar:\n\nsoma &lt;- sum (2, 3, 12, 15, 21, 4, 8, 7, 13, 21)\nsoma\n\n[1] 106\n\n\n\n4.3.2.1 Atributos de dados inseridos nos objetos\nOs dados inseridos nos objetos pertencem as seguintes classes 4:\n\nCaracteres (character): são categóricos ou qualitativos. Em geral, são carcteres ou texto. São escritos entre aspas simples ou duplas e não são modificáveis.\nNumérico (numeric): são números inteiros (integer) ou decimais ou ponto flutuante (dbl = double). A sintaxe para o número inteiro é 42L, 3L[^04-introducaor-5], pois se fosse escrito como 42, o R automaticamente o trata como um número flutuante (double). [^04-introducaor-5]: O L vem da linguagem C, onde long é um tipo inteiro. É uma convenção que o R herdou\nLógico (logical): são usados para combinar declarações condicionais. Retorna: verdadeiro (TRUE) ou falso (FALSE). São escritos obrigatoriamente com letras maiúsculas sem aspas. São utilizados para indicar opções onde há apenas duas opções ou como resultado de um teste lógico.\n\nÉ a partir do conhecimento do tipo de classe que as funções sabem o que extamente fazer com um objeto. Por exemplo, não é possivel somar duas letras e se for feita a tentativa de somar “a” e “b”, o Rretorna um erro: Error in “a” + “b”: non-numeric argument to binary operator.\nNo R, os textos são escritos entre aspas simples ou duplas. As aspas servem para diferenciar nomes (objetos, funções, pacotes) de textos (letras e palavras). Os textos são muito comuns em variáveis categóricas e são popularmente chamados de strings ou character. Além desta classe, o R tem outras classes básicas que são a numeric e a logical. Um objeto de qualquer uma dessas classes é chamado de objeto atômico. Esse nome se deve ao fato de essas classes não se misturarem (Damiani et al. 2015).\nPara saber qual o tipo de classe que um objeto pertence, basta usar a função class().\n\nidade &lt;- c(3, 5, 7, 9, 6, 7)\nclass (idade)\n\n[1] \"numeric\"\n\nnome &lt;- c(\"Pedro\", \"Maria\", \"Margarida\", \"Alice\", \"João\", \"Luís\")\nclass(nome)\n\n[1] \"character\"",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#sec-funcoes",
    "href": "04-introducaoR.html#sec-funcoes",
    "title": "4  Introdução ao uso do R",
    "section": "4.4 Funções",
    "text": "4.4 Funções\nA função é uma orientação ao R para que ele execute uma ação que é algum procedimento específico. Em decorrência, em geral, uma função ttem um nome sugestivo da ação que ela realiza. Por exemplo, a função mean () realiza a média aritmética de uma série de números. O resultado, como regra geral, deve ser colocado em um objeto que será armazenado na memória do computador.\nEsta série de números, concatenados na função c(), é armazenada por um objeto, nomeado dadose, posteriormente, se usa a função mean()com este objeto dados. O resultado da função mean, exibido no Console, será recebido por outro objeto media_dados e colocado na memória do computador.\n\ndados &lt;- c(3, 5, 7, 9, 6, 7)\nmedia_dados &lt;- mean(dados)\nmedia_dados\n\n[1] 6.166667\n\n\nDe acordo com as necessidades pode-se criar funções pessoais, customizadas. Entretanto, na maioria das vezes, elas são encontradas prontas, fazendo parte de um pacote. Pacotes contêm muitas funções que para serem executadas necessitam que estes estejam instalados e carregados. As funções para exercerem a sua ação devem receber dentro delas (entre parênteses) os argumentos que elas exigem. Os argumentos de uma função são sempre separados por vírgulas.\nPara se saber quais argumentos necessários para uma determinada função basta consultar a ajuda, onde se encontrará a documentação da mesma. Para isso basta digitar no Console, no caso da função mean(), help(mean) ou ?mean:\n\nhelp(mean)\n\nO resultado deste comando aparecerá na aba Help, na parte inferior, à direita (Figura 4.11):\n\n\n\n\n\n\n\n\nFigura 4.11: Ajuda para Média Aritmética\n\n\n\n\n\nOs principais argumentos da função mean() são:\n\nx \\(\\to\\) vetor numérico\ntrim \\(\\to\\) fração das observações (varia de 0 a 0,5) extraída de cada extremidade de x para calcular a média aparada\nna.rm \\(\\to\\) valor lógico (TRUE ou FALSE) que indicam se os valores ausentes (NA) devem ser removidos antes que o cálculo continue\n\nEste último argumento é muito importante quando, na sequência de valores existe algum não informado ou inexistente. No R, eles são denominados de valores ausentes (missing values) e denotados por NA (Not Available).\nPor exemplo, em uma coleta de uma série de valores, correspondentes ao peso de 15 recém-nascidos, havendo a “falta” de um dos registros, ao calcular a média com a função mean(), ela retornará NA.\n\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,NA,3970,3060,3180,  \n             2865,2815,3245,2051,2630)\nmean (pesoRN)\n\n[1] NA\n\n\nColocando o argumento na.rm = TRUE, para remover os valores faltantes, a função retornará a média aritmética sem este valor:\n\nmean (pesoRN, na.rm = TRUE)\n\n[1] 3227.929\n\n\n\n4.4.1 Criando funções\nNo R, é possível criar funções pessoais que podem simplificar um código e, eventualmente, diminuir o tempo de execução das análises.\n\n4.4.1.1 Fórmula geral\nAs funções têm uma fórmula geral:\n\nnome_da_funcao &lt;- function (x){transformar x}\n\nPor exemplo, a área de um circulo é igual a \\(\\pi\\times raio^2\\). Uma função pode otimizar o cálculo da área:\n\narea.circ &lt;- function(r){\n  area &lt;- pi*r^2\n  return(area)                \n}\n\nOu seja, foi usada a função function(), com o raio do círculo como argumento. A seguir, entre chaves {}, coloca-se a ação que a função realizará, no caso o cálculo da área do círculo. O resultado deste cálculo (pi*r^2) é recebido por um objeto denominado area.5 A seguir, usou-se a função return () para retornar o resultado do cálculo realizado.\nAo executar essa função, é possível usá-la para calcular a área de um círculo, cujo raio é igual a 5 cm:\n\narea.circ(5)\n\n[1] 78.53982\n\n\n\n\n4.4.1.2 Outros exemplos\nO Indice de Massa Corporal é igual ao peso (kg) dividido pela \\(altura^2\\), em metros. Uma função para fazer este cálculo é:\n\nimc &lt;- function(peso, altura){\n  res &lt;- peso/altura^2\n  return(res)\n}\n\nLogo, o IMC de um indivíduo que tenha 67 kg e 1,7 m é:\n\npeso &lt;-  67\naltura &lt;-  1.70\nimc(67, 1.70)\n\n[1] 23.18339\n\n\nOs exemplos mostrados são muito simples. Quase não haveria necessidade de construir uma função. Entretanto, quando se tem uma ação mais complexa, a função mostra a sua utilidade. Por exemplo, se for necessário realizar a comparação entre duas médias, usando um teste t e apresentar o resultado junto com boxplots, a função fica mais complexa. Sempre que for necessário cálculo semelhante, a função automatiza a ação, sem necessidade de repetir os códigos:\n\nplotBpT &lt;- function(df, var.x, var.y){\n  library(ggplot2)\n  library(ggpubr)\n  ggplot(df, aes(x = {{var.x}}, y = {{var.y}}, fill = {{var.x}})) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1) +\n    geom_boxplot() +\n    theme_classic() +\n    theme(legend.position = \"none\") +\n    stat_compare_means(method = \"t.test\", label.x = 0.5)\n}\n\nNeste momento, não serão discutidos os códigos da função. Ela será utilizada como uma função qualquer com os dados do arquivo dadosPop.xlsx^[Para maiores detalhes, consulte a o Capítulo 14. Os argumentos da função são o dataframe (df = dados), a variável x (var.x = pop) e a variável y (var.y = altura). Dessa forma, está se comparando a altura de mulheres de duas populações de duas regiões diferentes :\n\ndados &lt;- readxl::read_excel(\"dados/dadosPop.xlsx\")\ndados$pop &lt;- as.factor(dados$pop)\nplotBpT (df = dados, var.x = pop, var.y = altura)\n\n\n\n\n\n\n\nFigura 4.12: Comparação da altura de mulheres em duas populações\n\n\n\n\n\nObserve na Figura 4.12 o resultado do teste t (\\(P = 2,2 \\times 10^-16\\)) e os boxplots que têm posições bem diferentes (veja Seção 8.6).\n\n\n4.4.1.3 Ativação de uma função criada\nPara ativar uma função previamente criada, usa-se a função nativa source (). O argumento desta função é o caminho (no exemplo, é o diretório do autor) onde se encontra a função buscada, por exemplo, a função imc() criada acima:\n\nsource('C:/Users/petro/Dropbox/Estatistica/Bioestatistica_usando_R/Funcoes/imc.R')",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#sec-vetores",
    "href": "04-introducaoR.html#sec-vetores",
    "title": "4  Introdução ao uso do R",
    "section": "4.5 Vetores",
    "text": "4.5 Vetores\nOs vetores (vector) são objetos que armazenam um ou mais elementos, do tipo numeric, character ou logical. Um vetor é uma variável com um ou mais valores do mesmo tipo. Por exemplo, o número de filhos em 10 famílias foi 4, 5, 3, 2, 2, 1, 2, 1, 3 e 2. O vetor nomeado de n.filhos é um objeto numérico. Todo vetor tem duas características: comprimento e nomes dos elementos que podem ser encontradas com as funções length() e names, repectivamente. A maneira mais fácil de criar um vetor é concatenar (ligar) os 10 valores, usando a função concatenar c():\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\nn.filhos\n\n [1] 4 5 3 2 2 1 2 1 3 2\n\nlength(n.filhos)\n\n[1] 10\n\nnames(n.filhos)\n\nNULL\n\n\nComo não há nomes no objeto n.filhos, a saída é NULL\n\n4.5.1 Indexação de vetores\nComo os vetores são conjuntos indexados, pode-se dizer que cada valor dentro de um vetor tem uma posição. Essa posição é dada pela ordem em que os elementos foram colocados no momento em que o vetor foi criado. Isso nos permite acessar individualmente cada valor de um vetor (Damiani et al. 2015).\nPara acessar um determinado valor, basta colocar a posição do mesmo entre colchetes [ ]. Se há interesse em conhecer o número de filhos da quinta família, procede-se da seguinte forma:\n\nn.filhos[5]\n\n[1] 2\n\n\nSe houver tentativa de acessar um valor inexixtente, o R retorna NA.\n\nn.filhos[11]\n\n[1] NA\n\n\nSe houver necessidade de excluir um dos elementos, basta colocar entre colchetes a posição do mesmo com sinal negativo. Por exemplo, para excluir o valor correspondente a sexta família, usa-se:\n\nn.filhos[-6]\n\n[1] 4 5 3 2 2 2 1 3 2\n\n\nObserva-se que o valor 1 foi excluído da série de elementos.\nQuando são colocados elementos em um vetor que pertençam a classes diferentes, o R promove o que se denomina de coerção, pois o vetor pode ter apenas uma classe de objeto. Dessa forma, as classes mais fortes reprimem as mais fracas. Por exemplo, sempre que for misturado números e texto em um vetor, os números serão considerados como texto:\n\nvetor &lt;- c(12, 15, 4, 6, \"A\", \"D\")\nvetor\n\n[1] \"12\" \"15\" \"4\"  \"6\"  \"A\"  \"D\" \n\n\nVeja que, agora, todos os elementos do vetor passaram a ser textos e, por isso, estão entre aspas.\n\n\n4.5.2 Tipos de vetores\nDado um vetor, pode-se determinar seu tipo com typeof(), ou verificar se é um tipo específico com uma das funções: is.character(), ’is.double(),is.integer(),is.logical( )`.\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\ntypeof(n.filhos)\n\n[1] \"double\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\n\nAs expressões do tipo character devem aparecer entre aspas duplas ou simples. Os números no R são geralmente tratados como objetos numéricos (números reais de dupla precisão). Mesmo números inteiros são tratados como numéricos. Para fazer um número inteiro ser tratado como objeto inteiro, deve-se utilizar a letra L após o número.\nOs valores lógicos (ou booleanos) são TRUE ou FALSE. T ou F também são aceitos.\n\nn.filhos &lt;- c(4L, 5L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L)\ntypeof(n.filhos)\n\n[1] \"integer\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\nis.double(n.filhos)\n\n[1] FALSE\n\n\n\nnomes &lt;- c('Maria', 'João', 'Manuel', 'Petronio', 'José')\ntypeof(nomes)\n\n[1] \"character\"\n\nis.numeric(nomes)\n\n[1] FALSE\n\nis.double(nomes)\n\n[1] FALSE\n\n\n\naltura &lt;- c(1.60, 1.78, 1.55, 1.67, 1.69)\ntypeof(altura)\n\n[1] \"double\"\n\nis.numeric(altura)\n\n[1] TRUE\n\nis.double(altura)\n\n[1] TRUE",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#sec-fatores",
    "href": "04-introducaoR.html#sec-fatores",
    "title": "4  Introdução ao uso do R",
    "section": "4.6 Fatores",
    "text": "4.6 Fatores\nA classe factor serve para designar categorias para um vetor. Essa classe é semelhante a um vetor da classe character, mas tem importancia maior na modelagem estatística e análise exploratóriade dados (Wickham e Grolemund 2017). Tanto números quanto caracteres podem ser convertidos em fatores usando a função factor(). Esses fatores podem não ser ordenados representado apenas diferentes categorias ou podem representar categorias ordenadas. A função ordered() gera fatores ordenados, em ordem crescente ou descrescente.\nO vetor a seguir representa o tipo de parto ocorrido em 15 anscimentos, ond 1 = parto normal e 2 = parto cesáreo:\n\ntipoParto &lt;- c (1,1,2,1,2,2,1,2,1,1,1,2,1,1,1)\ntipoParto\n\n [1] 1 1 2 1 2 2 1 2 1 1 1 2 1 1 1\n\n\nVerificando a classe desse objeto:\n\nclass(tipoParto)\n\n[1] \"numeric\"\n\n\nComo é númerico, teoricamente, seria possível realizar operações matemáticas com ele. Entretanto, sabe-se que os números 1 e 2, representam categorias e, consequentemente, devem ser transformados em fatores, usando a função factor():\n\ntipoParto &lt;- factor(tipoParto, \n                    levels = c(1,2),\n                    labels = c(\"normal\",\"cesareo\"))\ntipoParto\n\n [1] normal  normal  cesareo normal  cesareo cesareo normal  cesareo normal \n[10] normal  normal  cesareo normal  normal  normal \nLevels: normal cesareo\n\nclass(tipoParto)\n\n[1] \"factor\"\n\n\nAgora, a variável tipoParto é um fator. Se for utilizada a função table() para ver o número de ocorrências de cada tipo de parto, tem-se:\n\ntable(tipoParto)\n\ntipoParto\n normal cesareo \n     10       5",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#matrizes",
    "href": "04-introducaoR.html#matrizes",
    "title": "4  Introdução ao uso do R",
    "section": "4.7 Matrizes",
    "text": "4.7 Matrizes\nUma matriz é uma estrutura bidimensional (linhas e colunas) que contém apenas um tipo de dado — geralmente números ou caracteres. É ideal para operações matemática e álgebra linear.\nUma matriz pode ser criada com a função matrix():\n\nm &lt;- matrix(data = 1:6, nrow = 2, ncol = 3)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nA indexação de uma matriz é feita com [linha, coluna], por exemplo:\n\nm[1,2]  # retorna o valor da primeira linha, segunda coluna\n\n[1] 3",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#listas",
    "href": "04-introducaoR.html#listas",
    "title": "4  Introdução ao uso do R",
    "section": "4.8 Listas",
    "text": "4.8 Listas\nUma lista é uma estrutura flexível e heterogênea que pode conter elementos de tipos diferentes, como vetores, strings, funções, até outras listas.\n\nCada elemento da lista pode ter um nome que pode ser acessado com $ ou [[]]. É muito usado para estruturas complexas, como resultados de modelos estatísticos.\nUma lista pode ser criada com a função `list()`` :\n\nlista &lt;- list(aluno = \"Gabriel\", idade = 14, notas = c(9.5, 8.7, 10, 9.1))\nlista\n\n$aluno\n[1] \"Gabriel\"\n\n$idade\n[1] 14\n\n$notas\n[1]  9.5  8.7 10.0  9.1\n\n\nSe o interesse é saber as notas:\n\nlista$notas\n\n[1]  9.5  8.7 10.0  9.1\n\n\nÉ possível calcular a média das notas, usando a função mean():\n\nmean(lista$notas)\n\n[1] 9.325\n\n\n\n\n\n\n\n\nBlog, The Jumping Rivers. 2024. «Positron vs rstudio – is it time to switch?» R-bloggers. https://www.r-bloggers.com/2024/12/positron-vs-rstudio-is-it-time-to-switch/.\n\n\nChang, Winston. 2021. «Cookbook for R». Cookbook for R. http://www.cookbook-r.com.\n\n\nDamiani, Athos, Beatriz Milz, Caio Lente, e et al. 2015. «Ciência de Dados em R». R6 Consultoria. https://livro.curso-r.com/index.html.\n\n\nRinker, Tyler W., e Dason Kurkiewicz. 2018. pacman: Package Management for R. Buffalo, New York. http://github.com/trinker/pacman.\n\n\nSoftware, Posit. 2025. «Frequently asked questions». Positron. Posit Software, PBC. https://positron.posit.co/faqs.html.\n\n\nVerzani, John. 2004. Using R for introductory statistics. Chapman; Hall/CRC.\n\n\nWickham, Hadley, e Garrett Grolemund. 2017. «15 Factors|R for data science». Welcome | R for Data Science. O’Reilly. https://r4ds.had.co.nz/factors.html.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "04-introducaoR.html#footnotes",
    "href": "04-introducaoR.html#footnotes",
    "title": "4  Introdução ao uso do R",
    "section": "",
    "text": "A instalação para Mac OS X pode ser facilmente obtida em busca do Google. Depois de instalado, o uso do RStudio não difere do Windows↩︎\nDisponível em 16/06/2025↩︎\nEvite acentos, maiúsculas ou caracteres especiais. Seja simples e objetivo, usando nomes que estejam relacionados com o assunto.↩︎\nAtributos de um objeto.↩︎\nFoi usado o nome area sem acentuação, mas poderia ser qualquer nome.↩︎",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introdução ao uso do R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html",
    "href": "05-manipulandoDados.html",
    "title": "5  Manipulando dados no R",
    "section": "",
    "text": "5.1 Dataframes no R\nDataFrames são objetos de dados genéricos em formato tabular, onde os dados são organizados de maneira lógica em linha-e-coluna semelhante ao de uma planilha do Excel. O dataframe é uma estrutura bidimensional. Os DataFrames podem ser formados com objetos criados previamente, desde que tenham o mesmo comprimento (Zuur, Ieno, e Meesters 2009).\nUma das formas de criar um DataFrame no R é a partir de um conjunto de vetores como, por exemplo, este relacionados a 15 nascimentos em uma determinada maternidade:\nid &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,3380,3970,3060,3180,  \n             2865,2815,3245,2051,2630)  \ncompRN &lt;- c (50,48,52,48,50,51,50,51,47,47,47,49,51,50,44)\nsexo &lt;- c (2,2,2,1,1,1,2,1,1,1,2,2,1,1,2)\ntipoParto &lt;- c (1,1,2,1,2,2,1,2,1,1,1,2,1,1,1)\nidadeMae &lt;- c (40,19,26,19,32,24,27,20,21,19,23,36,21,23,23)\nEste grupo de vetores (variáveis) isolados fica difícil de manusear. Portanto, seria útil reuni-los em um só objeto. Pode-se fazer isso, usando a função data.frame(), do R base. Este DataFrame será atribuído a um novo objeto de nome dadosNeonatos.\ndadosNeonatos &lt;- data.frame (id,\n                             pesoRN, \n                             compRN, \n                             sexo, \n                             tipoParto, \n                             idadeMae)\nVerificando a classe deste novo objeto, tem-se:\nclass (dadosNeonatos)\n\n[1] \"data.frame\"\nPara observar a modificação realizada, pode-se usar a função str() do R base, digitando no R Script:\nstr(dadosNeonatos)\n\n'data.frame':   15 obs. of  6 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : num  2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: num  1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\nNa saida da função, verifica-se que o dataframe contém 15 linhas e 6 colunas.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-dataframes",
    "href": "05-manipulandoDados.html#sec-dataframes",
    "title": "5  Manipulando dados no R",
    "section": "",
    "text": "5.1.1 Acrescentando variáveis a um dataframe\nSerá adicionada ao dataframe uma nova variável chamada utiNeo, que indica se cada recém-nascido foi encaminhado ou não para a UTI neonatal logo após o nascimento. Essa variável será construída a partir de um vetor contendo a situação de cada um dos 15 recém-nascidos, e será incorporada como uma nova coluna no dataframe. A sintaxe utilizada para essa operação segue o padrão , resultando em: nome-do-dataframe$nome-da-variável.\n\ndadosNeonatos$utiNeo &lt;- c (\"não\",\"não\",\"não\",\"não\",\"sim\",\"não\",\"sim\",\"não\",\"não\",\"não\",\"não\",\"sim\",\"não\",\"não\",\"não\")\n\nCom isso, ´utiNeopassa a fazer parte do conjunto de dados, permitindo análises específicas sobre a necessidade de cuidados intensivos neonatais. A funçãostr()`, pode ser usada, novamente, para observar a transformação:\n\nstr(dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : num  2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: num  1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr  \"não\" \"não\" \"não\" \"não\" ...\n\n\n\n\n5.1.2 Transformação de variáveis\nObserva-se que todas as variáveis estão como variáveis numéricas (num), exceto a variável ´utiNeoquestá como caractere (chr). Isto não está correto, pois as variáveissexo,tipoPartosão variáveis categóricas, bem como a  a variávelutiNeo, adicionada posteriormente. Elas necessitam ser transformadas para fatores, usando a funçãofactor()`. Os principais argumentos dessa função são:\n\nx \\(\\to\\) vetor numérico\nlevels \\(\\to\\) vetor opcional dos valores que x pode assumir\nlabels \\(\\to\\) vetor de caracteres dos rótulos para os níveis, na mesma ordem\nordered \\(\\to\\) vetor lógico (TRUE ou FALSE). Se TRUE, os níveis dos fatores são assumidos como ordenados\n\nNo exemplo, as variáveis não têm uma ordem lógica, então, o argumento ordered não é necessário.\n\ndadosNeonatos$tipoParto &lt;- factor(dadosNeonatos$tipoParto, \n                                  levels = c(1,2),\n                                  labels = c(\"normal\",\"cesareo\"))\ndadosNeonatos$sexo &lt;- factor (dadosNeonatos$sexo, \n                               levels = c(1,2), \n                               labels = c(\"M\",\"F\")) \n\nA variável utiNeo já foi inserida como string (texto) e pertence a classe character, então, basta usar a as.factor() sem necessidade de alterar os rótulos (labels) nos níveis (levels)\n\ndadosNeonatos$utiNeo &lt;- as.factor (dadosNeonatos$utiNeo)\n\nApós a transformação, executa-se, novamente, a função str() para ver como ficou a estrutura do dataframe:\n\nstr(dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : Factor w/ 2 levels \"M\",\"F\": 2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : Factor w/ 2 levels \"não\",\"sim\": 1 1 1 1 2 1 2 1 1 1 ...\n\n\nAgora, as três varáveis passaram a ser fatores e as outras mantiveram-se numéricas.\n\n\n5.1.3 Salvando um dataframe\nO dataframe, criado e modificado anteriormente, pode ser salvo para uso posterior no diretório de trabalho.\nA função save() realiza esta ação, usando como argumentos o dataframe a ser salvo e o nome do arquivo (file =) entre aspas. Por convenção, esta função salva com a extensão .RData que deve ser digitada, pois o R não a adiciona automaticamente.\n\nsave(dadosNeonatos, file = \"dadosNeonatos.RData\")\n\nEste comando colocará o arquivo no diretório de trabalho em uso. Portanto, se o objetivo é salvar em outro local, deve ser informado qual o novo diretório.\nPara carregar o objeto salvo anteriormente com o comando save(), usa-se a função load(). Se o arquivo a ser lido não estiver no diretório de trabalho da sessão, há necessidade de especificar o caminho até o arquivo:\n\nload(\"dadosNeonatos.RData\")\n\nÉ possível salvar em outro tipo de extensão como Excel (.xlsx), Valores Separados por Vírgula (.csv), etc. O procedimento é o mesmo, mudando a função. Para salvar em uma extensão .xlsx,utiliza-se a função write_xlsx () do pacote writexl (Ooms 2022):\n\nwritexl::write_xlsx(dadosNeonatos, \"dados/dadosNeonatos.xlsx\")\n\nPara salvar com a extensão .csv, usar a função write.csv() ou write.csv2() que faz parte do pacote utils, incluido no R base. A primeira função, usa \".\" para a separação dos decimais e \",\" para separar as variáveis; a segunda função usa \",\" para os decimais e \";\" para separar as variáveis, convenção do Excel para algumas localidades, como o Brasil (Team 2022). Portanto, uma maneira de salvar o arquivo é:\n\nwrite.csv2 (dadosNeonatos, \"dados/dadosNeonatos.csv\")",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "href": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "title": "5  Manipulando dados no R",
    "section": "5.2 Importando dados de outros softwares",
    "text": "5.2 Importando dados de outros softwares\nÉ possível inserir dados diretamente no R Script, como mostrado na Seção 5.1. Entretanto, se o conjunto de dados for muito extenso, torna-se complicado. Desta forma, é melhor construir o dataframe em outro software, como o Excel, SPSS, etc. e, após, quando necessário, importar os dados para o R.\n\n5.2.1 Importando dados de um arquivo CSV\nO formato CSV significa Comma Separated Values, ou seja, é um arquivo de valores separados por vírgula. Esse formato de armazenamento é simples e agrupa informações de arquivos de texto em planilhas. É possível gerar um arquivo .csv, a partir de uma planilha do Excel, usando o menu salvar como e escolher CSV.\nAs funções read.csv() e read.csv2(), incluídas no R base, podem ser utilizadas para importar arquivos CSV. Existe uma pequena diferença entre elas. Dois argumentos dessas funções têm padrão diferentes em cada uma. São eles: sep (separador de colunas) e dec (separador de decimais). Na read.csv(), o padrão é sep = ”,” e dec = ”.” e em read.csv2() o padrão é sep = “;” e dec = ”,”. Portanto, quando se importa um arquivo .csv, é importante saber qual a sua estrutura. Verificar se os decimais estão separados por ponto ou por vírgula e se as colunas (variáveis), por vírgula ou ponto e vírgula. Para ver isso, basta abrir o arquivo em um bloco de notas (por exemplo, Bloco de Notas do Windows, Notepad ++).\nQuando se usa o read.csv() há necessidade de informar o separador e o decimal, pois senão ele usará o padrão inglês e o arquivo não será lido. Já com read.csv2(), que usa o padrão brasileiro, não há necessidade de informar qual o separador de colunas e nem o separador dos decimais.\nAlém disso, é necessário saber em que diretório do computador está o arquivo para informar ao comando. Recomenda-se colocar o arquivo na pasta do diretório de trabalho, pois assim basta apenas colocar o nome do arquivo na função de leitura dos dados. Caso contrário, tem-se que se usar todo o caminho (path).\nComo exemplo, será importado o arquivo dadosNeonatos.csv que se encontra no diretório de trabalho do autor, salvo anteriormente. Para obter o arquivo, siga os passos da Seção 5.1 ou clique aqui e salve em seu diretório de trabalho.\nA estrutura deste arquivo mostra que as colunas estão separadas por ponto-e-virgula e, portanto, a leitura dos dados será feita com a função read.csv2() e, como o arquivo está no diretório de trabalho, não há necessidade de informar o diretório completo. Os dados serão colocados em um objeto de nome neonatos 1:\n\nneonatos &lt;- read.csv2(\"dados/dadosNeonatos.csv\")\n\nUse a função str() para visualizar o conjunto de dados:2\n\nstr(neonatos)\n\n'data.frame':   15 obs. of  8 variables:\n $ X        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : int  3340 3345 3750 3650 3220 4070 3380 3970 3060 3180 ...\n $ compRN   : int  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr  \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr  \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : int  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr  \"não\" \"não\" \"não\" \"não\" ...\n\n\nComo se observa na saída do comando, as variáveis foram importadas em classes que vão necessitar transformações para serem usadas. Isto deve ser feito como foi visto na Seção 5.1.2.\n\n\n\n\n\n\nAtenção\n\n\n\nToda vez que se importa um dataset, deve-se verificar atentamente a sua estrutura antes de dar seguimento as análises\n\n\nRecentemente, foi desenvolvido o pacote readr, incluído no conjunto de pacotes tidyverse (Wickham et al. 2019), para lidar rapidamente com a leitura de grandes arquivos. O pacote fornece substituições para funções como read.csv(). As funções read_csv() e read_csv2() oferecidas pelo readr são análogas às do R base. Entretanto, são muito mais rápidas e fornecem mais recursos, como um método compacto para especificar tipos de coluna. Além disso, produzem tibbles (ver adiante, Seção 5.3) que são mais reproduzíveis, pois as funções básicas do R herdam alguns comportamentos do sistema operacional e das variáveis de ambiente, portanto, o código de importação que funciona no seu computador pode não funcionar no de outra pessoa. Para usar a função é necessário instalar e ativar o pacote readr. A função read_csv2() será utilizada para criar um outro objeto de nome recemNascidos, mas o conjunto de dados a ser ativado é o mesmo (dadosNeonatos):\n\n library(readr)\n recemNascidos &lt;- read_csv2(\"dados/dadosNeonatos.csv\")\n\nQuando você executa read_csv2(), ele imprime uma especificação de coluna que fornece o nome e o tipo de cada coluna.\nNovamente, a função str() mostrará a estrutura do arquivo, incluindo mais detalhes 3:\n\nstr(recemNascidos)\n\nspc_tbl_ [15 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1     : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   id = col_double(),\n  ..   pesoRN = col_double(),\n  ..   compRN = col_double(),\n  ..   sexo = col_character(),\n  ..   tipoParto = col_character(),\n  ..   idadeMae = col_double(),\n  ..   utiNeo = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n5.2.2 Importando um arquivo do Excel\nO pacote readxl, pertencente ao conjunto de pacotes do tidyverse, facilita a obtenção de dados do Excel para o R, através da função read_excel(). esta função tem o argumento sheet = , que deve ser usado indicando o número ou o nome da planilha, colocado entre aspas. Este argumento é importante se houver mais de uma planilha, caso contrário, ele é opcional. Para saber os outros argumentos da função, colque o cursor dentro da função e aperte a tecla Tab (Figura 5.1). Isto abrirá um menu com os argumentos:\n\n\n\n\n\n\n\n\nFigura 5.1: Argumentos da função para importar arquivos .xlsx\n\n\n\n\n\nSerá feita a leitura dos mesmos dados, usados na leitura de dados csv, apenas o arquivo agora está no formato .xlsx. Para obter o arquivo, siga os mesmos passos, usados anteriormente. Clique aqui e salve em seu diretório de trabalho.\nOs dados serão atribuídos a um objeto com outro nome (recemNatos):\n\nrecemNatos &lt;- readxl::read_excel(\"dados/dadosNeonatos.xlsx\")\nstr(recemNatos)\n\ntibble [15 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n\n\nComo se vê ao analisar a estrutura, deve-se proceder transformações nas variáveis, como visto na Seção 5.1.2.\nNa Figura 5.1, o duplo dois pontos (::) precedido do nome do pacote, no caso readxl, especifica a procedência da função usada. Nesta situação, não há necessidade de usar a função library() para carregar o pacote já instalado em um diretório (biblioteca) previamente.\n\n\n5.2.3 Importando arquivos com o RStudio\nO RStudio permite importar arquivos sem a necessidade de digitar comandos, que, para alguns podem ser tediosos.\nNa tela inicial do RStudio, à direita, na parte superior, clique na aba Environment e em Import Dataset. Esta ação abre um menu que permite importar arquivos .csv, Excel, SPSS, etc.\nPor exemplo, para importar o arquivo dadosNeonatos.xlsx, clicar em From Excel... Abre uma janela com uma caixa de diálogo. Clicar no botão Browse..., localizado em cima à direita, para buscar o arquivo dadosNeonatos.xlsx. Assim que o arquivo for aberto, ele mostra uma preview do arquivo e, em baixo, à direita mostra uma preview do código (Figura 5.2)), igual ao digitado anteriormente, que cria um objeto denominado dadosNeonatos, nome do objeto escolhido pelo R, mas pode ser modificado na janela, à esquerda, Import Option em Name, onde pode-se digitar qualquer nome. Após encerrar as escolhas, clicar em Import. É um caminho diferente para fazer o mesmo. Este é um dos fascínios do R!\n\n\n\n\n\n\n\n\nFigura 5.2: Importando arquivos do excel com o RStudio",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-tibble",
    "href": "05-manipulandoDados.html#sec-tibble",
    "title": "5  Manipulando dados no R",
    "section": "5.3 Tibble",
    "text": "5.3 Tibble\nA maneira mais comum de armazenar dados no R é usar data.frames ou tibble.\nTibble é um novo tipo de dataframe. É como se fosse um dataframe mais moderno. Ele mantém muitos recursos importantes do data frame original, mas remove muitos dos recursos desatualizados.\nA maioria dos pacotes do R usa dataframes tradicionais, entretanto é possível transformá-los para tibble, usando a função as_tibble(), incluída no pacote tidyr (Wickham e Girlich 2022). O único propósito deste pacote é simplificar o processo de criação dados arrumados organizados (tidy data). A transformação de um dataframe tradicional em um tibble, é um procedimento rescomendável, em função da maior flexibilidade destes.\nComo exemplo deste procedimento, será usado o dataframes criado na Seção 5.1: dadosNeonatos.\nEste é um conjunto de dados da classe data.frame, contendo 15 observações de 7 variáveis (colunas), pode convetrtido a um tibble, usando a função as.tibble():\n\nlibrary(tidyr)\nas_tibble(dadosNeonatos)\n\n# A tibble: 15 × 7\n      id pesoRN compRN sexo  tipoParto idadeMae utiNeo\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt; \n 1     1   3340     50 F     normal          40 não   \n 2     2   3345     48 F     normal          19 não   \n 3     3   3750     52 F     cesareo         26 não   \n 4     4   3650     48 M     normal          19 não   \n 5     5   3220     50 M     cesareo         32 sim   \n 6     6   4070     51 M     cesareo         24 não   \n 7     7   3380     50 F     normal          27 sim   \n 8     8   3970     51 M     cesareo         20 não   \n 9     9   3060     47 M     normal          21 não   \n10    10   3180     47 M     normal          19 não   \n11    11   2865     47 F     normal          23 não   \n12    12   2815     49 F     cesareo         36 sim   \n13    13   3245     51 M     normal          21 não   \n14    14   2051     50 M     normal          23 não   \n15    15   2630     44 F     normal          23 não   \n\n\nPor padrão, quando o dataset é muito longo, apenas as primeiras dez linhas são mostradas. Aqui, aparece o toda a estrutura dos dados. São apresentadas a dimensão da tabela e as classes de cada coluna. Verifica-se que não houve grandes mudanças, apenas o conjunto de dados está estruturalmente mais organizado, mais flexível.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-tidyverse",
    "href": "05-manipulandoDados.html#sec-tidyverse",
    "title": "5  Manipulando dados no R",
    "section": "5.4 Pacote tidyverse",
    "text": "5.4 Pacote tidyverse\nA denominada ciência de dados é difícil de definir, pois a definição depende da formação específica de cada cientista de dados. Entretanto, é possível mostrar como a ciência de dados pode ser realizada na prática, constituindo aquilo que se costuma chamar de Ciclo da Ciência de Dados (Figura 5.3).\nPrimeiramente, os dados brutos são coletados de diversas maneiras (veja Capítulo 3). Após, são armazenados, por exemplo, em Excel, e, em seguida, são arrumados para reduzir problemas de padronização, conceituais e erros ou exclusão de variáveis e casos que não fazem parte do objetivo estabelecido no projeto de pesquisa. Isto constituirá a base de dados analítica.\nA base de dados analítica é então transformada, refinada, para produzir medidas resumidoras, tabela e gráficos. Quando necessário, são produzidos modelos estatíticos. O resultado final deve ser comunicado através dos meios de divulgação científica (relatórios, periódicos, livros, jornadas, congressos, GitHub, etc.).\n\n\n\n\n\n\n\n\nFigura 5.3: Ciclo da Ciência de Dados\n\n\n\n\n\nO pacote tidyverse é uma coleção de pacotes para a linguagem de programação R, pensada e desenvolvida para facilitar e otimizar o fluxo de trabalho em ciência de dados (Wickham et al. 2019).\nEm vez de ser um pacote único, ele é um “meta-pacote”, o que significa que, ao instalá-lo, você instala vários pacotes menores que trabalham em conjunto.\nA filosofia principal por trás do tidyverse é a de dados “tidy” (arrumados ou organizados) (Wickham 2014), onde:\n\nCada variável está em uma coluna.\nCada observação está em uma linha.\nCada valor está em uma célula.\n\nSeguindo essa lógica, as funções e pacotes do tidyverse são projetados para facilitar a manipulação dos dados dentro do Ciclo da Ciência de Dados (Figura 5.3), tornando a manipulação, a análise e a visualização de dados mais consistentes e intuitivas.\n\n5.4.1 Principais Pacotes do tidyverse\nO tidyverse simplifica tarefas complexas, oferecendo ferramentas específicas para cada etapa do Ciclo de Ciências de Dados. Alguns dos pacotes mais importantes são:\n\ndplyr: Para manipulação de dados. Contém funções essenciais para filtrar linhas, selecionar colunas, criar novas variáveis, e resumir dados.\nggplot2: Para visualização de dados. É um dos pacotes mais populares do R para criar gráficos esteticamente agradáveis e informativos.\ntidyr: Para organizar os dados. Ele transforma dados “bagunçados” (que não seguem o formato tidy) em um formato mais limpo e organizado.\nreadr: Para importar dados. Permite ler arquivos de texto (como CSVs) de forma rápida e robusta.\ntibble: Uma versão aprimorada do data.frame do R base. O tibble é mais fácil de usar e interage melhor com os pacotes do tidyverse.\nstringr: Para manipular strings (textos). Simplifica as tarefas de trabalhar com dados de texto.\nforcats: Para lidar com fatores (variáveis categóricas).\n\n\n\n5.4.2 Princípios do tidyverse\nO tidyverse causou quase uma revolução na comunidade do R. Aguns chegam a dizer que existe uma linguagem R antes e depois do tidyverse. Pode parecer exagero, mas o certo é que o uso dos princípios do tidyverse foi abraçado pela maioria dos usuários de R e, em função disso, foram criados uma grande quantidade de pacotes que conversam entre si, facilitando o manuseio dos dados.\nOs princípios fundamentais são:\n\nReutilizar estruturas de dados existentes.\nOrganizar funções simples usando o pipe.4. Esse operador, introduzido por Stefan Milton Bache no pacote magrittr(Bache e Wickham 2022), permite encadear múltiplas operações em uma sequência clara e legível, de forma que a saída de uma função se torna a entrada da próxima. Isso torna o código mais fácil de ler e entender, eliminando a necessidade de criar muitas variáveis intermediárias. O pipe pode ser acionado digitando %&gt;% ou usando o atalho ctrl + shift + M. 5\nUsar uma sintaxe consistente: As funções dos pacotes tidyverse seguem uma lógica de nomeação e de argumentos parecida, facilitando a memorização e o uso.\nProjetado com foco nos seres humanos, priorizando a eficiência do programador.\n\nQuando o tidyverse é carregado, todos os pacotes embutidos nele, serão carregados.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.1.0\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nConflito de funções ao carregar tidyverse\n\n\n\nAo carregar o pacote tidyverse ou qualquer outro, podem surgir mensagens de conflito indicando que funções previamente disponíveis foram sobrescritas por versões de mesmo nome.\nNo exemplo apresentado, as funções filter() e lag() do pacote stats foram substituídas pelas versões do pacote dplyr.\nPara utilizar as funções originais do pacote stats após esse carregamento, é necessário especificar o namespace diretamente:\nstats::filter() e stats::lag().",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-dplyr",
    "href": "05-manipulandoDados.html#sec-dplyr",
    "title": "5  Manipulando dados no R",
    "section": "5.5 Pacote dplyr",
    "text": "5.5 Pacote dplyr\nUm dos pacotes de maior utilidade abarcado pelo tidyverse é o dplyr. Ele permite realizar transformação dos dados de uma forma simples e eficiente. O uso dos verbos dplyr, aliado ao operador pipe, tendem a tornar os scripts mais “enxutos” e elegantes (Wickham et al. 2015).\nAs principais funções do dplyr são:\n\nselect() - seleciona colunas\n\narrange() - ordena uma variável em ordem crescente ou descrescente\n\nfilter() - filtra linhas\n\nmutate() - cria/modifica colunas\n\ngroup_by() - agrupa por fatores\n\nsummarise() - sumariza a base\n\nTodas as funções tem as mesmas características: o primeiro argumento é um tibble e o que será a ação da função nos outrso argumentos.\nNeste capítulo e em muitos outros deste livro, será outilizado o dataframe dadosMater.xlsx.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-dadosMater",
    "href": "05-manipulandoDados.html#sec-dadosMater",
    "title": "5  Manipulando dados no R",
    "section": "5.6 Dataframe dadosMater.xlsx",
    "text": "5.6 Dataframe dadosMater.xlsx\nO arquivo dadosMater.xlsx é um dataframe constituído por dados de 1568 partos consecutivos do Hospital Geral de Caxias do Sul (HGCS) 6 , durante um estudo sobre infecções congênitas (Madi et al. 2010). Para baixar esses dados, clique aqui e faça o download para o diretório de trabalho, para uso posterior.\n\n5.6.1 Leitura dos dados\nPara ler arquivos do Excel (.xlsx), o pacote ideal é o readxl, que também faz parte do tidyverse. Ele é leve, rápido e não depende do Excel instalado. A função a ser usada é read_excel(). O objeto mater será utilizado para receber os dados:\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\")\n\n\n\n\n\n\n\nImportante\n\n\n\nO comando para carregar o conjunto de dados somente funciona ,sem colocar o caminho completo, se tudo está sendo realizado no diretório de trabalho\n\n\nComo rotina, em análise de dados, após a leitura é interessante explorar a estrutura do dados. A função as_tibble() é interessante para ver a estrutura de um tibble:\n\nas_tibble(mater)\n\n# A tibble: 1,368 × 30\n      id idadeMae altura  peso ganhoPeso anosEst   cor eCivil renda  fumo\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1       42   1.65  69.9       3.9       3     2      1  1.45     2\n 2     2       29   1.66  78        16.5      11     1      2  2.41     2\n 3     3       19   1.72  81         5         9     2      1  1.93     2\n 4     4       31   1.55  74        43         5     2      2  1.45     2\n 5     5       34   1.6   60        15         7     2      2  0.48     2\n 6     6       29   1.5   60        11.4       8     2      2  0.96     1\n 7     7       30   1.54  75.5      10.5       4     1      2  1.2      1\n 8     8       34   1.63  61         9         6     1      2  2.41     2\n 9     9       17   1.68  57        15        10     1      2  2.17     2\n10    10       32   1.5   70        11.4       1     2      2  0.72     2\n# ℹ 1,358 more rows\n# ℹ 20 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;dbl&gt;, para &lt;dbl&gt;,\n#   droga &lt;dbl&gt;, ig &lt;dbl&gt;, tipoParto &lt;dbl&gt;, pesoPla &lt;dbl&gt;, sexo &lt;dbl&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;dbl&gt;, obito &lt;dbl&gt;, hiv &lt;dbl&gt;, sifilis &lt;dbl&gt;, rubeola &lt;dbl&gt;,\n#   toxo &lt;dbl&gt;, infCong &lt;dbl&gt;\n\n\nPor padrão, a função retorna as dez primeiras linhas. Além disso, colunas que não couberem na largura da tela serão omitidas. Também são apresentadas a dimensão da tabela e as classes de cada coluna. Observa-se que ele tem 1368 linhas (observações) e 30 colunas (variáveis). Além disso, verifica-se que todas as variáveis estão como numéricas (dbl) e, certamente, algumas, dependendo do objetivo na análise, precisarão ser transformadas.\nO significado de cada uma das variáveis do tibble mater é o seguinte:\n\nid \\(\\to\\) identificação do participante\n\nidadeMae \\(\\to\\) idade da parturiente em anos\n\naltura \\(\\to\\) altura da parturiente em metros\n\npeso \\(\\to\\) peso da parturiente em kg\n\nganhoPeso \\(\\to\\) aumento de peso durante a gestação\n\nanosEst \\(\\to\\) anos de estudo completos\n\ncor \\(\\to\\) cor declarada pela parturiente: 1 = branca; 2 = não branca\n\neCivil \\(\\to\\) estado civil: 1 = solteira; 2 = casada ou companheira\n\nrenda \\(\\to\\) renda familiar em salários minimos\n\nfumo \\(\\to\\) tabagismo: 1 = sim; 2 = não\n\nquantFumo \\(\\to\\) quantidade de cigarros fumados diariamente\n\nprenatal \\(\\to\\) realizou pelo menos 6 consultas no pré-natal? 1 = sim; 2 = não\npara \\(\\to\\) número de filhos paridos\n\ndroga \\(\\to\\) drogadição? 1 = sim; 2 = não\n\nig \\(\\to\\) idade gestacional em semanas\n\ntipoParto \\(\\to\\) tipo de parto: 1 = normal; 2 = cesareana\n\npesoPla \\(\\to\\) peso da placenta em gramas\nsexo \\(\\to\\) sexo do recém-nascido (RN): 1 = masc; 2 = fem\n\npesoRN \\(\\to\\) peso do RN em gramas\n\ncompRN \\(\\to\\) comprimento do RN em cm\n\npcRN \\(\\to\\) perímetro cefálico dorecém-nascido em cm\n\napgar1 \\(\\to\\) escore de Apgar no primeiro minuto\n\napgar5 \\(\\to\\) escore de Apgar no quinto minuto\n\nutiNeo \\(\\to\\) RN necessitou de terapia intesiva? 1 = sim; 2 = não\n\nobito \\(\\to\\) obito no período neonatal? 1 = sim; 2 = não\n\nhiv \\(\\to\\) parturiente portadora de HIV? 1 = sim; 2 = não\n\nsifilis \\(\\to\\) parturiente portadora de sífilis? 1 = sim; 2 = não\n\nrubeola \\(\\to\\) parturiente portadora de rubéola? 1 = sim; 2 = não\n\ntoxo \\(\\longrightarrow\\) parturiente portadora de toxoplasmose? 1 = sim; 2 = não\n\ninfCong \\(\\to\\) parturiente portadora de alguma infecção congênita? 1 = sim; 2 = não\n\nO tibble mater pode ser facilmente modificado com os verbos do pacote dplyr.\n\n\n5.6.2 Selecionando colunas\nA função select () pode ser usada para escolher quais colunas (variáveis) entrarão na análise. Ela recebe como primeiro argumento o conjunto de dados e os demais argumentos são os nomes das colunas. O nome das colunas devem estar entre aspas.\nO conjunto de dados mater contém 30 colunas e muitas podem ser removidas, dependendo do objetivo da análise.\n\nmater &lt;- dplyr::select(mater, - obito, -hiv, -sifilis, -rubeola, -toxo)\n\nNote que foi usado o sinal de menos (-) antes das variáveis, porque elas foram removidas. Também poderiam ser listadas as variáveia que permanecem que, automaticamente, as não listadas serão removidas.\nOutra maneira, pode ser colocando o número da coluna como abaixo, o sinal de subtração antes da função concatenar c()com os números das colunas a serem removidas (25 a 29):\n\nmater &lt;- dplyr::select(mater, - c(25:29))\n\nA função str() permite visualizar a nova estrutura:\n\nstr(mater)\n\ntibble [1,368 × 25] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:1368] 1 2 3 4 5 6 7 8 9 10 ...\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ ganhoPeso: num [1:1368] 3.9 16.5 5 43 15 11.4 10.5 9 15 11.4 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ cor      : num [1:1368] 2 1 2 2 2 2 1 1 1 2 ...\n $ eCivil   : num [1:1368] 1 2 1 2 2 2 2 2 2 2 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ fumo     : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ quantFumo: num [1:1368] 0 0 0 0 0 10 20 0 0 0 ...\n $ prenatal : num [1:1368] 2 1 2 2 2 1 1 2 2 1 ...\n $ para     : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ droga    : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: num [1:1368] 2 2 1 1 2 1 2 1 1 2 ...\n $ pesoPla  : num [1:1368] 224 1118 452 432 574 ...\n $ sexo     : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN   : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ pcRN     : num [1:1368] 28 32 28 32 32 29 32 32 30 27 ...\n $ apgar1   : num [1:1368] NA NA NA NA NA NA NA NA NA NA ...\n $ apgar5   : num [1:1368] NA NA NA NA NA NA NA NA NA NA ...\n $ utiNeo   : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n $ infCong  : num [1:1368] 2 2 2 2 1 2 2 2 2 2 ...\n\n\nA função select () pode ser combinada com outras funções, como filter ().\n\n\n5.6.3 Modificando e criando novas colunas\nPara esta ação, usa-se a função mutate(). Por exemplo, todas as variáveis no tibble mater foram lidas como numéricas. Entretanto, as variáveis cor, estCivil, fumo, prenatal, droga, tipoParto, sexo, utiNeo e infCong são categóricas e devem ser convertidas para fator, usando a função factor() associada a mutate():\n\nmater &lt;- dplyr::mutate(mater,\n                        cor = factor(cor, \n                                     levels = c(1,2), \n                                     labels = c(\"branca\", \"não branca\")), \n                        eCivil = factor(eCivil, \n                                          levels = c(1,2), \n                                          labels = c(\"solteira\", \"não branca\")),\n                        fumo = factor(fumo, \n                                      levels = c(1,2), \n                                      labels = c(\"sim\", \"não\")), , \n                        prenatal = factor(prenatal, \n                                          levels = c(1,2), \n                                          labels = c(\"sim\", \"não\")), \n                        droga = factor(droga, \n                                       levels = c(1,2), \n                                       labels = c(\"sim\", \"não\")), \n                        tipoParto = factor(tipoParto, \n                                           levels = c(1,2), \n                                           labels = c(\"normal\", \"cesareo\")), \n                        sexo = factor(sexo, \n                                      levels = c(1,2), \n                                      labels = c(\"masc\", \"fem\")), \n                        utiNeo = factor(utiNeo, \n                                        levels = c(1,2), \n                                        labels = c(\"sim\", \"não\")),\n                        infCong = factor(infCong, \n                                         levels = c(1,2), \n                                         labels = c(\"sim\", \"não\")))\n\nO conjunto de dados está, agora, estruturado de forma correta.\n\nstr(mater)\n\ntibble [1,368 × 25] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:1368] 1 2 3 4 5 6 7 8 9 10 ...\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ ganhoPeso: num [1:1368] 3.9 16.5 5 43 15 11.4 10.5 9 15 11.4 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ cor      : Factor w/ 2 levels \"branca\",\"não branca\": 2 1 2 2 2 2 1 1 1 2 ...\n $ eCivil   : Factor w/ 2 levels \"solteira\",\"não branca\": 1 2 1 2 2 2 2 2 2 2 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ quantFumo: num [1:1368] 0 0 0 0 0 10 20 0 0 0 ...\n $ prenatal : Factor w/ 2 levels \"sim\",\"não\": 2 1 2 2 2 1 1 2 2 1 ...\n $ para     : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ droga    : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 2 2 2 2 2 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ pesoPla  : num [1:1368] 224 1118 452 432 574 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN   : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ pcRN     : num [1:1368] 28 32 28 32 32 29 32 32 30 27 ...\n $ apgar1   : num [1:1368] NA NA NA NA NA NA NA NA NA NA ...\n $ apgar5   : num [1:1368] NA NA NA NA NA NA NA NA NA NA ...\n $ utiNeo   : Factor w/ 2 levels \"sim\",\"não\": 1 2 1 1 1 1 2 2 1 1 ...\n $ infCong  : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 2 2 2 2 ...\n\n\nO Índice de Massa Corporal (IMC) é um cálculo que relaciona o peso e a altura de uma pessoa para avaliar se ela está com o peso ideal, abaixo do peso, acima do peso ou obesa. É uma ferramenta simples e amplamente utilizada na área da saúde para triagem e acompanhamento do estado nutricional. Para obter esse índice, serão utilizadas as variáveis peso e altura da gestante no início da gravidez. Como visto na Seção 4.4, o cálculo do IMC é dado pela razão entre o peso em kg e a altura em metros elevada ao quadrado. Para criar uma nova coluna com a variável imc, pode-se também usar o mutate().\n\nmater &lt;- mutate(mater,\n                imc = peso/altura^2)\n\n\n\n5.6.4 Filtrando linhas\nA função filter() é usada para criar um subconjunto de dados que obedeçam determinadas condições lógicas: & (e), | (ou) e ! (não). Por exemplo:\n\ny & !x \\(\\to\\) seleciona y e não x\nx & !y \\(\\to\\) seleciona x e não y\nx | !x \\(\\to\\) seleciona x ou y\nx & y \\(\\to\\) seleciona x e y\n\nUm recém-nascido é dito a termo quando a duração da gestação é igual a 37 a 42 semanas incompletas. Para extrair do banco de dados mater os recém-nascidos a termo (dadosRNT), pode-se usar a função filter():\n\ndadosRNT &lt;- dplyr::filter (mater, ig&gt;=37 & ig&lt;42)\n\nObserve que, agora, o conjunto de dados dadosRNT tem 1085 linhas, número de recém-nascidos a termo do banco de dados original mater (1368). Logo, os recém nascidos a termo correspondem a 79.3% dos nascimentos, nesta maternidade.\nOutro exemplo\nPara selecionar apenas os meninos, nascidos a termo (dadosRNT), codificados como \"masc\", procede-se da seguinte maneira:\n\ndadosRNT_masc &lt;- filter (dadosRNT, sexo == 'masc')\n\n\n\n\n\n\n\nAlerta\n\n\n\nNão esquecer que o sinal de igualdade, no R, é representado por um == (duplo igual)\n\n\n\n\n5.6.5 Sumarizando uma coluna\nPara resumir uma coluna, utilizando uma métrica de interesse, como média, mediana, desvio padrão, etc. (Capítulo 6), pode-se usar a função summarize().\n\nresumo &lt;- dplyr::summarize(dadosRNT,\n  n = length (id),\n  media = mean(pesoRN, na.rm = TRUE),\n  desvpad = sd(pesoRN, na.rm = TRUE)\n)\nresumo\n\n# A tibble: 1 × 3\n      n media desvpad\n  &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  1085 3216.    462.\n\n\nMuitas vezes, há necessidade de sumarizar uma coluna agrupada pelas categorias de uma segunda coluna. Por exemplo, peso dos recém-nascidos a termo por sexo. Para isso, além do summarize(), utilizamos também a função group_by().\nPara facilitar o trabalho, será usado o operador pipe que pode ser acionado digitando %&gt;% ou usando o atalho ctrl + shift + M7, como observado na Seção 5.4.2. Em vez de passar o argumento para a função separadamente, é possível escrever o valor ou objeto e, em seguida, usar o pipe para convertê-lo como o argumento da função na mesma linha. Funciona como se o pipe jogasse o objeto dentro da função seguinte.\n\nlibrary(dplyr)\nresumo &lt;- dadosRNT %&gt;% \n        group_by(sexo) %&gt;% \n        summarize(\n  n = length (id),\n  media = mean(pesoRN, na.rm = TRUE),\n  desvpad = sd(pesoRN, na.rm = TRUE)\n)\nresumo\n\n# A tibble: 2 × 4\n  sexo      n media desvpad\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 masc    592 3274.    458.\n2 fem     493 3147.    458.\n\n\n\n\n5.6.6 Selecionando linhas específicas\nA função slice() do pacote dplyr é usada para selecionar linhas específicas de um dataframe (ou tibble) com base em suas posições. Ela é bastante útil quando se quer extrair subconjuntos de dados sem usar condições lógicas, mas sim índices de linha.\nDiferente de filter(), que seleciona linhas baseado em condições, slice() usa números de linhas. Por exemplo, para visualizar as cinco primeiras linhas do conjunto de dados dadosRNT, usa-se:\n\nslice(dadosRNT, 1:5)\n\n# A tibble: 5 × 26\n     id idadeMae altura  peso ganhoPeso anosEst cor        eCivil    renda fumo \n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;\n1    20       28   1.5   48.5      11         6 não branca não bran…  3.13 não  \n2    21       31   1.55  65        24         5 branca     não bran…  0.72 não  \n3    22       27   1.6   60        15         8 não branca não bran…  2.41 sim  \n4    23       28   1.58  47         9         8 branca     não bran…  1.69 não  \n5    24       18   1.76  65.5       6.5       7 branca     solteira   1.93 sim  \n# ℹ 16 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;fct&gt;, para &lt;dbl&gt;,\n#   droga &lt;fct&gt;, ig &lt;dbl&gt;, tipoParto &lt;fct&gt;, pesoPla &lt;dbl&gt;, sexo &lt;fct&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;fct&gt;, infCong &lt;fct&gt;, imc &lt;dbl&gt;\n\n\nA função slice() é compatível com agrupamentos, por exemplo, para selecionar os cinco primeiros casos do tibble dadosRNT por sexo:\n\ndadosRNT %&gt;%\n        group_by(sexo) %&gt;% \n        slice(1:5)\n\n# A tibble: 10 × 26\n# Groups:   sexo [2]\n      id idadeMae altura  peso ganhoPeso anosEst cor        eCivil   renda fumo \n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;\n 1    20       28   1.5   48.5      11         6 não branca não bra…  3.13 não  \n 2    21       31   1.55  65        24         5 branca     não bra…  0.72 não  \n 3    22       27   1.6   60        15         8 não branca não bra…  2.41 sim  \n 4    23       28   1.58  47         9         8 branca     não bra…  1.69 não  \n 5    24       18   1.76  65.5       6.5       7 branca     solteira  1.93 sim  \n 6   751       17   1.65  60        11.4       7 não branca solteira  1.92 não  \n 7   752       30   1.6   54        12         5 branca     não bra…  1.92 não  \n 8   753       27   1.53  43.5      20.5      11 branca     não bra…  1.93 não  \n 9   755       28   1.4   60        11.4       8 não branca não bra…  2.17 não  \n10   756       17   1.55  78        20        10 branca     solteira  4.82 sim  \n# ℹ 16 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;fct&gt;, para &lt;dbl&gt;,\n#   droga &lt;fct&gt;, ig &lt;dbl&gt;, tipoParto &lt;fct&gt;, pesoPla &lt;dbl&gt;, sexo &lt;fct&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;fct&gt;, infCong &lt;fct&gt;, imc &lt;dbl&gt;\n\n\nO tidyverse introduziu funções auxiliares como:\n\nslice_head(): que seleciona as primeiras n linhas. A saída dessa função é a mesma anterior, se for solicitado n = 5:\n\n\n\n# A tibble: 5 × 26\n     id idadeMae altura  peso ganhoPeso anosEst cor        eCivil    renda fumo \n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;\n1    20       28   1.5   48.5      11         6 não branca não bran…  3.13 não  \n2    21       31   1.55  65        24         5 branca     não bran…  0.72 não  \n3    22       27   1.6   60        15         8 não branca não bran…  2.41 sim  \n4    23       28   1.58  47         9         8 branca     não bran…  1.69 não  \n5    24       18   1.76  65.5       6.5       7 branca     solteira   1.93 sim  \n# ℹ 16 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;fct&gt;, para &lt;dbl&gt;,\n#   droga &lt;fct&gt;, ig &lt;dbl&gt;, tipoParto &lt;fct&gt;, pesoPla &lt;dbl&gt;, sexo &lt;fct&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;fct&gt;, infCong &lt;fct&gt;, imc &lt;dbl&gt;\n\n\n\nslice_tall(): que seleciona as últimas n linhas.\n\nslice_sample(): seleciona linhas aleatórias. Uma amostra de n = 200 será extraída do tibble dadosRNT, como exemplo:\n\n\ndadosRNT200 &lt;- dadosRNT %&gt;% slice_sample(n = 200)\n\n\nslice_min(): selecioan as linhas com os menores valores em uma coluna específica. Não ordena todo o dataframe, mas sim identifica e extrai as linhas que têm os menores valores na coluna indicada com order_by. Por exemplo, se o objetivo é extrarir do tibble dadosRNT so cinco menores pesos ao nascer por sexo:\n\n\ndadosRNT %&gt;% \n  select(pesoRN, sexo) %&gt;% \n  slice_min(order_by = pesoRN, \n            n = 5, \n            with_ties = TRUE, \n            by = sexo)\n\n# A tibble: 10 × 2\n   pesoRN sexo \n    &lt;dbl&gt; &lt;fct&gt;\n 1   1425 masc \n 2   1440 masc \n 3   1795 masc \n 4   1810 masc \n 5   1980 masc \n 6   1715 fem  \n 7   1785 fem  \n 8   1895 fem  \n 9   2090 fem  \n10   2095 fem  \n\n\n\nslice_max(): funcion a da mesma que o slice_min(), apenas para os maiores valores.\n\n\n\n\n\n\n\nExercício\n\n\n\nVerificar a média e o desvio padrão dos pesos dos recém-nascidos a termo de mães fumantes e não fumantes, por sexo.\n\n\n\n\n5.6.7 Ordenando uma coluna\nPara ordenar os dados de uma coluna, pode-se usar a função arrange() do dplyr. O primeiro argumento é o conjunto de base. Os demais argumentos são as colunas a serem ordenadas.\nPor padrão, a função ´ coloca os dados em ordem crescente, mas é possível alterar e organizar em ordem decrescente usando a função desc(), que recebe o nome da coluna como argumento e ordena os valores em ordem decrescente.\nPor exemplo, a renda familiar das parturientes será ordenada de forma ascendente. Em primeiro lugar, apenas como exercício, a renda familiar em salários mínimos será convertida em reais, tomando como base o valor de 2025 de 1518 reais. Após, a variável renda será colocada em ordem crescente com a função arrange(). A seguir, usando as funções slice_head() e slice_tail(), se verificará as 5 menores e as 5 maiores rendas que serão atribuídos a dois objetos (menores e maiores). Estes vão ser exibidos juntos, usando a função bind_rows(), também do dplyr:\n\n# Cinco menores salários em ordem crescente\nmenores &lt;- mater %&gt;% select(renda) %&gt;% \n    mutate(renda = renda *1518.00) %&gt;% \n    arrange(renda) %&gt;% \n    slice_head(n=5)\nmaiores &lt;- mater %&gt;% select(renda) %&gt;% \n    mutate(renda = renda *1518.00) %&gt;% \n    arrange(renda) %&gt;% \n    slice_tail(n=5)\n\n# Exibição\nbind_rows(menores, maiores)\n\n# A tibble: 10 × 1\n    renda\n    &lt;dbl&gt;\n 1   288.\n 2   364.\n 3   622.\n 4   653.\n 5   729.\n 6 14634.\n 7 14634.\n 8 14634.\n 9 16227.\n10 16455.\n\n\n\n\n5.6.8 Função count()\nPermite contar rapidamente os valores únicos de uma ou mais variáveis. Esta função tem os seguintes argumentos.\n\nx \\(\\to\\) dataframe\nwt \\(\\to\\) pode ser NULL (padrão) ou uma variável\nsort \\(\\to\\) padrão = FALSE; se TRUE, mostrará os maiores grupos no topo\nname \\(\\to\\) O nome da nova coluna na saída; padrão = NULL\n\nQuando o argumento name é omitido, a função retorna n como nome padrão.\nUsando o dataframe mater, a função count() irá contar o número de parturientes fumantes, variável dicotômica fumo:\n\ncount(mater, fumo)\n\n# A tibble: 2 × 2\n  fumo      n\n  &lt;fct&gt; &lt;int&gt;\n1 sim     301\n2 não    1067\n\n\n\n\n\n\n\n\nExercício\n\n\n\nCalcule o percentual de partos cesáreos no tibble mater.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#pacote-forcats",
    "href": "05-manipulandoDados.html#pacote-forcats",
    "title": "5  Manipulando dados no R",
    "section": "5.7 Pacote forcats()",
    "text": "5.7 Pacote forcats()\nO pacote forcats() é uma das maravilhas do tidyverse voltada exclusivamente para o tratamento de variáveis categóricas no R — ou seja, os fatores. O nome vem de “for categorical variables”, e ele foi criado para resolver os desafios que surgem ao lidar com fatores, especialmente em visualizações e modelagens (Damiani et al. 2022).\nA finalidade do pacote forcats():\nOferece funções intuitivas e poderosas para:\n\nReordenar os níveis de um fator;\n\nModificar, combinar e recodificar níveis;\n\nLidar com níveis raros ou ausentes;\n\nPreparar fatores para gráficos com ggplot2 (Seção 8.3)\nAs principais funções doforcats() são:\nfct_reorder() - Reordena os níveis com base em outra variável (ex: média);\n\nfct_infrequent() - Reordena os níveis pela frequência (mais comum primeiro);\n\nfct_rev() - Inverte a ordem dos níveis;\n\nfct_lump() - Agrupa níveis menos frequentes em “outros”:\n\nfct_recode() - Renomeia níveis manualmente;\n\nfct_drop() - Remove níveis não utilizados;\n\nfct_expand() - Adiciona novos níveis.\n\nComo exemplo, será modificada a ordem de como a variável sexo será apresentada. Para ver a ordem dos níveis, pode-se usar:\n\nlevels(mater$sexo)\n\n[1] \"masc\" \"fem\" \n\n\nOu seja, o sexo masculino está colocado antes do feminino. Como é uma variável dicotômica, basta inverter a ordem, usando a função fct_rev():\n\nmater$sexo &lt;- fct_rev(mater$sexo)\n\nlevels(mater$sexo)\n\n[1] \"fem\"  \"masc\"",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#manipulação-de-datas",
    "href": "05-manipulandoDados.html#manipulação-de-datas",
    "title": "5  Manipulando dados no R",
    "section": "5.8 Manipulação de datas",
    "text": "5.8 Manipulação de datas\nOriginalmente, todos os que trabalham com o R queixavam-se de como era frustrante trabalhar com datas. Era um processo que causava grande perda de tempo nas análises. O pacote lubridate (Grolemund e Wickham 2011) foi criado para simplificar ao máximo a leitura de datas e extração de informações das mesmas.\n\nlibrary(lubridate)\n\nQuando o lubridate é carregado aparece uma mensagem, avisando que alguns nomes de funções também estão contidas no pacote base do R.\nPara evitar confusões e verificar que as funções corretas estão sendo usadas, usa-se o duplo dois pontos (::) antes do nome da função, precedido do nome do pacote, por exemplo: lubridate::date().\nPara obter a data atual ou a data-hora, você pode usar as funções today() ou now():\n\ntoday()\n\n[1] \"2025-11-27\"\n\nnow()\n\n[1] \"2025-11-27 18:27:50 -03\"\n\n\n\n5.8.1 Convertendo strings ou caractere para data\nPara converter string ou caracteres em datas, basta executar funções específicas adequadas aos dados. Elas determinam automaticamente o formato quando você especifica a ordem do componente. Para usá-los, identifique a ordem em que o ano, o mês e o dia aparecem em suas datas e, em seguida, organize “y”, “m” e “d” na mesma ordem. Isso lhe dá o nome da função do lubridate que analisará a data. Por exemplo, suponhamos a data de 25/12/2022:\n\nnatal &lt;- \"25/12/2022\"\nnatal\n\n[1] \"25/12/2022\"\n\n\nAparentemente, o R aceitou a informação como uma data. Entretanto, se for verificada a classe do objeto, tem-se:\n\nclass(natal)\n\n[1] \"character\"\n\n\nEstando como caractere, esta data não poderá ser usada em operações com datas, pois necessitaria estar como uma classe date. Para converte-la em data, usa-se a função dmy():\n\nnatal &lt;- dmy(natal)\nnatal\n\n[1] \"2022-12-25\"\n\nclass(natal)\n\n[1] \"Date\"\n\n\nDessa forma, a data, agora está sendo reconhecida pelo R como date. É sempre importante verificar a classe da data.\nÀs vezes, as datas escritas estão com o mês abreviado, como 25/dez/2022. O procedimento é o mesmo\n\nminha.data &lt;- \"25/dez/2022\"\nclass (minha.data)\n\n[1] \"character\"\n\n\n\nminha.data &lt;- dmy(minha.data)\nclass (minha.data)\n\n[1] \"Date\"\n\n\nSe além da data, houver necessidade de especificar o horário, basta usar dmy_h(), dmy_hm() e dmy_hms(). No padrão americano, pode ser usado ymd().\nO lubridate traz diversas funções para extrair os componentes de um objeto da classe date.\n\nsecond() \\(\\to\\) extrai os segundos.\nminute() \\(\\to\\) extrai os minutos.\nhour() \\(\\to\\) extrai a hora.\nwday() \\(\\to\\) extrai o dia da semana.\nmday() \\(\\to\\) extrai o dia do mês.\nmonth() \\(\\to\\) extrai o mês.\nyear() \\(\\to\\) extrai o ano.\n\nPor exemplo, usando a data de nascimento (dn) de um dos netos do autor:\n\ndn &lt;- dmy(\"06/06/2018\")\nyear(dn)\n\n[1] 2018\n\n\nPara acrescentar um horário ao objeto data de nascimento (dn)^[UTC = Coordinated Universal Time}:\n\nhour(dn) &lt;- 18\ndn\n\n[1] \"2018-06-06 18:00:00 UTC\"\n\n\n\n\n5.8.2 Juntando componentes de datas\nPara juntar componentes de datas e horas, pode-se utilizar as funções make_date() e make_datetime(). Em muitos arquivos, os componentes da data estão em colunas diferentes e há necessidade de juntá-los em uma única coluna para compor a data:\n\nfelix &lt;- make_date(year = 2018, month = 06, day = 06)\nfelix\n\n[1] \"2018-06-06\"\n\n\nPara juntar ano, mês, dia, hora e minuto:\n\nminha.data &lt;- make_datetime(year = 2018, \n                            month = 06, \n                            day = 06, \n                            hour = 18 ,\n                            min = 00, \n                            sec = 15)\nminha.data\n\n[1] \"2018-06-06 18:00:15 UTC\"\n\n\n\n\n5.8.3 Extraindo componentes de datas\nQuando temos objetos do tipo POSIXt8 podemos extrair componentes ou elementos deles. Para isso são usadas algumas funções específicas do pacote lubridate como mostrado a seguir.\n\ndata &lt;- now()\n\nyear(data)            # Extrai o ano\n\n[1] 2025\n\nmonth(data)           # Extrai o mês\n\n[1] 11\n\nweek(data)            # Extrai a semana\n\n[1] 48\n\nday(data)             # Extrai o dia\n\n[1] 27\n\nminute(data)          # Extrai o minuto\n\n[1] 27\n\nsecond(data)          # Extrai o segundo\n\n[1] 50.79869\n\n\nPara verificar o número de dias tem em um determinado mês, usa-se a função days_in_month():\n\n data1 &lt;- dmy(\"25/02/2000\")\n days_in_month(data1)          \n\nFeb \n 29 \n\n\n\n\n5.8.4 Operações com datas\nO pacote lubridate possui funções de duração e de período para manipular as datas. As funções de duração calculam o número de segundos em um determinado num determinado número de dias. As funções de duração não levam em consideração anos bissextos e horário de verão, enquanto as funções de período consideram esses fatores.\n\nddays (1)           # Número de segundos em 1 dia\n\n[1] \"86400s (~1 days)\"\n\ndhours (1)          # Número de segundos em 1 hora\n\n[1] \"3600s (~1 hours)\"\n\ndminutes (1)        # Número de segundos em 1 minuto\n\n[1] \"60s (~1 minutes)\"\n\ndays (5)            # Cria um período de 5 dias\n\n[1] \"5d 0H 0M 0S\"\n\nweeks (5)           # Cria um período de 5 semanas\n\n[1] \"35d 0H 0M 0S\"\n\n\nSuponha-se que haja necessidade de saber em qual dia cairá após acrescentarmos 5 semanas à data1 (25/02/2000), criada acima:\n\ndata1 + weeks (5)           \n\n[1] \"2000-03-31\"\n\n\nAdicionando 1 ano à data1 (25/02/2000) com uma função de duração, tem-se:\n\ndata1 + dyears (1)           \n\n[1] \"2001-02-24 06:00:00 UTC\"\n\n\nSe for adicionado um ano à mesma data, mas agora com uma função de período, tem-se:\n\ndata1 + years (1)           \n\n[1] \"2001-02-25\"\n\n\nUm intervalo de tempo pode ser obtido a partir de uma data inicial e uma data final. Suponha que uma gestante tenha como data da sua última menstruação 04/10/2022 e o bebê tenha nascido em 30/06/2023. Qual a idade gestacional em dias? A sintaxe para calcular um intervalo é dada pela subtração das duas datas:\n\ndata.inicial &lt;- dmy(\"04/10/2022\")\ndata.final &lt;- dmy(\"30/06/2023\")\nidade_gesta &lt;- data.final - data.inicial\nidade_gesta\n\nTime difference of 269 days\n\n\nOu seja a gestação durou 269 dias, constituindo-se em um parto a termo, entre 37 (259 dias) e 42 semanas (294 dias).\nPara mais informações sobre o lubridate, consulte a ajuda do pacote ou o capítulo 16 do livro R for Data Science, Hadley Wickman e Garrett Grolemund, 2017 [https://r4ds.had.co.nz/index.html] .\n\n\n\n\n\n\nBache, Stefan Milton, e Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R.\n\n\nDamiani, Athos, Beatriz Milz, Caio Lente, e Outros. 2022. «O pacote forcats». Ciência de Dados em R. R6 Consultoria. https://livro.curso-r.com/7-6-forcats.html#o-que-s%C3%A3o-fatores.\n\n\nGrolemund, Garrett, e Hadley Wickham. 2011. «Dates and Times Made Easy with lubridate». Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nMadi, José Mauro, Ricardo da Silva de Souza, Breno Fauth de Araujo, Petrônio Fagundes Oliveira Filho, et al. 2010. «Prevalence of toxoplasmosis, HIV, syphilis and rubella in a population of puerperal women using Whatman 903 filter paper». The Brazilian Journal of Infectious Diseases 14 (1). Elsevier: 24–29.\n\n\nOoms, Jeroen. 2022. writexl: Export Data Frames to Excel ’xlsx’ Format. https://CRAN.R-project.org/package=writexl.\n\n\nTeam, R Core. 2022. «write.table: Data Output/CSV files». DataCamp. https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table.\n\n\nWickham, Hadley. 2014. «Tidy Data». Journal of Statistical Software 59 (10): 11–23.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, et al. 2019. «Welcome to the Tidyverse». Journal of Open Source Software 4 (43): 1686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et al. 2015. «dplyr: A grammar of data manipulation». R package version 0.4 3: 156.\n\n\nWickham, Hadley, e Maximilian Girlich. 2022. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nZuur, Alain F, Elena N Ieno, e Erik HWG Meesters. 2009. «Getting Data into R». Em A Beginner’s Guide to R, 29–56. Springer.",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#footnotes",
    "href": "05-manipulandoDados.html#footnotes",
    "title": "5  Manipulando dados no R",
    "section": "",
    "text": "A mudança do nome do dataframe de dadosNeonatos para neonatos é desnecessária. Foi realizada apenas por questões didáticas.↩︎\nObserve, na saída, que a variável utiNeo aparece palavras com acentuação (“não”). Às vezes, ao abrir o arquivo com a função read.csv2(), pode acontecer de esta palavra aparecer, por exemplo, como: “n3o”. Louco, não é? Se ocorrer isso, use, após o nome do arquivo e separado por vírgula, o argumento fileEncoding = “latin1”. Dessa forma, o erro será corrigido.↩︎\nDa mesma maneira, como acontece com a função read.csv2(), a função equivalente do readr pode retornar erro na leitura de palavras com acento. Para corrigir isso, usa-se o argumento locale (encoding = \"latin1\")↩︎\nO nome é uma referência ao famoso quadro do pintor belga René Magritte La Trahison des images (Ceci n’est pas une pipe).↩︎\nAlém do pipe, indiretamente, embutido no tidyverse, existe o pipe nativo do R (|&gt;) que também pode ser usado com as mesmas funções do %&gt;%.↩︎\nHospital Escola da Universidade de Caxias do Sul, RS.↩︎\nPara que o pipe (%&gt;%) seja ativado é necessário carregar o pacote dplyr; para o pipe nativo (|&gt;) não há necessidade.↩︎\nPOSIXt é uma classe de objetos do R que representa datas e horas. POSIXt significa Portable Operating System Interface for Unix Time, que é um padrão para medir o tempo em segundos desde 1 de janeiro de 1970. Existem duas formas internas de implementar POSIXt: POSIXct e POSIXlt. POSIXct armazena os segundos desde a época UNIX e POSIXlt armazena uma lista de dia, mês, ano, hora, minuto, segundo, etc.↩︎",
    "crumbs": [
      "Parte II - Ambiente Computacional com R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando dados no R</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html",
    "href": "06-medidasResumidoras.html",
    "title": "6  Medidas Resumidoras",
    "section": "",
    "text": "6.1 Dados brutos\nHabitualmente, costuma-se armazenar os dados em bancos de dados (dataframes ou tibbles). Entretanto, eles estão registrados de forma aleatória e não classificada. Ao se visualizar um dataframe, é difícil responder perguntas em relação a qualquer variável, principalmente, em grandes banco de dados. Eles se constituem uma lista, um rol de valores colocados na ordem em que foram obtidos. Parecem um jogo de quebra cabeça antes de ser organizado! A Tabela 6.1 é o tipo mais simples de tabela possível. Nela é apresentado um conjunto de dados, os valores dos pesos de 30 recém-nascidos. Não há nenhum significado especial para as linhas e colunas, estão dispostos da maneira como coletados. Eles pouco informam; são apenas os dados na sua forma inicial sem nenhum tratamento ou ordem. São denominados como dados brutos e se constituem no ponto de partida para uma análise. Quando são colocados de maneira crescente e permitem uma compreensão inicial são chamados de dados não agrupados e de dados agrupados, quando classificados em categorias ou intervalos.\nTabela 6.1: Peso de 30 recém-nascidos de partos consecutivos\n\n\n\n      2,9403,5753,1503,5453,3652,8253,0602,5803,1102,4154,6703,6701,9303,0003,1152,8502,4901,4652,7904,4453,2903,2153,2453,4201,7502,9253,3451,1053,4453,150",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html#introdução",
    "href": "06-medidasResumidoras.html#introdução",
    "title": "6  Medidas Resumidoras",
    "section": "6.2 Introdução",
    "text": "6.2 Introdução\nNos relatórios ou artigos científicos, a comunicação dos resultados é feita através da combinação de medidas resumidoras e visualização dos dados por meio de tabelas e gráficos, constituindo o que se costuma chamar de Estatística Descritiva. Neste capítulo, serão discutidas as principais medidas resumidoras dos dados de duas maneiras:\n\nPrimeiro, um valor em torno do qual os dados têm uma tendência para se reunir ou se agrupar, denominado de medida sumária de localização ou medida de tendência central.\nEm segundo lugar, um valor que mede o grau em que os dados se dispersam, denominado de medida de dispersão ou variabilidade\n\nNos próximos capítulos, desta Parte III, estarão em discussão a construção de Tabelas (Capítulo 7) e visualização dos dados através de gráficos (Capítulo 8).",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html#sec-dados6",
    "href": "06-medidasResumidoras.html#sec-dados6",
    "title": "6  Medidas Resumidoras",
    "section": "6.3 Dados usados neste capítulo",
    "text": "6.3 Dados usados neste capítulo\nPara as demonstrações práticas, será usado o banco de dados dadosMater.xlsx (ver Seção 5.6). Após carregá-lo, serão filtrados os partos a termo e selecionada as variáveis necessárias (idadeMae, anosEst, pesoRN, apgar1). Por última, será extraída com a função slice_sample() do pacote dplyr (Seção 5.5) uma amostra de n = 200. Como cada vez que este comando for reproduzido, retornará uma nova série de 200 valores diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada. Após extrair a amostra, esta será atribuída a um objeto denominado, dados:\n\nlibrary(dplyr)\n\nset.seed(123)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig &gt;= 37 & ig &lt; 42) %&gt;% \n  select(idadeMae, anosEst, pesoRN, apgar1) %&gt;% \n  slice_sample(n=200)",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html#medidas-de-tendência-central",
    "href": "06-medidasResumidoras.html#medidas-de-tendência-central",
    "title": "6  Medidas Resumidoras",
    "section": "6.4 Medidas de tendência central",
    "text": "6.4 Medidas de tendência central\n\n6.4.1 Média\nA média (\\(\\overline{x}\\)) é a mais usada medida de tendência central para representar um valor típico dentro de um conjunto de números. O conceito mais comum é a média aritmética, que se calcula somando todos os valores do conjunto e dividindo pelo número total de elementos. A média é mais adequada para medidas numéricas simétricas, pois ela é sensível aos valores extremos (outliers).\n\\[\n\\overline{x}= \\frac{\\sum(x_1 + x_2 + x_3 + ... + x_n)}{n}\n\\]\nO R base possui uma função para o cálculo da média, mean(), apresentada na Seção 4.4, onde foi mostrado os seus argumentos. Se a variável analisada contiver algum valor ausente (missing), deve-se usar o argumento na.rm = TRUE, para removê-los, pois, caso contrário, a função retorna um resultado como NA (Not Available). Para evitar transtornos, recomenta-se usar sempre o argumento.\n\nmean (dados$pesoRN, na.rm = TRUE)\n\n[1] 3257.95\n\n\nPara reduzir o número de dígitos decimais, na saída do resultado, pode-se colocar a função mean(), dentro da função round()1, atribuindo o resultado da função a um objeto, por exemplo media.\n\nmedia &lt;- round(mean (dados$pesoRN, na.rm = TRUE), 1)\nprint(media)\n\n[1] 3257.9\n\n\nOu, usar a função round(), separadamente:\n\nmedia &lt;- mean (dados$pesoRN, na.rm = TRUE)\nround(media, 1)\n\n[1] 3257.9\n\n\n\n\n6.4.2 Mediana\nA mediana (Md) representa o valor central em uma série ordenada de valores. Assim, metade dos valores será igual ou menor que o valor mediano e a outra metade igual ou maior do que ele. Para encontrar a mediana procede-se da seguinte maneira:\n\nOrdenar o conjunto de dados, por exemplo, a variável dados$pesoRN, usando a função sort():\n\n\nvalores_ordenados &lt;- sort(dados$pesoRN)\nprint(valores_ordenados)\n\n  [1] 2100 2110 2125 2170 2300 2330 2340 2345 2345 2395 2415 2570 2650 2660 2680\n [16] 2685 2695 2700 2710 2720 2730 2735 2750 2750 2770 2775 2780 2780 2795 2805\n [31] 2830 2845 2845 2850 2860 2870 2875 2880 2900 2900 2910 2920 2930 2935 2940\n [46] 2965 2980 2985 2985 3000 3010 3015 3020 3020 3025 3030 3030 3040 3040 3040\n [61] 3050 3050 3055 3055 3060 3060 3060 3075 3080 3080 3080 3090 3090 3105 3115\n [76] 3120 3120 3135 3145 3150 3150 3150 3150 3160 3160 3165 3165 3170 3180 3180\n [91] 3180 3200 3215 3230 3250 3260 3270 3270 3275 3280 3290 3295 3300 3300 3300\n[106] 3300 3300 3305 3305 3315 3320 3320 3320 3325 3330 3330 3335 3340 3340 3345\n[121] 3355 3370 3385 3395 3400 3405 3410 3410 3410 3415 3415 3425 3430 3430 3430\n[136] 3450 3460 3465 3465 3470 3480 3490 3490 3490 3495 3500 3500 3505 3510 3540\n[151] 3545 3545 3550 3550 3560 3570 3580 3585 3585 3600 3610 3610 3625 3625 3635\n[166] 3640 3650 3660 3660 3665 3710 3715 3715 3730 3730 3770 3780 3795 3830 3840\n[181] 3845 3880 3890 3905 3920 3920 3920 3945 3970 3995 4025 4045 4080 4080 4090\n[196] 4160 4315 4350 4370 4485\n\n\n\nQuando o número de valores é par , caso do exemplo, a mediana é a média dos dois valores do meio, ou seja, o valor central corresponde a média do valor 100 e do valor 101 dos valores ordenados:\n\n\nmediana = (valores_ordenados[100] + valores_ordenados[101])/2\nprint(mediana)\n\n[1] 3285\n\n\n\nQuando o número de valores no conjunto de dados for ímpar, a mediana é o valor do meio.\n\n\nNo exemplo, tem-se 200 valores. Para transformar a amostra em uma amostra com n ímpar, remover aleatoriamente uma observação dos valores dos dados$pesoRN:\n\n\nset.seed(234)\nindex_aleatorio &lt;- sample(length(dados$pesoRN), 1)\nindex_aleatorio\n\n[1] 31\n\n\n\nOu seja, o valor selecionado é o 31º valor. A seguir, obtem-se o peso aleatório que foi selecionado:\n\n\npeso_aleatorio &lt;- dados$pesoRN[index_aleatorio]\npeso_aleatorio\n\n[1] 3585\n\n\n\nRemover o 31º valor (index_aleatório) da lista de dados$pesoRN que corresponde ao peso selecionado:\n\n\npesos_restantes &lt;- dados$pesoRN[-index_aleatorio]\n\n\nColocar os pesos_restantes em ordem crescente:\n\n\nrestantes_ordenados &lt;- sort(pesos_restantes)\nprint(restantes_ordenados)\n\n  [1] 2100 2110 2125 2170 2300 2330 2340 2345 2345 2395 2415 2570 2650 2660 2680\n [16] 2685 2695 2700 2710 2720 2730 2735 2750 2750 2770 2775 2780 2780 2795 2805\n [31] 2830 2845 2845 2850 2860 2870 2875 2880 2900 2900 2910 2920 2930 2935 2940\n [46] 2965 2980 2985 2985 3000 3010 3015 3020 3020 3025 3030 3030 3040 3040 3040\n [61] 3050 3050 3055 3055 3060 3060 3060 3075 3080 3080 3080 3090 3090 3105 3115\n [76] 3120 3120 3135 3145 3150 3150 3150 3150 3160 3160 3165 3165 3170 3180 3180\n [91] 3180 3200 3215 3230 3250 3260 3270 3270 3275 3280 3290 3295 3300 3300 3300\n[106] 3300 3300 3305 3305 3315 3320 3320 3320 3325 3330 3330 3335 3340 3340 3345\n[121] 3355 3370 3385 3395 3400 3405 3410 3410 3410 3415 3415 3425 3430 3430 3430\n[136] 3450 3460 3465 3465 3470 3480 3490 3490 3490 3495 3500 3500 3505 3510 3540\n[151] 3545 3545 3550 3550 3560 3570 3580 3585 3600 3610 3610 3625 3625 3635 3640\n[166] 3650 3660 3660 3665 3710 3715 3715 3730 3730 3770 3780 3795 3830 3840 3845\n[181] 3880 3890 3905 3920 3920 3920 3945 3970 3995 4025 4045 4080 4080 4090 4160\n[196] 4315 4350 4370 4485\n\n\n\nCálculo da mediana com os valores restantes ordenados. O valor central entre 1 e 199 é o 100º valor:\n\n\nmediana &lt;- restantes_ordenados[100]\nmediana\n\n[1] 3280\n\n\nImaginem que sempre que se for calcular a mediana houvesse necessidade de se proceder como realizado acima. Seria tedioso, quase um caos! Entretanto, usando o R, a situação fica bem mais agradável e intuitiva. O R facilita esse trabalho, fornecendo a função median().\nComo exemplo, será usada a variável apgar1 já incluída no dataframe dados. Como o Apgar é um escore (Pediatrics e Obstetricians 2006), a medida resumidora mais adequada, realmente, é a mediana.\n\nmedian (dados$apgar1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\n\n\n\n\nExercício\n\n\n\nRepetrir o cálculo da mediana dos valores dados$peso e destes dados quando foi extraído um valor..\n\n\n\n\n6.4.3 Moda\nModa (Mo) é o valor que ocorre com maior frequência em um conjunto de dados. O R não possui uma função nativa e direta para calcular a moda como tem para a média (mean()) e a mediana (median()). Isso acontece porque a moda pode não ser única em um conjunto de dados (podem existir múltiplos valores com a mesma frequência máxima) ou pode nem existir (se todos os valores ocorrerem apenas uma vez).Tem o menor nível de sofisticação. No entanto, pode-se facilmente criar uma função própria para calcular a moda ou usar pacotes que oferecem essa funcionalidade, como o DescTools que oferece uma função chamada Mode(). Aqui estão algumas maneiras de calcular a moda em R:\nFunção personalizada\n\nmoda &lt;- function(v) {\n  freq_tab &lt;- table(v)\n  max_freq &lt;- max(freq_tab)\n  moda &lt;- names(freq_tab[freq_tab == max_freq])\n  return(moda)\n}\n\nEsta função moda()é constituída por:\n\ntable(v): Cria uma tabela de frequência dos valores v.\nmax(freq_tab): Encontra a frequência máxima.\nfreq_table[freq_table == max_freq]: Seleciona as entradas da tabela de frequência que são iguais à frequência máxima.\nnames(…): Obtém os nomes (os valores originais) dessas entradas, que são as modas.\n\nUsando a função criada, a moda da variável dados$apgar1 é igual a:\n\nmoda (dados$apgar1) \n\n[1] \"8\"\n\n\nA função moda() pode ser salva em seu diretório de trabalho, na pasta das suas funções próprias. Quando necessário ela pode ser acessada, como foi visto na Seção 4.4.1.\nFunção Mode() do pacote DescTools\n\nlibrary(DescTools)\nmoda &lt;- Mode(dados$apgar1)\nprint(moda)\n\n[1] 8\nattr(,\"freq\")\n[1] 85\n\n\n\n\n6.4.4 Quantil\nUma medida de localização bastante utilizada são os quantis que são pontos estabelecidos em intervalos regulares que dividem a amostra em subconjuntos iguais. Se estes subconjuntos são em número de 100, são denominados de percentis; se são em número de 10, são os decis e em número de 4, são os quartis. A função nativa no R para obter o quantil é quantile().\nPara determinar os três quartis do peso dos recém-nascidos (dados$pesoRN), usa-se:\n\nquantile (dados$pesoRN, c (0.25, 0.50, 0.75))\n\n    25%     50%     75% \n3007.50 3285.00 3541.25 \n\n\nObserve que o percentil 50º é igual a mediana. O percentil 75º é o ponto do conjunto de dados onde 75% dos recém-nascidos têm um peso inferior a 3541.25g e 25% está acima deste valor.\n\n\n6.4.5 Média aparada\nAs médias aparadas são estimadores robustos da tendência central. Para calcular uma média aparada, é removida uma quantidade predeterminada de observações em cada lado de uma distribuição e realizada a média das observações restantes. Um exemplo de média aparada é a própria mediana.\nA base R tem como calcular a média aparada acrescentando o argumento trim =, proporção a ser aparada. Se for aparado 20%, usa-se trim = 0.2. isto significa que serão removidos 20% dos dados dos dois extremos. No caso da amostra de 15 recém-nascidos, serão removidos três valores mais baixos e três valores mais altos, passando a mostra a ter 9 valores, e a média aparada será a média destes 9 valores.\nO comando para obter a média aparada é:\n\nround(mean (dados$pesoRN, na.rm = TRUE, trim = 0.20), 1)\n\n[1] 3258.6",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html#medidas-de-dispersão",
    "href": "06-medidasResumidoras.html#medidas-de-dispersão",
    "title": "6  Medidas Resumidoras",
    "section": "6.5 Medidas de Dispersão",
    "text": "6.5 Medidas de Dispersão\n\n6.5.1 Amplitude\nA amplitude de um grupo de medições é definida como a diferença entre a maior observação e a menor.\nNo conjunto de dados dos pesos dos recém-nascidos, a amplitude pode ser obtida, no R, com a função range(), que retorna o valor mínimo e o máximo.\n\nrange (dados$pesoRN, na.rm = TRUE)\n\n[1] 2100 4485\n\n\n\n\n6.5.2 Intervalo Interquartil\nA intervalo interquartil (IIQ), também conhecido como amplitude interquartil (AIQ) é uma forma de média aparada. É simplesmente a diferença entre o terceiro e o primeiro quartil, ou seja, a diferença entre o percentil 75 e o percentil 25. Considere a variável escolaridade (dados$anosEst), anos de estudos completos.\nOs percentis 25 e 75 são obtidos, usando a função quantile(), vista acima, ou com a função summary() , que retorna os valores mínimo, primeiro quartil, mediana, média, terceiro quartil e máximo.\n\nquantile (dados$anosEst, c(0.25,0.75))\n\n25% 75% \n  5   9 \n\n\n\nsummary(dados$anosEst)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    5.00    7.50    7.29    9.00   16.00 \n\n\nPortanto, o IIQ está entre 5 a 9 anos de estudo ou, 4 anos de estudos completos. Em outras palavras, 50% das mulheres desta amostra têm entre 5 a 9 anos de estudo.\nO R possui uma função específica para calcular o intervalo interquartil, denominada IQR() e incluída no R base. Ela possui os seguintes argumentos:\nx \\(\\to\\) Representa o vetor numérico;\nna.rm \\(\\to\\) Este assume um valor lógico, TRUE ou FALSE, indicando se os valores ausentes devem ser removidos ou não;\ntype \\(\\to\\) Representa um número inteiro selecionando um dos muitos algoritmos de quantil. Este é um parâmetro opcional.\n\nIQR(dados$anosEst, na.rm = TRUE)\n\n[1] 4\n\n\n\n\n6.5.3 Variância e Desvio Padrão\nA variância e o desvio padrão fornecem uma indicação de quão aglomerados em torno da média os dados de uma amostra estão. Estes tipos de medidas representam desvios (erros)da média. Quando se verifica o desvio de cada valor (x) em relação à média \\(\\overline{x}\\), os desvios positivos se anulam com os negativos, resultando em uma soma igual a zero.\nA consequência deste fato é que não é possível resumir os desvios numa única medida de variabilidade. Para se chegar a uma medida de variabilidade há necessidade de se eliminar os sinais, antes de somar todos os desvios em relação à média.\nUma maneira de se fazer isso é elevar todas as diferenças ao quadrado. Assim, se obtém o desvio em relação à média elevado ao quadrado. A soma destes valores é denominada de Soma dos Quadrados (SQ) dos Desvios ou Soma dos Erros ao Quadrado. Se o interesse é apenas saber o erro ou desvio médio, divide-se por n (tamanho da amostra). No entanto, em geral o interesse se concentra em usar o desvio ou erro na amostra para estimar o erro na população. Dessa maneira, divide-se a Soma dos Quadrados por \\(n-1\\). Essa medida é conhecida como variância (\\(s^2\\)). O divisor, \\(n – 1\\), é denominado de graus de liberdade (gl) associados à variância.\nOs graus de liberdade representam o número de desvios que estão livres para variar. É um conceito de difícil explicação, mas é possível compreendê-lo, usando a seguinte explicação:\n\n\n\n\n\n\nGraus de liberdade\n\n\n\nSuponha uma maternidade há 50 anos atrás, quando não havia alojamento conjunto. Nessa época era comum os recém-nascidos normais ficarem em um berçário. A cada horário de amamentação eles eram levados para os quartos de suas mães para mamar. Posteriormente, eram trazidos para o berçário e colocados nos berços até a próxima mamada. Suponha que, em um determinado momento, havia 15 bebês e que, no berçário, existiam 15 berços (postos) para colocá-los durante o intervalo das mamadas. Quando o primeiro recém-nascido chega, a enfermeira poderá escolher qualquer um dos berços para o colocar. Depois, quando o próximo recém-nascido chegar, ela terá 14 opções de escolha, pois um dos berços está ocupado. Ainda existe uma boa liberdade de escolha. No entanto, à medida que os recém-nascidos forem sendo trazidos para o berçário, chegará a um ponto em que 14 berços estarão ocupados. Agora, a enfermeira não terá liberdade de escolha, pois só resta um berço. Nesse exemplo existem 14 graus de liberdade (gl)2. Para o último recém-nascido não houve liberdade de escolha (Field, Miles, e Field 2012). Portanto, os graus de liberdade são iguais ao tamanho da amostra menos um (\\(n-1\\)).\n\n\nA variância é a razão entre a soma dos quadrados e os graus de liberdade (observações realizadas menos um).\n\\[\ns^2= \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\n\\]\nNo R existem embutidas as funções sd() e var()que facilmente calculam essas medidas de dispersão.\nUsando a variável dados$pesoRN, tem-se:\n\nvar(dados$pesoRN, na.rm =TRUE)\n\n[1] 199704.3\n\n\nO desvio padrão é a raiz quadrada da variância: \\(s = \\sqrt var\\)\n\nsqrt (var(dados$pesoRN))\n\n[1] 446.8829\n\n\nOu, usando a função sd() e arredondando para 1 dígito decimal:\n\nround(sd (dados$pesoRN, na.rm = TRUE), 1)\n\n[1] 446.9\n\n\nA variância e desvio padrão são medidas de variabilidade e revelam quão bem a média representa os dados. Informa se ela está funcionando bem como modelo. Pequenos desvios padrão mostram que existe pouca variabilidade nos dados, que eles se aproximam da média. Quando existe um grande desvio padrão, a média não é muito precisa para representar os dados.\nO desvio padrão, além de medir a precisão com que a média representa os dados, também informa sobre o formato dos dados e por isso é uma medida de dispersão. Em uma amostra onde desvio padrão é pequeno, os dados se agrupam próximo a média e o formato da distribuição fica mais pontiagudo (curva em azul, Figura 6.1). Nesse caso a média representa bem os dados. Em outra amostra, com a mesma média anterior, mas com os dados mais dispersos entorno da média, o desvio padrão é maior e o formato da distribuição fica achatado (curva verde, na Figura Figura 6.1). Nesse caso a média não é uma boa representação dos dados.\n\n\n\n\n\n\n\n\nFigura 6.1: Dispersão dos dados em torno da média.\n\n\n\n\n\n\n\n6.5.4 Coeficiente de Variação\nO desvio padrão por si só tem limitações. Um desvio padrão igual a 2 pode ser considerado pequeno para um conjunto de valores cuja média é 100. Entretanto, se a média for 5, ele se torna muito grande. Além disso, o desvio padrão por ser expresso na mesma unidade dos dados, não permite aplicá-lo na comparação de dois ou mais conjunto de dados que têm unidades diferentes. Para eliminar essas limitações, é possível caracterizar a dispersão ou variabilidade dos dados em termos relativos, usando uma medida denominada Coeficiente de Variação (CV), também conhecido como como Desvio Padrão Relativo ou Coeficiente de Variação de Pearson. É expresso, em geral como uma porcentagem, sendo definido como a razão do desvio padrão pela média:\n\\[\nCV = \\frac{s}{\\overline{x}}\n\\]\nMultiplicando o valor da equação por 100 tem-se o CV percentual. O R não possui uma função específica para calcular o CV.\nFoi criada uma função específica para isso,já multiplicada por 100.\n\ncoef_var &lt;- function (valores) {\n  (sd(valores, na.rm=TRUE) / mean(valores, na.rm=TRUE))*100}\n\nPortanto, o CV da variável dados$pesoRN é igual a:\n\nround (coef_var (dados$pesoRN),1)\n\n[1] 13.7\n\n\nUsdando outra variável do banco de dados, por exemplo, dados$idadeMae, o CV será igual a:\n\nround(coef_var (dados$idadeMae), 1)\n\n[1] 25\n\n\nO peso do recem-nascido tem um CV = 13.7 % e a idade materna um CV = 25 %, mostrando que esta tem uma maior variabilidade. Quanto menor o desvio padrão, menor o CV e, consequentemente, menor a variabilidade. Um CV \\(\\ge\\) 50%, sugere que a variável tem uma distribuição assimétrica.\n\n\n6.5.5 Escolha da medida resumidora\nA seleção da medida de tendência central mais adequada depende de vários fatores, incluindo a natureza dos dados e do propósito da sumarização.\nO tipo da variável tem substancial influência na escolha da medida de tendência central a ser usada. A moda é mais apropriada para dados nominais e seu uso com variáveis ordinais resulta em uma perda no poder em termos de informação que se poderia obter dos dados.\nA mediana é mais adequada para variáveis ordinais, embora possa ser usada para variáveis contínuas, especialmente quando a distribuição dos dados é assimétrica. A mediana não deveria ser usada com dados nominais porque os postos assumidos não podem ser obtidos com dados de nível nominal.\nFinalmente, a média somente deve ser usada com dados contínuos simétricos, se houver assimetria a mediana deve ser preferida.\nAs medidas de dispersão devem estar associadas a uma medida de tendência central. Elas caracterizam a variabilidade dos dados na amostra. Com dados ordinais usar a amplitude ou o intervalo interquartil. O desvio padrão não é apropriado em dados ordinais devido à natureza não numérica destes.\nCom os dados numéricos deve-se usar o desvio padrão, que utiliza toda a informação nos dados, ou o intervalo interquartil (IIQ). Quando os dados forem simétricos, usar a média acompanhada do desvio padrão, caso contrário, usar a mediana e o IIQ. Não misturar e combinar medidas (Bowers 2008).\n\n\n\n\n\n\nBowers, David. 2008. «First things first-the nature of data». Em Medical Statistics from Scratch, Second Edition, 3–13. John Wiley; Sons.\n\n\nField, Andy, Jeremy Miles, e Zoë Field. 2012. «Everithing you ever wanted to know about statistics (well, sort of)». Em Discovering statistics using R, 38. Sage Publications, Ltd.\n\n\nPediatrics, American Academy of, e American College of Obstetricians. 2006. «The apgar score». Pediatrics 117 (4). American Academy of Pediatrics: 1444–47.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "06-medidasResumidoras.html#footnotes",
    "href": "06-medidasResumidoras.html#footnotes",
    "title": "6  Medidas Resumidoras",
    "section": "",
    "text": "Usa-se esta sintaxe: round(x, digits = 0), onde x é o numero que se quer arredondar e digits é número de casas decimais. O padrão é 0, ou seja, arredonda para o inteiro mais próximo.↩︎\nNo texto deste livro, graus de liberdade são abreviados por gl. Muitas vezes, é usado a abreviação df de degree of freedom porque em algumas funções aparecem como df e optou-se por manter sem a tradução.↩︎",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Medidas Resumidoras</span>"
    ]
  },
  {
    "objectID": "07-tabelas.html",
    "href": "07-tabelas.html",
    "title": "7  Tabelas",
    "section": "",
    "text": "7.1 Componentes de uma tabela\nA apresentação tabular de dados, ou tabulação, é a organização dos dados por meio de tabelas. Uma tabela é uma disposição sistemática e lógica dos dados na forma de linhas e colunas com relação às características dos dados. É uma forma ordenada, compacta, autoexplicativa e eficiente de mostrar os dados levantados, facilitando a sua compreensão e interpretação. Permite identificar padrões, tendências e intuições valiosos.\nA tabela mais utilizada para descrever os dados é a tabela de frequência.\nUma tabela bem estruturada é essencial para comunicar dados de forma clara e científica, especialmente em publicações acadêmicas. Os componentes fundamentais que uma tabela (Figura 7.1) deve conter, para estar pronta para publicação, são os seguintes:\nFigura 7.1: Elementos de uma Tabela",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tabelas</span>"
    ]
  },
  {
    "objectID": "07-tabelas.html#componentes-de-uma-tabela",
    "href": "07-tabelas.html#componentes-de-uma-tabela",
    "title": "7  Tabelas",
    "section": "",
    "text": "Título da Tabela - deve ser claro, conciso e informativo. Indica o conteúdo da tabela e o contexto do estudo. Deve estar no topo da tabela.\nCabeçalho - Identifica as variáveis apresentadas. Inclui unidade de medidas quando necessário (ex: idade em anos, glicemia em md/dL)\nCorpo da tabela - Apresenta os valores estatísticos: média, mediana, desvio padrão, intervalo de confiança, etc. Pode incluir frequências absolutas e relativas (em proporção ou porcentagem). Os dados devem estar alinhados corretamente para facilitar a leitura.\nNotas de rodapé - Explicações adicionais sobre abreviações, símbolos ou indicação de testes estatísticos aplicados (ex: teste t de Student, ANOVA, qui-quadrado). Pode incluir significância estatística (p &lt; 0,05).\nFonte dos dados - Caso os dados sejam secundários ou provenientes de outra pesquisa, deve-se citar a fonte.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tabelas</span>"
    ]
  },
  {
    "objectID": "07-tabelas.html#tabelas-de-frequências",
    "href": "07-tabelas.html#tabelas-de-frequências",
    "title": "7  Tabelas",
    "section": "7.2 Tabelas de Frequências",
    "text": "7.2 Tabelas de Frequências\nAs tabelas de frequências são essenciais em estatística para organizar e resumir dados. Ajudam a entender como os valores em um conjunto de dados se distribuem, mostrando a frequência com que cada valor ou intervalo de valores aparece.\nBasicamente, uma tabela de frequências responde a uma pergunta simples: “Quantas vezes cada coisa aconteceu?”.\nDe um modo geral, uma tabela de frequência deve cumprir algumas finalidades:\n\nClareza e Organização: Elas transformam dados brutos e desorganizados em informações claras e fáceis de interpretar.\n\nIdentificação de padrões: Uma tabela deve ajudar a visualizar rapidamente os valores mais comuns (a moda) e a distribuição geral dos dados.\n\nBase para Outras Análises: São o ponto de partida para criar gráficos como histogramas, polígonos de frequência e gráficos de setores, que oferecem uma representação visual ainda mais intuitiva.\n\nTomada de decisão: Permitem que se tire conclusões rápidas sobre um conjunto de dados. Por exemplo, na Tabela 7.1 , se verifica que “a maioria das gestantes, no Hospital Geral de Caxias do Sul, referem não ser drogaditas” ou “72% das gestantes, no Hospital Geral de Caxias do Sul, referem não ser drogaditas”.\n\n\n7.2.1 Tipos de Frequências\nGeralmente, em uma tabela de frequência aparecem os seguintes tipos de frequência:\n\n7.2.1.1 Frequência Absoluta (f)\nÉ o número de vezes que um valor específico ocorre no conjunto de dados. Por exemplo, na Tabela 7.1, 913 das parturientes referem não usar drogas. Podem ser:\n\nDistribuição de frequência não agrupada\nAs distribuições de frequência não agrupada listam cada valor individual de um conjunto de dados com a sua respetiva contagem (frequência), sendo ideais para conjuntos de dados pequenos ou com poucas categorias. A Tabela 7.1 mostra a frequência da variável drogadição na gestação, com os diferentes tipos de drogas usadas\nDistribuição de frequência agrupada\nAs distribuições de frequência agrupadas organizam os dados em intervalos ou classes, úteis para conjuntos de dados extensos, pois condensam a informação em grupos menores., como faixas etárias , estado nutricional da gestante na gravidez, usando o IMC categorizado, idade gestacional categorizada em pré-termo, termo e pós-termo.\n\n\n\n7.2.1.2 Frequência Relativa (fr)\nÉ a proporção da frequência absoluta em relação ao total de dados. É calculada dividindo a frequência absoluta pelo número total de observações (\\(fr=f/n\\)).\nA frequência relativa pode ser expressa como uma fração, decimal ou, mais comumente, como uma porcentagem (frp). É útil para comparar distribuições de dados com tamanhos totais diferentes. Por exemplo, na Tabela 7.1, 2,13% da das parturientes declararam ser alcoolistas.\n\n\n\n7.2.1.3 Frequência Absoluta Acumulada (F)\nÉ a soma das frequências absolutas até um determinado ponto na tabela. Ela mostra quantos dados estão abaixo ou iguais a um certo valor.Na Tabela 7.1 não tem importância, são variáveis nominais sem uma ordem estabelecida. Não faz sentido somar “pessoas que fumantes” com “pessoas não usuárias de drogas”. As frequências cumulativas não fazem sentido para variáveis nominais porque os valores não têm ordem — um valor não é maior ou menor que outro valor.\n\n\n7.2.1.4 Frequência Relativa Acumulada (Fr)\nÉ a soma das frequências relativas até um determinado ponto. Ela mostra a proporção de dados que estão abaixo ou iguais a um certo valor.Assim como an frequência absoluta acumulada, não faz sentido usar a frequência relativa acumulada com dados nominais, onde a ordem das categorias não tem valor.\n\n\n\n\nTabela 7.1: Distribuição de frequência de drogadição em gestantes\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Drogadição em Gestantes1\n    \n    \n    \n      Droga\n      Frequência Absoluta2\n      Frequência Relativa (%)\n    \n  \n  \n    Nenhuma\n913\n72.06\n    Fumo\n301\n23.76\n    Álcool\n27\n2.13\n    Medicamentos\n23\n1.82\n    Crack\n2\n0.16\n    Cocaína\n1\n0.08\n    Total\n1267\n100.00\n  \n  \n    \n      Fonte: Oliveira Filho, PF (2025)\n    \n  \n  \n    \n      1 Hospital Geral de Caxias do Sul, RS, 2008\n    \n    \n      2 101 gestantes não informaram a condição de drogadição\n    \n  \n\n\n\n\n\n\n\n\n\n\n7.2.2 Regras gerais para construção de tabelas de frequência\nExistem algumas recomendações na construção de uma tabela de frequência (Arango 2009):\n\nDeve ter um título na parte superior que responda as perguntas: “o que? quando? onde?” relativas ao fato estudado;\nDeve ter um rodapé, na parte inferior da tabela, onde se coloca notas necessárias e a fonte dos dados;\nAs colunas externas da tabela devem ser abertas, o emprego de linhas verticais para a separação das colunas no corpo da tabela é opcional;\nNa parte superior e inferior, as tabelas devem, ser fechadas por linhas horizontais;\nNenhuma casela deve ficar vazia, apresentando um número ou um símbolo. Se não se dispuser do dado, colocar reticências … e a presença de um X representa que o dado foi omitido para evitar a identificação.\nTabelas devem ter, pelo menos, duas linhas no seu corpo e, prioritariamente, pelo menos três colunas. Tabelas com duas colunas devem ser evitadas, pois as informações contidas nelas podem, em geral, ser apresentadas no corpo do texto da obra.\n\n\n\n7.2.3 Construção de tabelas de frequência no R\n\n7.2.3.1 Dados para os exemplos\nPara os exemplos, o dataframe dadosMater.xlsx será acionado para fornecer dados, semelhante ao realizado na Seção 6.3 .\nApós carregar o dataframe, serão selecionadas as variáveis necessárias. As variáveis utiNeo, sexo serão convertidas a fatores; a variável idadeMae será categorizada em três níveis (&lt;20 anos, 20-35 anos, &gt;35 anos), criando uma nova variável categIdade. Além disso, será criada a variável imc que também será categorizada em uma nova variável estNutri em quatro níveis (Baixo Peso, Peso adequado, Sobrepeso, Obesidade). Por último, será extraída uma amostra de n = 200 e atribuída a um objeto dados:\n\nlibrary(dplyr)\nlibrary(readxl)\n\nset.seed(123)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(idadeMae, peso, altura, anosEst, fumo, pesoRN, apgar1, utiNeo) %&gt;% \n  mutate(fumo = factor(fumo, \n                       levels = c(1,2), \n                       labels = c(\"fumante\", \"não fumante\")),\n         utiNeo = factor(utiNeo, \n                         levels = c(1,2), \n                         labels = c(\"sim\", \"não\")),\n         categIdade = case_when(\n           idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n           idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n           idadeMae &gt; 35 ~ \"&gt; 35 anos\"),\n         categIdade = factor(categIdade, \n                             levels = c(\"&lt; 20 anos\", \"20 a 35 anos\", \"&gt; 35 anos\")),\n         imc = peso/altura^2,\n         estNutri = case_when(\n           imc &lt; 18.5 ~ \"Baixo Peso\",\n           imc &gt;= 18.5 & imc &lt; 25 ~ \"Peso adequado\", \n           imc &gt;= 25 & imc &lt; 30 ~ \"Sobrepeso\",\n           imc &gt;= 30 ~ \"Obesidade\"),\n         estNutri = factor(estNutri, \n                           levels = c(\"Baixo Peso\", \"Peso adequado\", \n                                      \"Sobrepeso\", \"Obesidade\"))) %&gt;%\n  slice_sample(n=200) \n\nstr(dados)\n\ntibble [200 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:200] 16 19 27 18 29 38 15 26 27 22 ...\n $ peso      : num [1:200] 68 49 64 53 76 55 64 81 62 59.5 ...\n $ altura    : num [1:200] 1.59 1.62 1.78 1.65 1.64 1.6 1.69 1.65 1.72 1.65 ...\n $ anosEst   : num [1:200] 9 11 13 7 11 4 7 6 8 11 ...\n $ fumo      : Factor w/ 2 levels \"fumante\",\"não fumante\": 2 2 1 2 2 2 1 2 2 2 ...\n $ pesoRN    : num [1:200] 2940 3060 1930 2790 1750 ...\n $ apgar1    : num [1:200] 8 9 NA 9 NA 4 9 8 8 9 ...\n $ utiNeo    : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 2 2 2 2 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 1 1 2 1 2 3 1 2 2 2 ...\n $ imc       : num [1:200] 26.9 18.7 20.2 19.5 28.3 ...\n $ estNutri  : Factor w/ 4 levels \"Baixo Peso\",\"Peso adequado\",..: 3 2 2 2 3 2 2 3 2 2 ...\n\n\n\n\n7.2.3.2 Tabelas de frequência para dados categóricos\nFunção table() e função prop.table\nO R básico possui uma função incorporada, table() que permite a verificação da frequência absoluta de variáveis categóricas de uma maneira bem simples. A função table() é a base para criar tabelas de frequência. Ela conta o número de ocorrências de cada valor em um vetor (ou coluna de um dataframe). Por exemplo, a frequência absoluta (f) em cada uma das categorias da variável categIdade: (idade das parturientes dividida em classes):\n\nf  &lt;- table (dados$categIdade)\nprint (f)\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n          33          144           23 \n\n\nA função prop.table() é usada para calcular as frequências relativas a partir de uma tabela de frequência absoluta. Em outras palavras, ela transforma as contagens em proporções ou porcentagens. A funçaõ round() recebe a função prop.table para arredondar para três dígitos a frequência relativa (fr):\n\nfr  &lt;- round(prop.table(f), 3)\nprint(fr)\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n       0.165        0.720        0.115 \n\n\nMultiplicando por 100 a fr, tem-se a frequência relativa em porcentagem – frp ou fr(%). A operação será colocada dentro da função round(), como feito com a fr, arredondando o resultado para dois dígitos.\n\nfrp &lt;- round(fr*100, 2)\nprint(frp)\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n        16.5         72.0         11.5 \n\n\nEmbora as funções table() e prop.table() sejam separadas, é muito comum usá-las juntas para construir uma tabela de frequência completa. Portanto, os passos para a contrução de uma tabela de frequência no R são os segintes\nPara atingir este objetivo, a construção de uma tabela de frequência absoluta e de frequência relativa (%) deve ser o primeiro passo. Os passos seguintes são;\nPasso 1:Usar as funções table() e prop.table() juntas como feito acima, para construir as frequências absoluta e relativa.\nPasso 2: Criar um dataframe combinando as funções para uma melhor visualização:\n\ntab_completa &lt;- data.frame(\n     f = as.vector(f),\n     fr = as.factor(fr),\n    frp = as.vector(frp))\n\nprint(tab_completa)\n\n               f    fr  frp\n&lt; 20 anos     33 0.165 16.5\n20 a 35 anos 144  0.72 72.0\n&gt; 35 anos     23 0.115 11.5\n\n\nEste último exemplo mostra a flexibilidade do R para manipular e apresentar dados de forma clara e organizada, combinando as saídas de funções básicas em uma única tabela.\nPara análises mais avançadas ou para variáveis quantitativas contínuas, pode-se usar a função mutate() ,junto com a função case_when() (GeeksforGeeks 2025) , usada para criar a variáveis categIdade e estNutri, na Seção 7.2.3.1. Também é possível associar a função mutate() com a função cut(), como se verá a seguir.\n\n\n7.2.3.3 Tabelas de frequência para dados numéricos\nA construção de tabelas de frequência com dados agrupados em classes é crucial quando se lida com variáveis quantitativas contínuas, como idade, altura, peso do recém-nascido, renda, etc. Usar table() diretamente, nesses casos não faz sentido, pois cada valor pode ser único e o número de “categorias” seria imenso, tornando a tabela quase inútil. A solução é agrupar os dados em intervalos ou classes. Para realizar esta operação, pode-se acionar a mesma função mutate() associada a função cut(), que divide um vetor numérico em fatores com base em intervalos especificados.\nPasso a passo com a função cut()\nComo exemplo prático, a variável de interesse será representada pelos pesos dos recém-nascidos (pesoRN) do conjunto de dados, dados, mencionado na (Seção 7.2.3.1).\nPasso 1: Definir os intervalos de classe\nAntes, as classes foram estabelecidas de acordo com algum critério escolhido pelo autor ou por algum critério relacionado à saúde, como por exemplo a idade materna, onde as gestante com idade abaixo de 20 anos e acima de 35 anos, gestantes com baixa escolaridade (&lt; de 5 anos), situação conjugal insegura, etc., constituem fatores de risco na gravidez (Schirmer e Outros 2000). Em geral, quando não há um padrão pré-determinado, o número de classes é estabelecido de acordo com o tamanho da amostra. Este número pode ser escolhido lembrando-se das oscilações que ocorrem nos dados e do interesse do pesquisador em mostrar seus dados. Não existe uma regra totalmente eficiente para determinar o número de classes. É importante ter bom senso, de maneira que seja possível ver como os valores se distribuem. Uma regra geral é usar entre 5 e 15 classes (Daniel e Cross 2013), mas isso pode variar. Com poucas classes, perde-se precisão e, com muitas classes, a tabela torna-se muito extensa. Uma forma comum é usar a Regra de Sturges 1, para estimar o número de classes. Baseado na regra de Sturges é sugerido usar a recomendação da Tabela 7.2.\n\n\n\n\nTabela 7.2: Numero de classes, baseado em Sturges\n\n\n\nNº de observações (n)Nº de classes11223 a 536 a 11412 a 23524 a 46647 a 93794 a 1878188 a 3769377 a 75610FONTE:Arango, H. G. (2009)\n\n\n\n\n\nPara a variável dados$pesoRN, como existem 200 observações, pode-se, de acordo com a Tabela 7.2, usar ao redor de 9 classes. O R tem uma função nclass.Sturges (), que também pode ser usada:\n\nk &lt;- nclass.Sturges(dados$pesoRN)\nprint(k)\n\n[1] 9\n\n\nO cálculo da função retorna o mesmo resultado.\nPasso 2: Amplitude e limites das classes\nA classe possui um limite inferior e um limite superior. O importante é que os limites dos intervalos sejam mutuamente exclusivos, isto é, cada valor deve ser representado em um único intervalo. Além disso, os intervalos devem ser exaustivos, isto é, devem conter todos os valores possíveis entre o valor mínimo e o máximo. O recomendado é que as classes sejam homogêneas, ou seja, tenham a mesma amplitude2. A amplitude dos valores pode ser obtida com a função range():\n\namplitude &lt;- range(dados$pesoRN) \namplitude\n\n[1]  810 4670\n\n\nUsando esta amplitude dos dados, é possível ter a largura das classes (h):\n\nh &lt;- round(diff(amplitude)/k, 0)\nh\n\n[1] 429\n\n\nA fórmula é apenas a diferença absoluta dos valores mínimos e máximo , calculada pela função diff() 3, dividida pelo número de classes (k), arredondado com o a função round ().\nA partir desses dados, é possível construir as classes: O limite inferior da primeira classe é 810 e o limite superior é 1239, exclusive; o limite inferior da segunda classe inclui o valor de 1239 e vai até 1668, excluindo-o, pois ele será limite inferior da terceira classe e assim por diante\nPasso 3: Agrupar os dados usando cut()\nSeguir esse processo seria longo, tedioso e sujeito a erros. O R permite agilizar este trabalho com as funções mutate() e função cut(). Dentro da função cut(), pode ser acionada a função seq() para estabelecer os pontos de corte.\n\nCriar uma sequência dos pontos de corte desejados, usando a função seq():\n\n\n\n\n\n\nSintaxe função seq()\n\n\n\nNecessita os seguintes argumentos:\n\nfrom : valor inicial da sequência\nto: valor final da sequência\nlength.out: número inteiro que especifica o comprimento desejado da sequência. Este último argumento é importante, pois se o objetivo é construir  9 classes, preciso de uma sequência de 10 valores.\n\n\n\n\nComo os pontos de cortes precisam ser números inteiros, usa-se a função round() com 0 (zero) dígitos:\n\nvalor_min &lt;- min(dados$pesoRN)    # valor inicial\nvalor_max &lt;- max(dados$pesoRN)    # valor final\nlength.out = k + 1                # 10 valores para obter 9 classes\n\ncortes &lt;- round(seq(valor_min, valor_max, length.out = k + 1), 0)  \n\n\nTendo os pontos de corte estabelecidos, basta agora criar a variável pesoCateg, ou seja a variável pesoRN numérica classificada em 9 classes:\n\n\ndados &lt;- dados%&gt;% \n  mutate(pesoCateg = cut(\n    pesoRN,\n    breaks = cortes,\n    include.lowest = TRUE,\n    right = FALSE,\n    ordered_result = TRUE))\n\n\nCriar uma tabela de frequência absoluta com a variável pesoCateg:\n\n\nf &lt;- table(dados$pesoCateg)\nprint(f)\n\n\n     [810,1.24e+03) [1.24e+03,1.67e+03)  [1.67e+03,2.1e+03)  [2.1e+03,2.53e+03) \n                  4                   5                  10                  14 \n[2.53e+03,2.95e+03) [2.95e+03,3.38e+03) [3.38e+03,3.81e+03) [3.81e+03,4.24e+03) \n                 47                  62                  42                  12 \n[4.24e+03,4.67e+03] \n                  4 \n\n\nComo diria , o famoso personagem Obelix, amigo inseparável do gaulês Asterix4 ; “Os céus caíram sobre nós! O que é isso? Vocês são loucos romanos!! Por Júpiter!!!”\nNada de especial. simplesmente, o R retornou valores em notação científica. Não tem nada errado! Apenas, isso torna a legibilidade dos números mais complicada, menos intuitiva. A primeira classe [810,1.24e+03) ficaria mais fácil de ser lida se estivesse como [810, 1240), ou seja, inclui o valor de 810 e exclui o 1240 e a última classe [4.24e+03,4.67e+03] inclui ambos os limites, melhor escritos como [4240, 4670].\n\n\n\n\n\n\nNota\n\n\n\nObserve a lógica para o último intervalo, que é fechado nos dois lados [ ] , incluindo os valores no intervalo de classe. Os outros obedecem a lógica [ ), incluindo o valor limite inferior e excluindo o superior. A forma como isso ocorre é determinado pela função cut() , que cria os intervalos (variável categPeso), depende de dois argumentos lógicos:\n\nright = TRUE (padrão): Os intervalos são do tipo (X, Y]. Isso significa que o valor X não é incluído, mas o valor Y é. O primeiro intervalo é a exceção, sendo [X, Y].\nright = FALSE: Os intervalos são do tipo [X, Y), como no exemplo usado, [810,1240). Isso significa que o valor 810 é incluído, mas o valor 1240 não é. O último intervalo é a exceção, sendo [X, Y]. No exemplo, [4240,4670] e significa que os dois valores estão incluídos.\nO argumento include.lowest entra em jogo, modificando o comportamento padrão:\n\nQuando se usa right = FALSE (como o código usado acima), o R cria intervalos [X, Y), no exemplo, igual a [810,1240). O argumento include.lowest = TRUE faz com que o primeiro intervalo seja [mínimo, Y), garantindo que o valor mínimo da variável - 810 - seja incluído no primeiro intervalo.\nResumindo:\n1) right = FALSE e include.lowest = TRUE:\n\nIntervalos: [A, B), [C, D), [E, F), e assim por diante.\nO primeiro intervalo ([min(x), …) ) inclui o valor mínimo.\nO último intervalo ([…, max(x)]) inclui o valor máximo.\n\n2) right = TRUE (padrão) e include.lowest = FALSE (padrão):\n\nIntervalos: (A, C], (C, D], (E, F].\nO primeiro intervalo [min(x), …] inclui o valor mínimo.\n\n\n\n\nPara que os rótulo apareçam sem notação científica5 e no mesmo formato [) , com exceção da última classe que ficará [], procede-se do seguinte modo:\n\n\nrotulos &lt;-  paste0(\"[\", \n                   format(head(cortes, -1), \n                          scientific = FALSE),\n                   \",\",\n                   format(tail(cortes, -1), \n                          scientific = FALSE),\n                   \")\")\ndados$pesoCateg &lt;- factor(rotulos[dados$pesoCateg], \n                          levels = rotulos, \n                          ordered = TRUE)\n\nPasso 4: Construção da tabela de frequência\nCom os dados agrupados em classes, agora pode-se usar as funções table() e prop.table() para construir a tabela de frequência absoluta e relativa, com realizado anteriormente.\n\n# Frequência Absoluta\nf &lt;- table(dados$pesoCateg)\nprint(f)\n\n\n[ 810,1239) [1239,1668) [1668,2097) [2097,2526) [2526,2954) [2954,3383) \n          4           5          10          14          47          62 \n[3383,3812) [3812,4241) [4241,4670) \n         42          12           4 \n\n# Frequência Relativa Percentual\nfrp &lt;- prop.table(f) * 100\nprint(frp)\n\n\n[ 810,1239) [1239,1668) [1668,2097) [2097,2526) [2526,2954) [2954,3383) \n        2.0         2.5         5.0         7.0        23.5        31.0 \n[3383,3812) [3812,4241) [4241,4670) \n       21.0         6.0         2.0 \n\n\nTabela de Frequência Completa\nPara uma apresentação mais clara, combinar tudo em um dataframe, adicionando as colunas de frequência acumulada.\n\n# Dataframe com as f e fr como vetores\ntab_frequencia &lt;- data.frame(f = as.vector(f),\n                             frp = as.vector(frp))\n  \n# Adicionar Frequência Absoluta Acumulada\ntab_frequencia$F &lt;- cumsum(tab_frequencia$f)\n\n# Adicionar Frequência Relativa Acumulada (%)\ntab_frequencia$Frp &lt;- cumsum(tab_frequencia$frp)\n  \n# Adicionar os intervalos como uma coluna  \ntab_frequencia$Classes &lt;- names(table(dados$pesoCateg))\n\n# Reorganizar as colunas para melhor visualização\ntab_frequencia &lt;- tab_frequencia[c(5, 1, 2, 3, 4)]\n\nprint(tab_frequencia)\n\n      Classes  f  frp   F   Frp\n1 [ 810,1239)  4  2.0   4   2.0\n2 [1239,1668)  5  2.5   9   4.5\n3 [1668,2097) 10  5.0  19   9.5\n4 [2097,2526) 14  7.0  33  16.5\n5 [2526,2954) 47 23.5  80  40.0\n6 [2954,3383) 62 31.0 142  71.0\n7 [3383,3812) 42 21.0 184  92.0\n8 [3812,4241) 12  6.0 196  98.0\n9 [4241,4670)  4  2.0 200 100.0\n\n\nA saída retorna uma tabela de frequência completa, mostrando a distribuição dos pesos dos recém-nascidos por classes.\nA combinação de mutate(). cut(), table() e prop.table() é a abordagem padrão e mais robusta para criar tabelas de frequência com dados agrupados no R.`\nMesmo que seja possível observar com clareza como os pesos dos recém-nascidos se distribuem, fica pouco intuitivo analisar, levando em conta as classificações recomendadas pela Organização Mundial da Saúde (OMS) e Mistério da Saúde do Brasil (MS), através do Guia para os Profissionais de Saúde de Atenção à Saúde do Recém-Nascido. Esta tabela de frequência serve mais para se observar o comportamento da variável pesoRN. Entretanto, essa ação é mais adequadamente realizada, utilizando-se um gráfico do tipo histograma (veja Seção 8.5).\nA tabela construída será repetida ,usando para classificar, a função case_when() e a classificação dos recém-nascidos de acordo com a OMS.\nPasso a passo com a função case_when()\nA função cut() foi já usada para classificar a variável pesoRN, entretanto, no início deste capítulo na Seção 7.2.3.1 , para realizar esta ação, foi usada a função case_when() para agrupar a variável idadeMae e imc em classes. A principal diferença entre essas funções, é que cut() é especificamente projetada para agrupar dados em intervalos numéricos, enquanto case_when() é uma ferramenta mais generalista e poderosa do pacote dplyr, que permite criar classes com base em qualquer tipo de condição lógica. A função case_when() pode ser vantajosa por apresentar flexibilidade total, permitindo definir intervalos com bastante facilidade. Além disso para pessoas familiarizadas com a sintaxe do tidyverse (veja Seção 5.4), a estrutura condição ~ valor de case_when() é extremamente intuitiva e fácil de entender. Como faz parte do tidyverse, case_when() se encaixa perfeitamente em fluxos de trabalho que usam funções como mutate() e group_by(). Os dados dos pesos dos recém-nascidos (pesoRN) servirão coo exemplo prático para criar uma tabela mais útil e comparação entre as duas funções.\nPasso 1: Criação da variável classePeso, usando o critério da OMS e das de frequência absoluta e relativa percentual:\n\n\n\n\n\n\nClassificação dos receém-nascidos de acordo com o peso (OMS)\n\n\n\nExtremo baixo peso ao nascer: RNs com peso inferior a 1.000 gramas.\nMuito baixo peso ao nascer: RNs com peso inferior a 1.500 gramas.\nBaixo peso ao nascer: RNs com peso inferior a 2.500 gramas.\nAdequado peso ao nascer: RNs com peso inferior a 4000 gramas.\nExcesso de peso ao nascer: RNs com peso ≥ 4000 gramas.\n\n\n\n# Classificação dos RNs pelo citério da OMS -- classePeso\ndados &lt;- dados %&gt;%  \n  mutate(classePeso = case_when(\n    pesoRN &lt; 1000 ~ \"BP Extremo\",\n    pesoRN &gt;= 1000 & pesoRN &lt; 1500 ~ \"Muito BP\", \n    pesoRN &gt;= 1500 & pesoRN &lt; 2500 ~ \"Baixo Peso\",\n    pesoRN &gt;= 2500 & pesoRN &lt; 4000 ~ \"Peso Normal\",\n    pesoRN &gt;= 4000 ~ \"Excesso de Peso\"\n  )) %&gt;% \n  mutate(classePeso = factor(classePeso, \n                             levels = c(\"BP Extremo\", \"Muito BP\", \"Baixo Peso (BP)\",\n                                        \"Peso Normal\", \"Excesso Peso\")))  \n\n# Tabela de frequência absoluta\nf &lt;- table(dados$classePeso)\nprint(f)\n\n\n     BP Extremo        Muito BP Baixo Peso (BP)     Peso Normal    Excesso Peso \n              2               5               0             163               0 \n\n# Tabela de frequência relativa (em porcentagem)\nfrp &lt;- round(prop.table(f) * 100, 1)\nprint(frp)\n\n\n     BP Extremo        Muito BP Baixo Peso (BP)     Peso Normal    Excesso Peso \n            1.2             2.9             0.0            95.9             0.0 \n\n\nPasso 2: Combinar as frequências em um dataframe\n\ntab_completa &lt;- data.frame(\n  f = as.vector(f),\n  frp = as.vector(frp))\n\nPasso 3: Adicionar a frequência absoluta e a frequência relativa (em %) acumuladas\n\n# Frequência Absoluta Acumulada\ntab_completa$F &lt;- cumsum(tab_completa$f)\n\n# Frequência Relativa Acumulada (%)\ntab_completa$Frp &lt;- cumsum(tab_completa$frp)\n\nPasso 4: Adicionar os nomes das categorias como uma coluna\n\ntab_completa$classePeso &lt;- rownames (f)\n\nPasso 5: Renomeie a coluna classePeso como “Classificação”\n\nlibrary(dplyr)\ntab_completa &lt;- tab_completa %&gt;%\n  rename(\"Classificação\" = classePeso)\n\nPasso 6: Renomeie a coluna classePeso como “Classificação”\n\ntab_completa &lt;- tab_completa[c(\"Classificação\", \"f\", \"frp\", \"F\", \"Frp\")]\n\nprint(tab_completa)\n\n    Classificação   f  frp   F   Frp\n1      BP Extremo   2  1.2   2   1.2\n2        Muito BP   5  2.9   7   4.1\n3 Baixo Peso (BP)   0  0.0   7   4.1\n4     Peso Normal 163 95.9 170 100.0\n5    Excesso Peso   0  0.0 170 100.0\n\n\nAssim, pode-se dizer que a prevalência de baixo peso na maternidade do Hospital Geral de Caxias do Sul é bem mais alta que a prevalência no Brasil que é de 6,1% (IC95%: 4,5-8,3%) (Viana et al. 2013).",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tabelas</span>"
    ]
  },
  {
    "objectID": "07-tabelas.html#tabelas-prontas-para-publicação",
    "href": "07-tabelas.html#tabelas-prontas-para-publicação",
    "title": "7  Tabelas",
    "section": "7.3 Tabelas Prontas para Publicação",
    "text": "7.3 Tabelas Prontas para Publicação\nO R possui várias alternativas para produzir tabelas científicas, bonitas e elegantes que possam ser publicadas. Neste livro, serão mostradas algumas dessas alternativas.\n\n7.3.1 Pacote gt\nO pacote gt é um pacote que permite a apresentação de tabelas de maneira limpa e organizada (Iannone et al. 2020). funciona através do tidyverse com os comandos pipe %&gt;%, o que facilita sua aplicação.\n\n7.3.1.1 Passos para criar uma tabela com o pacote gt\nPasso 1: Criar uma tabela base (dataframe ou tibble), como a tabela recém criada (tab_completa) com os dados dos recém-nascidos. Verificar se os dados são realmente um dataframe ou tibble:\n\nclass(tab_completa)\n\n[1] \"data.frame\"\n\n\nA seguir, transformar o dataframe em uma tabela gt com a função gt():\n\nlibrary(gt)\ntab_gt &lt;- gt(data = tab_completa)\ntab_gt\n\n\n\nTabela 7.3: Tabela simples com o pacote gt\n\n\n\n\n\n\n\n\n\nClassificação\nf\nfrp\nF\nFrp\n\n\n\n\nBP Extremo\n2\n1.2\n2\n1.2\n\n\nMuito BP\n5\n2.9\n7\n4.1\n\n\nBaixo Peso (BP)\n0\n0.0\n7\n4.1\n\n\nPeso Normal\n163\n95.9\n170\n100.0\n\n\nExcesso Peso\n0\n0.0\n170\n100.0\n\n\n\n\n\n\n\n\n\n\nA Tabela 7.3 mostra duas partes, os rótulos das colunas e o corpo da tabela. Entretanto, o pacote gt permite de maneira fácil modificar ou acrescentar outras partes.\nAs partes que constituem a tabela gt são nomeadas de forma semelhante ao mostrado na Figura 7.1: cabeçalho (table header), rótulo das linhas (stub), corpo (body) e rodapé (footer). Reconhecer essas partes é importante para a adição outras características da tabela.\nPasso 2: Personalizar o estilo (Tabela 7.4)\nA família de funções tab_*() permite adicionar outras partes. O título e subtítulo podem ser adicionados com a função tab_header(). Alterações de estilo são feitas pela função tab_style(). Essa função define a formatação a ser aplicada, que pode ser cell_text() para formatar texto e locations = que define quais células ou áreas da tabela devem receber o estilo. As opções incluem:\n• cells_column_labels(): Para os cabeçalhos das colunas.\n• cells_title(): Para o título da tabela.\n• cells_stub(): Para o cabeçalho da linha.\n• cells_body(): Para o corpo da tabela.\n• E outras funções específicas, como cells_row_groups().\nPara maiores detalhes consulte Create beautiful tables with gt ou Introduction to Creating gt Tables.\n\ntab_gt &lt;- tab_gt %&gt;%\n  tab_header(\n    title = \"Classificação dos Pesos ao Nascer, de acordo com a OMS\",\n    subtitle = \"Hospital Geral de Caxias do Sul, 2008\") %&gt;%\n  tab_style(\n    style = cell_text(\n      weight = \"bold\",\n      size = \"medium\",\n      font = \"Arial\",\n      color = \"gray18\"),\n    locations = cells_title(groups = \"title\")) %&gt;%\n  tab_style(\n    style = cell_text(\n      weight = \"bold\"),\n    locations = cells_column_labels()) %&gt;% \n  tab_style(\n    style = cell_text(color = \"red\"),\n    locations = cells_body(\n      columns = Frp,\n      rows = near(Frp, 15))) %&gt;%\n    cols_width(\n      1 ~ px(150),\n      2:5 ~ px(80)\n    ) %&gt;% \n  tab_source_note(source_note = md(\"Fonte: Oliveira Filho, PF (2025)\")) %&gt;% \n  tab_footnote(\n    footnote = \"BP = Baixo Peso\",\n    locations = cells_body(columns = Classificação, rows = c(1,2))\n  )\n\ntab_gt\n\n\n\nTabela 7.4: Tabela com o pacote gt personalizada\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Classificação dos Pesos ao Nascer, de acordo com a OMS\n    \n    \n      Hospital Geral de Caxias do Sul, 2008\n    \n    \n      Classificação\n      f\n      frp\n      F\n      Frp\n    \n  \n  \n    BP Extremo1\n2\n1.2\n2\n1.2\n    Muito BP1\n5\n2.9\n7\n4.1\n    Baixo Peso (BP)\n0\n0.0\n7\n4.1\n    Peso Normal\n163\n95.9\n170\n100.0\n    Excesso Peso\n0\n0.0\n170\n100.0\n  \n  \n    \n      Fonte: Oliveira Filho, PF (2025)\n    \n  \n  \n    \n      1 BP = Baixo Peso\n    \n  \n\n\n\n\n\n\n\nPasso 3: Explicação do código\n\nEstrutura geral: inicialmente foi construída uma tabela gt com os dados da tab_completa. A partir daí foi encadeado, através do operador pipe, várias funções.\nCabeçalho da tabela: a função tab_header() adiciona um título e subtítulo à tabela.\n\nAplicado estílo ao título principal com a função tab_style() - negrito (bold), tamanho médio, fonte arial e cor cinza escura (gray18)\n\nEstilo dos rótulos das colunas: deixa os nomes das colunas em negrito, dando maior destaque.\nDestaque condicional no corpo da tabela: aplica cor vermelha à células da coluna Frp (frequência relativa acumulada em porcentagem) somente onde o valor está próximo de 15. Isto indica que 15% dos recém-nascidos têm peso ao nascer igual ou abaixo deste valor. A função near() é útil para destacar valores com tolerância numérica.\nLargura das colunas: define a largura das colunas, onde a primeira coluna tem 150 px e as demais (2 a 5) 80 px (pixels). Podem ser usadas duas unidades de medidas: pixels ou pct (percentual de largura da tabela). A conversão de pixels (px) para centímetros (cm) depende da resolução da tela medida em pixels por polegada (PPI). O padrão mais comum é 96 PPI.\n\\[\ncm = (px \\ \\times 2.54)/PPI\n\\]\nCom base em 96 PPI, \\(150 px \\approx 4\\) cm.\nPor úktimo se incluiu a fonte e uma nota explicativa.\n\n\n\n\n7.3.2 Pacote flextable\nO pacote flextable fornece uma estrutura para criar facilmente tabelas elegantes e personalizadas para relatórios e publicações (Gohel e Skintzos 2025).\nAs finalidades básicas do pacote flextable são:\n\nConstrói tabelas a partir de dataframes ou tibbles.\nPermite formatar títulos, subtítulos, rodapés, cabeçalhos e corpo da tabela.\nDá suporte a estilos avançados:\n\ncores de células e bordas;\nalinhamento (horizontal e vertical);\nnegrito/itálico;\nlargura das colunas e altura das linhas;\nmesclagem de células;\nnumeração e percentuais formatados.\n\nGera tabelas que podem ser dinâmicas (em R Markdown, Quarto).\n\n\n7.3.2.1 Passos para criar uma tabela com o pacote flextable\nPasso 1: Da mesma forma como nas tabelas criadas pelo pacote gt, o flextable também necessita de tabela base (dataframe ou tibble) (Navarro 2024). Será usada a mesma tabela (tab_completa). Uma tabela simples (Tabela 7.5) pode ser facilmente gerada:\n\nlibrary(flextable)\nlibrary(dplyr)\n\nft &lt;- flextable(data = tab_completa)\nft\n\n\n\nTabela 7.5: Tabela simples com o pacote flextable\n\n\n\nClassificaçãoffrpFFrpBP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0\n\n\n\n\n\nPasso 2: As colunas podem ter sua largura ajustadas de forma manual, passando para o argumento width() um vetor com as larguras em polegadas.\n\nft &lt;- ft %&gt;% width(width = c(2, 1.5, 1.5, 1.5, 1.5))\nft\n\n\n\nTabela 7.6: Ajuste das colunas\n\n\n\nClassificaçãoffrpFFrpBP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0\n\n\n\n\n\nO controle manual pode ser útil e fácil. A Tabela 7.6 melhorou o aspecto da Tabela 7.5, mas as colunas ficaram muito largas. O processo de tentativa e erro para encontrar as larguras ideais torna-se tedioso, irritante. Felizmente, o flextable permite que se ignore esse procedimento com a função autofit(), que tenta selecionar larguras de coluna adequadas automaticamente.\n\nft &lt;- ft %&gt;% autofit()\nft\n\n\n\nTabela 7.7: Ajuste automático das colunas\n\n\n\nClassificaçãoffrpFFrpBP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0\n\n\n\n\n\nAgora, a Tabela 7.7 já apresenta um layout bem mais bonito e , dependendo, do contexto, quase pronta para publicação.\nPasso 3: Ajuste do cabeçalho e rótulos das colunas. Dependendo da necessidade os rótulos do cabeçalho podem ser facilmente modificados com a função set_header_labels ():\n\nft &lt;- ft %&gt;% \n  autofit() %&gt;% \n  set_header_labels(\n    frp = \"fr (%)\",\n    Frp = \"Fr (%)\"\n  )\nft\n\n\n\nTabela 7.8: Ajuste do cabeçalho\n\n\n\nClassificaçãoffr (%)FFr (%)BP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0\n\n\n\n\n\nA Tabela 7.8 mudou pouca coisa, está mais ajustada.\nPasso 4: O flextable possui vários temas (theme) que possibilitam customizar o estilo da tabela. Esses temas oferecem diferentes aparências para as tabelas. É possível combinar esses temas com outros comandos de formatação, para criar um visual personalizado!\n\nft &lt;- ft %&gt;% \n  autofit() %&gt;% \n  set_header_labels(\n    frp = \"fr (%)\",\n    Frp = \"Fr (%)\"\n  ) %&gt;% \n  theme_vanilla()\nft\n\n\n\nTabela 7.9: Modificando o tema da tabela flextable\n\n\n\nClassificaçãoffr (%)FFr (%)BP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0\n\n\n\n\n\nA Tabela 7.9 já está ótima, mas utros temas podem ser usados como theme_booktabs() (padrão), theme_vader(), theme_apa(), theme_zebra(), theme_box(), theme_tron_legacy ().\n\n\n\n\n\n\nExercício\n\n\n\nTeste a aparência da tabela com diferentes temas.\n\n\nPasso 5: Uma nota de rodapé pode ser adicionada, usando-se uma função específica, add_footer_lines() junto com as funções que determinam o tamanho da fonte:\n\nft1 &lt;- ft %&gt;% \n  autofit() %&gt;% \n  set_header_labels(\n    frp = \"fr (%)\",\n    Frp = \"Fr (%)\"\n  ) %&gt;%  \n  theme_vanilla() %&gt;% \n  add_footer_lines(value = \"FONTE: Hospital Geral, Caxias do Sul, RS, 2008\") %&gt;% \n  fontsize(size = 9, part = \"footer\") \n\nft1\n\n\n\nTabela 7.10: Tabela com nota de rodapé\n\n\n\nClassificaçãoffr (%)FFr (%)BP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0FONTE: Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\nA Tabela 7.10 é igual a Tabela 7.9 adicionada de um rodapé.\nPasso 6: Muitas vezes, é útil definir regras de formatação que se aplicam apenas a um subconjunto da tabela. Por exemplo, algumas linhas ou colunas devam aparecer em uma cor diferente por um motivo ou outro. Todas as tabelas flexíveis são compostas por três partes: cabeçalho (header) na parte superior, uma grade de células no corpo (body) da tabela e um conjunto de linhas de rodapé (footer) na parte inferior. Muitas funções na tabela flexível têm um argumento que se pode usar para selecionar uma (ou todas) essas três partes. Por exemplo, a função bg() é usada para definir a cor de fundo. Pode-se adicionar um conteúdo extra ao cabeçalho, add_header_lines() e colorir o fundo:\n\nft2 &lt;- ft %&gt;% \n  autofit() %&gt;% \n  set_header_labels(\n    frp = \"fr (%)\",\n    Frp = \"Fr (%)\"\n  ) %&gt;%  \n  theme_booktabs() %&gt;% \n  add_footer_lines(value = \"FONTE: Hospital Geral, Caxias do Sul, RS, 2008\") %&gt;% \n  fontsize(size = 9, part = \"footer\") %&gt;% \n  add_header_lines(\"TABELA 1: Pesos dos RN de acordo com a OMS\") %&gt;% \n  bg(bg = \"gray30\", part =\"header\") %&gt;% \n  color(part = \"header\", color = \"ghostwhite\")\n\nft2\n\n\n\nTabela 7.11: Tabela com nota de rodapé\n\n\n\nTABELA 1: Pesos dos RN de acordo com a OMSClassificaçãoffr (%)FFr (%)BP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0FONTE: Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\nA Tabela 7.11 mostra um título com fundo escuro e letras brancas.\nPasso 7: Se for especificado uma seleção de linha e uma seleção de coluna, a regra de formatação será aplicada às células que satisfizerem ambos os critérios. Por exemplo, será selecionada a coluna 5 (frequência relativa acumulada ) e as linhas 1 a 3 coloridas em um gradiente de azul para representar todos os recém-nascidos abaixo de 2500 g (baixo peso).\nAntes cria-se uma função geradora de cores numéricas, criada com a função col_numeric() do pacote scales que cria um mapeamenteo de cores para valores numéricos. A paleta define um gradiente de cores que vai do transparente para o azul. O intervalo de valores é definido pelo argumento domain = c(0,50). Isto significa que qualquer número é convertido em uma cor proporcional dentro do gradiente. Os valores mais baixos serão transparentes (próximo a 0) e os valores mais altos (próximos a 50) serão azuis (Tabela 7.12).\n\nlibrary(scales)\ncolourer &lt;- scales::col_numeric(\n  palette = c(\"transparent\", \"deepskyblue4\"),\n  domain = c(0, 50))\n\nft3&lt;- ft %&gt;% \n  autofit() %&gt;% \n  set_header_labels(\n    frp = \"fr (%)\",\n    Frp = \"Fr (%)\"\n  ) %&gt;%  \n  theme_booktabs() %&gt;% \n  add_footer_lines(value = \"FONTE: Hospital Geral, Caxias do Sul, RS, 2008\") %&gt;% \n  fontsize(size = 9, part = \"footer\") %&gt;% \n  add_header_lines(\"TABELA 1: Pesos dos RN de acordo com a OMS\") %&gt;% \n  bg(i = 1:3, j = 5, bg =colourer)\n\nft3\n\n\n\nTabela 7.12: Tabela com seleção esécial de células\n\n\n\nTABELA 1: Pesos dos RN de acordo com a OMSClassificaçãoffr (%)FFr (%)BP Extremo21.221.2Muito BP52.974.1Baixo Peso (BP)00.074.1Peso Normal16395.9170100.0Excesso Peso00.0170100.0FONTE: Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\nMuitas outras funções existem para alterar o layout, dependendo do que o pesquisador deseja mostrar. Para isso, o flextable tem funções para altera as bordas, as fontes, formato, cores, alinhamento, etc. que podem ser pesquisadas na documentação do pacote.\n\n\n\n7.3.3 Pacote gtsummary\nO pacote gtsummary foi criado (Sjoberg et al. 2021) como complemento do pacote gt. Para usar o pacote , ele deve estar instalado e carregado:\n\nlibrary(gtsummary)\n\nO pacote gtsummary possui uma função tbl_summary() que permite sumarizar um dataframe.\nEla calcula e apresenta automaticamente estatísticas resumidas para variáveis contínuas, categóricas e dicotômicas dentro de uma estrutura de dados, formatadas para tabelas prontas para publicação. Detecta automaticamente os tipos de variáveis e aplica estatísticas resumidas padrão apropriadas (por exemplo, mediana e intervalo interquarti, média e desvio padrão para variáveis contínuas, contagens e porcentagens para variáveis categóricas).\nPara maiores detalhes consulte a vinheta da função.\n#### Dados para o exemplo\nComo exemplo, será usado um dataframe originário do conjunto de dados dadosMater.xlsx (veja Seção 7.2.3.1), contento uma amostra de 200 observações com as seguintes variáveis relacionadas aos recém-nascidos:\n\nlibrary(dplyr)\nlibrary(readxl)\n\nset.seed(1234)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  mutate(eCivil = factor(eCivil, \n                         levels = c(1,2), \n                         labels = c(\"Sem companheiro\", \"Com companheiro\")),\n         tipoParto = factor(tipoParto, \n                            levels = c(1,2), \n                            labels = c(\"Normal\", \"Cesareo\")),\n         fumo = factor(fumo, \n                       levels = c(1,2), \n                       labels = c(\"Fumante\", \"Não fumante\")),\n         obito = factor(obito, \n                        levels = c(1,2), \n                        labels = c(\"Sim\", \"Não\")),\n         categIdade = case_when(\n           idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n           idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n           idadeMae &gt; 35 ~ \"&gt; 35 anos\"),\n         categIdade = factor(categIdade, \n                             levels = c(\"&lt; 20 anos\", \"20 a 35 anos\", \"&gt; 35 anos\")),\n         sexo = factor(sexo, \n                       levels = c(1,2), \n                       labels = c(\"Masculino\", \"Feminino\")),\n         categIg = case_when(\n           ig &lt; 37 ~ \"RN Pré-termo\",\n           ig &gt;= 37 & ig &lt; 42 ~ \"RN a Termo\",\n           ig &gt;= 42 ~ \"RN Pós-termo\"),\n         categIg = factor(categIg, \n                          levels = c(\"RN Pré-termo\", \"RN a Termo\", \"RN Pós-termo\"))) %&gt;%  \n  dplyr::select(categIdade, eCivil, anosEst, renda, fumo, tipoParto, sexo,\n                pesoRN, compRN, apgar1, obito) %&gt;% \n  slice_sample(n=200)\n\n\n7.3.3.1 Construção da tabela\n\ntbl_summary(dados, \n            by = sexo,\n            missing = \"ifany\",\n            type = list(pesoRN ~ \"continuous2\",\n                        compRN ~ \"continuous2\"),\n            list(categIdade ~ \"Faixa Etária\",\n                 eCivil ~ \"Estado Civil\",\n                 anosEst ~\"Anos de estudo completos\",\n                 renda ~ \"Renda Familiar (SM)\",\n                 fumo ~ \"Tabagismo\",\n                 tipoParto ~\"Tipo de Parto\",\n                 pesoRN ~ \"Peso RN (g)\",\n                 compRN ~ \"Comp RN (cm)\",\n                 apgar1 ~ \"Apgar primeiro min\",\n                 obito ~\"Óbito\")) %&gt;% \n  add_n() %&gt;% \n  add_p(test = all_continuous() ~ \"t.test\",\n        pvalue_fun = ~style_pvalue(., digits = 2)) %&gt;% \n  modify_header(label = \"**Variáveis**\") %&gt;% \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariáveis\nN\nMasculino\nN = 1081\nFeminino\nN = 921\np-value2\n\n\n\n\nFaixa Etária\n200\n\n\n\n\n0.39\n\n\n    &lt; 20 anos\n\n\n16 (15%)\n10 (11%)\n\n\n\n\n    20 a 35 anos\n\n\n78 (72%)\n74 (80%)\n\n\n\n\n    &gt; 35 anos\n\n\n14 (13%)\n8 (8.7%)\n\n\n\n\nEstado Civil\n200\n\n\n\n\n0.82\n\n\n    Sem companheiro\n\n\n29 (27%)\n26 (28%)\n\n\n\n\n    Com companheiro\n\n\n79 (73%)\n66 (72%)\n\n\n\n\nAnos de estudo completos\n200\n8.00 (5.00, 10.00)\n7.50 (5.00, 11.00)\n0.55\n\n\nRenda Familiar (SM)\n200\n1.93 (1.45, 2.89)\n1.92 (1.45, 2.53)\n0.86\n\n\nTabagismo\n200\n\n\n\n\n0.046\n\n\n    Fumante\n\n\n15 (14%)\n23 (25%)\n\n\n\n\n    Não fumante\n\n\n93 (86%)\n69 (75%)\n\n\n\n\nTipo de Parto\n200\n\n\n\n\n0.17\n\n\n    Normal\n\n\n54 (50%)\n55 (60%)\n\n\n\n\n    Cesareo\n\n\n54 (50%)\n37 (40%)\n\n\n\n\nPeso RN (g)\n200\n\n\n\n\n0.21\n\n\n    Median (Q1, Q3)\n\n\n3,080 (2,703, 3,428)\n2,873 (2,525, 3,325)\n\n\n\n\nComp RN (cm)\n200\n\n\n\n\n0.34\n\n\n    Median (Q1, Q3)\n\n\n48.0 (45.3, 49.0)\n47.0 (44.8, 48.5)\n\n\n\n\nApgar primeiro min\n173\n8.00 (8.00, 9.00)\n9.00 (8.00, 9.00)\n0.077\n\n\n    Unknown\n\n\n15\n12\n\n\n\n\nÓbito\n200\n\n\n\n\n&gt;0.99\n\n\n    Sim\n\n\n1 (0.9%)\n0 (0%)\n\n\n\n\n    Não\n\n\n107 (99%)\n92 (100%)\n\n\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n2 Pearson’s Chi-squared test; Welch Two Sample t-test; Fisher’s exact test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArango, Hector Gustavo. 2009. «Organização dos dados em tabelas». Em Bioestatística: teórica e computacional, 3ª edição, 32–57. Guanabara Koogan.\n\n\nDaniel, Wayne W., e Chad L. Cross. 2013. «Grouped data: The frequency distribuition». Em Biostatistics: A Foundation for Analysis in the Health Sciences, Tenth Edition, 22--23. Wiley.\n\n\nGeeksforGeeks. 2025. «Case when statement in R Dplyr package using case_when() function». GeeksforGeeks. GeeksforGeeks. https://www.geeksforgeeks.org/r-language/case-when-statement-in-r-dplyr-package-using-case_when-function/.\n\n\nGohel, David, e Panagiotis Skintzos. 2025. flextable: Functions for Tabular Reporting. doi:10.32614/CRAN.package.flextable.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, et al. 2020. «Gt: Easily create presentation-ready display tables». The R Foundation. https://CRAN.R-project.org/package=gt.\n\n\nNavarro, Danielle. 2024. «Use of flextable». Notes from a data witch. https://blog.djnavarro.net/posts/2024-07-04_flextable/.\n\n\nSchirmer, Janine, e Outros. 2000. «Fatores de Risco reprodutivo». Em Assistência Pré-Natal: Manual Técnico, 3a Edição, 25–26. Ministério da Saúde. https://bvsms.saude.gov.br/bvs/publicacoes/cd04_11.pdf.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, e Joseph Larmarange. 2021. «Reproducible Summary Tables with the gtsummary Package». The R Journal 13: 570–80. doi:10.32614/RJ-2021-053.\n\n\nViana, Kelly de Jesus, José Augusto de Aguiar Carrazedo Taddei, Monize Cocetti, e Sarah Warkentin. 2013. «Birth weight in Brazilian children under two years of age». Cadernos de Saúde Pública 29. SciELO Brasil: 349–56.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tabelas</span>"
    ]
  },
  {
    "objectID": "07-tabelas.html#footnotes",
    "href": "07-tabelas.html#footnotes",
    "title": "7  Tabelas",
    "section": "",
    "text": "Determina que o número de classes (k) pode ser calculado pela fórmula: \\(k = 1 + 3.322 * log10(n)\\), onde n é igual ao número de observações.↩︎\nQuando já existe um critério, como mostrado acima, ou o pesquisador tem um determinado interesse, as classes podem ter amplitudes diferentes.↩︎\nA função diff() no R calcula as diferenças entre elementos consecutivos de um vetor, matriz ou série temporal. Ela subtrai cada elemento do elemento seguinte, retornando um valor absoluto.↩︎\nPersonagens criados pelos franceses Albert Uderzo e Rene Goscinny.↩︎\nSe houver interesse de remover a notação científica e deixar os gauleses felizes.↩︎",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tabelas</span>"
    ]
  },
  {
    "objectID": "08-graficos.html",
    "href": "08-graficos.html",
    "title": "8  Gráficos",
    "section": "",
    "text": "8.1 Pacotes necessários neste capítulo\nPara descrever os dados e visualizar o que está acontecendo, recomenda-se utilizar um gráfico adequado. O que é adequado depende principalmente do tipo de dados, bem como das características particulares do que se quer explorar. Além disso, um gráfico em um relatório sempre é um fator de “impacto”. Ou seja, pode ter um efeito positivo no leitor ou fazê-lo abandonar a leitura. Finalmente, um gráfico de frequência pode ser utilizado para ilustrar, explicar uma situação complexa onde palavras ou uma tabela podem ser confusos, extensos ou de outro modo insuficiente. Por outro lado, deve-se evitar usar gráficos onde poucas palavras expressam claramente o que se quer mostrar. Aconselha-se que, ao analisar os dados, é importante inspecioná-los como se fossem uma imagem, uma fotografia, ver como eles se parecem, qual o seu aspecto, e só então pensar em interpretar os aspectos vitais da estatística (Field, Miles, e Field 2012).\nO R básico fornece uma grande variedade de funções para visualizar dados, elas de uma maneira relativamente simples permitem a construção de gráficos que facilitam a interpretação tanto de variáveis categórica como numéricas. Existe uma farta bibliografia para a construção de gráficos, utilizando o R básico, mas a The R Graph Gallery responde a maioria das dúvidas, apesar de ter seu foco em tidyverse (veja Seção 5.4) e ggplot2.\nNeste livro, a ênfase será no pacote ggplot2 (Wickham 2016). Este pacote é uma ferramenta extremamente versátil que oferece uma estrutura com grande flexibilidade para exibir os dados através de gráficos. O gpglot2 não é apenas uma instrumento para criar gráficos, mas uma maneira de pensar sobre a visualização de dados de uma forma mais estruturada e poderosa.\nCertifique-se que estes pacotes estejam instalados e carregados, utilizando o pacote pacman (consulte a Seção 4.2.2.4):\npacman::p_load(tidyverse, readxl, scales, ggsci,  paletteer, knitr, RColorBrewer, scico)",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-dados8",
    "href": "08-graficos.html#sec-dados8",
    "title": "8  Gráficos",
    "section": "8.2 Fonte de dados para este capítulo",
    "text": "8.2 Fonte de dados para este capítulo\nOs dados serão provenientes do conjunto de dados apresentado na Seção 5.6, denominado de dadosMater.xlsx e que pode ser encontrado para baixar aqui.Serão selecionadas as variáveis de interesse e adicionadas variáveis que categorizam a idade da mãe e a intensidade do tabagismo durante a gestação1. Além disso, serão feitas transformações para fatores das variáveis numéricas que na realidade são fatores. O código inicia com a semente set.seed() para garantir a repetibilidade.\n\nset.seed(123)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;%\n  select(idadeMae,fumo, quantFumo, ig, pesoRN, compRN, para, sexo) %&gt;% \n  mutate(fumo = factor(fumo, \n                       levels = c(1,2), \n                       labels = c(\"Fumante\", \"Não fumante\")),\n         sexo = factor(sexo, \n                       levels = c(1,2), \n                       labels = c(\"Masculino\", \"Feminino\")),\n         categIdade = case_when(\n           idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n           idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n           idadeMae &gt; 35 ~ \"&gt; 35 anos\"),\n         categIdade = factor(categIdade, \n                             levels = c(\"&lt; 20 anos\", \n                                        \"20 a 35 anos\",\n                                        \"&gt; 35 anos\")),\n         categFumo = case_when(\n           quantFumo == 0 ~ \"nao_fumante\",\n           quantFumo &lt;= 10 ~\"fumante_leve\",\n           quantFumo &gt; 10  & quantFumo &lt; 20 ~ \"fumante_moderada\",\n           quantFumo &gt;= 20 ~ \"fumante_pesada\"),\n         categFumo = factor(categFumo, \n                            levels = c(\"nao_fumante\", \n                                       \"fumante_leve\",\n                                       \"fumante_moderada\",\n                                       \"fumante_pesada\"))) \n\nstr(dados)\n\ntibble [1,368 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ fumo      : Factor w/ 2 levels \"Fumante\",\"Não fumante\": 2 2 2 2 2 1 1 2 2 2 ...\n $ quantFumo : num [1:1368] 0 0 0 0 0 10 20 0 0 0 ...\n $ ig        : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ pesoRN    : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN    : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ para      : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ sexo      : Factor w/ 2 levels \"Masculino\",\"Feminino\": 2 2 2 2 2 2 2 2 2 2 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 3 2 1 2 2 2 2 2 1 2 ...\n $ categFumo : Factor w/ 4 levels \"nao_fumante\",..: 1 1 1 1 1 2 4 1 1 1 ...",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-ggplot2",
    "href": "08-graficos.html#sec-ggplot2",
    "title": "8  Gráficos",
    "section": "8.3 Pacote ggplot2",
    "text": "8.3 Pacote ggplot2\nO ggplot2 é um pacote da linguagem R voltado para a visualização de dados, oferecendo uma abordagem poderosa e elegante baseada na Gramática dos Gráficos (Wickham 2010).\n\n8.3.1 Gramática dos Gráficos\nO R base usa funções específicas para a construção de um gráfico, por exemplo, hist() para criar um histograma ou a função plot() que é uma função mais genérica que produz um gráfico de dispersão, no R, quando são passados a ela dois vetores numéricos ou boxplots quando são fornecidos dados categóricos.\nTomando as variáveis compRN e pesoRN de uma amostra de de 100 observações do conjunto dados, com filtro para as gestações a termo 2, será construído um gráfico de dispersão (Figura 8.1) com a funçãoplot() do R base:\n\nplot (x =jitter(dadosRNT100$compRN,10),\n      y = dadosRNT100$pesoRN,\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      cex.axis = 0.8,\n      las = 1,\n      bty = \"L\",\n      pch = 19,\n      cex = 1.5,\n      col = \"cyan4\")\n\n\n\n\n\n\n\nFigura 8.1: Gráfico de dispersão produzido com uma função específica plot(), do R base\n\n\n\n\n\nEste gráfico não difere muito do gráfico da Figura 8.7 no seu aspecto final, produzido no ggplot2. Entretanto, a filosofia de construção dos gráficos é muito diferente.\nEm vez de se pensar em funções específicas para cada tipo de gráfico, o ggplot2 permite que se construa gráficos combinando diferentes componentes. Pode-se pensar nisso como se fosse a montagem de um quebra-cabeça, onde cada peça representa uma parte do gráfico. Essa abordagem modular e intuitiva é o que torna o ggplot2 tão flexível e poderoso.\nOs principais componentes combinados para criar um gráfico no ggplot2 são:\n\nDados (Data): cada camada deve ter dados associados. É o conjunto de dados que se quer visualizar. Geralmente, ele deve estar em um formato tidy (arrumado), onde cada coluna é uma variável e cada linha é uma observação (Seção 5.3).\nMapeamentos Estéticos: Os mapeamentos estéticos são definidos com a função aes(). A parte onde se associa as variáveis do conjunto de dados a propriedades visuais do gráfico, como os eixos x e y, a cor, o tamanho, a forma e a transparência. Por exemplo, é possível mapear a variável peso do recém-nascido para o eixo y e a variável comprimento do recém-nascido para o eixo x. Os mapeamentos estéticos podem ser fornecidos na ggplot() , chamada inicial, em camadas individuais ou em uma combinação de ambos. Todas essas chamadas criam a mesma especificação de plotagem.\nEscalas (Scales) podem ser usadas para controlar a forma dos mapeamentos estéticos. Pode-se usar escalas para ajustar cores, tamanhos e a aparência dos eixos.\nGeometrias (Geoms): Formas geométricas que representam os dados. É aqui que se define o tipo de gráfico a ser criado. Alguns exemplos são:\n\ngeom_point() para um gráfico de dispersão (pontos).\ngeom_line() para um gráfico de linha.\ngeom_bar() para um gráfico de barras.\ngeom_errorbar(): barras de erro.\ngeom_bar(stat = \"identity\"): um gráfico de barras de resumos pré-calculados\ngeom_histogram() para um histograma.\ngeom_boxplot() para um boxplot.\n\nPode-se adicionar múltiplas camadas que permitem combinar diferentes tipos de geoms em uma única visualização. Por exemplo, é possível colocar uma linha de tendência (geom_smooth()) em cima de um gráfico de dispersão (geom_point()), veja a Figura 8.16.\nEstatísticas: O ggplot2 não permite a colocação direta de estatísticas dentro dos geoms. A forma mais comum de adicionar estatísticas em um gráfico é usando a função stat_. As funções stat_ calculam as estatísticas (como média, mediana, contagem) e, em seguida, as representam no gráfico, criando uma camada de dados calculados.\n\nstat_summary: adiciona um resumo estatístico a cada grupo. Usada para adicionar a média, mediana ou desvio padrão em um geom já existente, como um gráfico de dispersão ou de barras.\nstat_smooth: Adiciona uma linha de tendência (como regressão linear) com um intervalo de confiança.\nstat_bin: Calcula a contagem de cada bin e os plota como um histograma.\n\nA partir das últimas versões do ggplot2, é possível mapear variáveis calculadas pelo stat_ diretamente em uma estética, como y, size ou label. Isso dá uma grande flexibilidade.\nEm vez de usar as funções stat_, é possível calcular as estatísticas em um passo separado usando o dplyr e, em seguida, plotar esses dados pré-calculados usando ggplot2.\n\nCalcule as estatísticas usando group_by() e summarize() do dplyr.\nCrie o gráfico usando os novos dados calculados.\n\nAjustes de posição: aplicam pequenos ajustes na posição dos elementos dentro de uma camada. Por exemplos, há três ajustes que são úteis em gráficos de pontos\n\nposition_nudge(): mover pontos por um deslocamento fixo.\nposition_jitter(): adicione um pouco de ruído aleatório a cada posição (Figura 8.6).\nposition_jitterdodge(): desviar de pontos dentro de grupos e depois adicionar um pouco de ruído aleatório. Na construção do gráfico de dispersão será mostrado exemplo desses ajustes (Seção 8.4.4).\n\nPara o gráfico de barras, pode-se aplicar alguns ajustes:\n\n\nposition_stack(): empilhar barras (ou áreas) sobrepostas umas sobre as outras.\nposition_fill(): empilhe barras sobrepostas, dimensionando para que o topo esteja sempre em 1.\nposition_dodge(): coloque barras sobrepostas (ou boxplots) lado a lado.\n\nFacetas (Facets): Permitem criar múltiplos subgráficos baseados em uma ou mais variáveis categóricas. É uma ótima maneira de explorar as relações entre diferentes grupos de dados de forma visual (Figura 8.15).\nTemas (Themes): Controlam a aparência geral do gráfico, como a cor de fundo, a fonte, as linhas da grade e a aparência dos títulos. O ggplot2 permite construir gráficos complexos camada por camada, possibilitando a criação de gráficos sofisticados (Wickham, Navarro, e Pedersen 2023). Em cada uma das camadas deve-se ter preocupação em controlar os componentes do gráfico.\n\n\n\n8.3.2 Vantagens do ggplot2\n\nConsistência e Flexibilidade: A abordagem de camadas e a gramática de gráficos permitem criar uma variedade enorme de visualizações de forma consistente.\nQualidade Visual: Os gráficos produzidos pelo ggplot2 são esteticamente agradáveis e prontos para publicações.\nIntuitividade: Uma vez que se entenda o conceito, é fácil construir gráficos complexos de forma gradual.\nExtensibilidade: O pacote pode ser estendido com outros pacotes que adicionam novas geoms, temas ou funcionalidades, como o gganimate para animações ou o ggrepel para evitar sobreposição de rótulos.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-scatter",
    "href": "08-graficos.html#sec-scatter",
    "title": "8  Gráficos",
    "section": "8.4 Gráfico de dispersão",
    "text": "8.4 Gráfico de dispersão\nUm gráfico de dispersão (Scatterplot) exibe a relação entre duas variáveis numéricas. Cada ponto representa uma observação. Suas posições nos eixos x (horizontal) e y (vertical) representam os valores das duas variáveis.\nO gráfico de dispersão permite identificar padrões, tendências e a força de uma possível correlação entre essas variáveis. Frequentemente, vem acompanhado por um cálculo do coeficiente de correlação (Capítulo 17), que , em geral, mede a relação linear.\nPretende-se, na construção de um gráfico de dispersão introduzir a lógica do ggplot2. Os dados usados para o exemplo, serão os mesmos usados na construção do gráfico de dispersão com a função nativa plot()(Seção 8.3.1).\n\n8.4.1 Gráfico de dispersão básico\nEste é o exemplo mais simples que começa com o mapeamento de duas variáveis para os eixos x e y. A função central do pacote ggplot2 é ggplot(), que recebe os dados por meio do argumento data. Em seguida, a função estética aes() define os mapeamentos dos eixos x e y, iniciando o gráfico com uma camada base — ainda vazia, mesmo que os dados já tenham sido fornecidos. Essa camada base corresponde a um painel cinza (Figura 8.2), sobre o qual outras camadas serão adicionadas. Funciona como um terreno pronto para receber uma construção, que será erguida com o uso de uma função geométrica.\n\nggplot(data = dadosRNT100, aes(x = compRN, y = pesoRN))\n\n\n\n\n\n\n\nFigura 8.2: Camada base do ggplot\n\n\n\n\n\n\n\n8.4.2 Geometria\nA seguir, adiciona-se 3 a camada dos pontos que usa a geometria geom_point() para criar um gráfico de dispersão.\n\nggplot(data = dadosRNT100,\n       mapping = aes(x = compRN, y = pesoRN)) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 8.3: Gráfico de dispersão simples\n\n\n\n\n\nA Figura 8.3 mostra um gráfico de dispersão ainda sem um aspecto elegante, mas com as informações necessárias. Tem este fundo escuro que não é do agrado da maioria, além de não apresentar o rótulos das variáveis de forma mais clara, mais adequada.\nO mesmo resultado da Figura 8.3 pode ser obtido, colocando o mapeamento com a estética aes() dentro do geom_point():\n\nggplot(data = dadosRNT100) + \n  geom_point(aes(x = compRN, y = pesoRN))\n\n\n\n8.4.3 Customização do gráfico de dispersão\nA geometria geom_point() múltiplas opções de customização, através de seus argumentos:\n\ncolor: a cor do traço, o contorno do círculo\nstroke: a largura do traço no ponto\nfill: cor da parte interna do ponto\nshape: forma do marcador (Figura 8.4)\nalpha: transparência do ponto, varia de 0 a 1, 0 é totalmente transparente; 1 = opaco.\nsize: tamanho do ponto\n\n\n\n\n\n\n\n\n\nFigura 8.4: Formato dos pontos disponíveis no R\n\n\n\n\n\nPara definir um tamanho uniforme para todos os pontos do gráfico, basta especificar um valor numérico no argumento size da função geom_point(), como por exemplo size = 1.5 (padrão). Para melhor visualização, será escolhido size = 3 ou 4. O mesmo princípio se aplica à cor. No argumento color (vai colorir o contorno dos pontos), colocar, por exemplo, color = “gray20” ou, se o formato (shape), no argumento fill para a cor de preenchimento do ponto. A escolha das cores depende do gosto pessoal, na Seção 8.4.6, serão mostrados alguns princípios que auxiliam esse processo. Para alterar o formato dos pontos, usar o argumento shape, conforme as opções na figura Figura 8.4. Somente os formatos 21 a 25 permitem preenchimento. No exemplo, será usado shape = 21. Nesse caso, é possível adicionar o argumento para definir a cor interna do ponto, ou seja, uma cor fixa (fill = “tomato”) ou uma variável categórica, como sexo. Ao utilizar uma variável como fill = sexo, o ggplot2 preencherá os pontos com cores diferentes automaticamente, de acordo com os níveis dessa variável.\n\nggplot(data = dadosRNT100,\n       mapping = aes(x = compRN, y = pesoRN)) +\n  geom_point(color = \"gray20\",\n             fill =\"tomato\",\n             alpha = 1,\n             shape = 21, \n             size = 3,\n             stroke =1)\n\n\n\n\n\n\n\nFigura 8.5: Gráfico de dispersão colorido\n\n\n\n\n\n\n\n8.4.4 Lidando com a sobreposição dos pontos e os rótulos\nNa Figura 8.5 a modificação realizada melhorou o aspecto do gráfico. Entretanto, os pontos estão se sobrepondo, porque o comprimento dos recém-nascidos está registrado como uma variável numérica discreta e existem vários com o mesmo comprimento. Nesse caso, a solução para evitar a sobreposição, é provocar um pequeno deslocamento aleatório dos pontos, tornando o gráfico mais legível. Isto é feito, embutindo o jitter (espalhamento) com um argumento dentro do geom_point(), o position = position_jitter (width = 0.2, height = 0). O argumento width controla o deslocamento horizontal (eixo x); height controla o deslocamento vertical (eixo y). No exemplo (Figura 8.6), os pontos serão espalhados horizontalmente, mantendo a posição vertical. Será usado o argumento width = 0.2 que espalha os pontos de forma leve, sem alterar o eixo y.\nNesta modificação, será colocada mais uma camada para trabalhar com os rótulos dos eixos x e y., usando as funções ylab() e xlab().\n\nggplot(data = dadosRNT100, \n       mapping = aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             color = \"gray20\",\n             fill =\"tomato2\",\n             shape = 21,\n             alpha = 1,\n             size = 3,\n             stroke = 1) +\n  ylab(\"Peso do Recém-nascido (g)\") +\n  xlab(\"Comprimento do Recém-nascido (cm)\") \n\n\n\n\n\n\n\nFigura 8.6: Gráfico de dispersão com jitter\n\n\n\n\n\n\n\n8.4.5 Mudando o tema\nA Figura 8.6 já é um gráfico bem aceitável, praticamente sem defeitos, apesar de o autor implicar muito com o fundo cinza – theme_gray(). Essa cor acinzentada padrão do ggplot2 pode ser alterada pela definição de outro tema integrado, entre muitos, como theme_classic() que é um tema de aparência clássica, com linhas dos eixos x e y e sem linhas de grade, semelhante ao da Figura 8.1, criado com a função nativa plot(). Outro tema interessante é o theme_bw() que usa um fundo branco e linhas finas de grade cinza.\nPara ver outras possibilidades acesse Completes themes - ggplt2. Foi adicionado o argumento base_size = 13, para modificar o tamaho das letras.\nO gráfico da Figura 8.6 com a adição do theme_classic() e aumento dp tamanho das letras, pode ser observado na Figura 8.7.\n\nggplot(data = dadosRNT100,\n       mapping = aes(x = compRN, y = pesoRN)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             color = \"gray20\",\n             fill =\"steelblue\",\n             shape = 21, \n             alpha = 1,\n             size = 3,\n             stroke =1) +\n  ylab(\"Peso do Recém-nascido (g)\") +\n  xlab(\"Comprimento do Recém-nascido (cm)\")+\n  theme_classic(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.7: Gráfico de dispersão com jitter e theme_bw()\n\n\n\n\n\n\n\n8.4.6 Mudando as cores\nNa Seção 8.4.3 foi introduzido o uso de cores no R. Agora, apesar deste tema praticamente não ter limites, serão mostrados alguns princípios do manuseio das cores no ggplot2. É possível visualizar as cores para usar no ggplot2 de uma maneira fácil, acessando, por exemplo, An overview of color names in R ou um guia completo Colors (ggplot2).\nA escolha das cores pode ser feita especificando o seu nome em inglês. Essa escolha é pessoal. O R possui 657 cores integradas que permitem uma gama ampla de opções. Pode-se chamar uma cor pelo nome e a função colors() exibe todos elas. Uma outra maneira de especificar as cores, é usar o sistema RGB ou hexadecimal. O código hexadecimal da cor branca é #FFFFFFF, da “gray58” é #949494, da “yellow4” é #999900, etc. Opcionalmente, a cor pode ser transparente, usando o formato “#RRGGBBAA”. Alpha refere-se à transparência de um geom_. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes. O argumento alpha também pode ser modificado por meio da estética de color ou fill se qualquer uma das estéticas fornecer valores de cor usando uma especificação RGB.\n\n8.4.6.1 Mapeando Cores com aes()\nA forma mais comum de usar cores é mapeando uma variável para a estética color ou fill dentro da função aes(). Quando se faz isso, o ggplot2 atribui cores automaticamente, criando uma legenda. Esta legenda pode ser modificada de posição com a função theme(legend.position = “bottom”), que colocará a legenda na parte inferior do gráfico4. No caso deste gráfico, o título da legenda pode ser removido, porque ele é óbvio. Para isso, basta usar a função labs() que manauseia os títulos, usando color ou fill, dependendo se for a cor do contorno ou do preenchimento do ponto.\n\ncolor: Usado para o contorno de formas, como a borda dos pontos ( ou como será visto adiante, as linhas de um gráfico de linha, ou a borda de um boxplot, de um gráfico de barra).\nfill: Usado para preencher formas, como pontos (shape 21 a 25) ou como as barras de um histograma ou as caixas de um boxplot (ver adiante).\n\nInicialmente, serão manipuladas as cores do gráfico da Figura 8.7, utilizando uma variável categórica, fill = sexo, dentro da função aes().\n\nggplot(data = dadosRNT100, \n       mapping = aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21,\n             alpha = 1,\n             size = 4,\n             stroke = 1) +\n  labs(title=\"\",\n       x = \"Comprimento do Recém-nascido (cm)\",\n       y = \"Peso do Recém-nascido (g)\",\n       fill = \"\") +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigura 8.8: Gráfico de dispersão com cores de acordo com o sexo\n\n\n\n\n\nNeste caso, a variável categórica sexo foi mapeada para a estética fill. O ggplot2 atribuiu uma cor diferente para cada sexo e criou uma legenda automaticamente (Figura 8.8). No exemplo da Figura 8.7, foi mostrado que quando a cor for única, o mapeamento da mesma se dá dentro do geom_point().\n\n\n\n\n\n\nAtenção!\n\n\n\nSe estiver usando shape que não aceita preenchimento (como 16 ou 19), então deve-se usar color = sexo e scale_color_manual().\n\n\n\n\n\n\n\n\nExercício\n\n\n\nO que acontece se a cor determinada por uma variável categórica (fill = sexo ou color= sexo) for colocada fora do aes()?\n\n\nResposta5\n\n\n8.4.6.2 Paletas de Cores\nPara ter controle total sobre as cores, deve-se usar funções de escala (scale). Existem diferentes funções de escala para variáveis categóricas e numéricas. Na Figura 8.8, o ggplot2 definiu as cores automaticamente. Agora, as cores serão personalizadas, usando a função scale_color_manual() (Figura 8.9).\n\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21, \n             color = \"gray20\", \n             size = 4, \n             stroke = 1) +\n  scale_fill_manual(values = c(Masculino = \"cyan\",\n                               Feminino = \"pink3\")) +\n  labs(title=\"\",\n       x = \"Comprimento do Recém-nascido (cm)\",\n       y = \"Peso do Recém-nascido (g)\",\n       fill = \"\") +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigura 8.9: Gráfico de dispersão com cores personalizadas de acordo com o sexo\n\n\n\n\n\nObservar que, na Figura 8.8, os pontos que representam os meninos estavam cor rosa e as meninas com cor azul. Na figura Figura 8.9, isto foi modificado manualmente. Para realizar o mesmo processo, é muito comum usar pacotes de paletas de cores.\n\n8.4.6.2.1 Pacote ggsci\nO ggsci é um pacote que oferece uma coleção de paletas de alta qualidade inspiradas em cores usadas em revistas científicas, bibliotecas de visualização de dados, filmes de ficção científica e programas de TV. As paletas de cores no ggsci estão disponíveis como escalas ggplot2, Para todas usa-se as seguintes funções:\n\nscale_color_nomedapaleta() e\nscale_fill_nomedapaleta ().\n\nPor exemplo, para a paleta do Lancet, usa-se para o preenchimento: scale_fill_lancet() . O pacote ggsci deve ser instalado e carregado para usar estas paletas.\nPara visualizar as opções do pacote ggsci acessar Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2.\nComo exemplo, será usado o código que gerou o gráfico da Figura 8.9 com alterações, usando a paleta do periódico Lancet.\n\nlibrary(ggsci)\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21, \n             color = \"gray20\", \n             size = 4, \n             stroke = 1) +\n  scale_fill_lancet(alpha = 0.6) +\n  labs(title=\"\",\n       x = \"Comprimento do Recém-nascido (cm)\",\n       y = \"Peso do Recém-nascido (g)\",\n       fill = \"\") +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigura 8.10: Gráfico de dispersão com cores personalizadas usando o ggsci\n\n\n\n\n\nAs cores usadas são agora as da paleta do Lancet (Figura 8.10). Como eles são muito vivas e para que aparecesse que o ponto foi preenchido, foi utilizada uma transparência de alpha = 0.6. A paleta do Lancet pode ser visualizada com a função show_col(pal_lancet())(Figura 8.11)\n\nlibrary(ggsci)\nshow_col(pal_lancet())\n\n\n\n\n\n\n\nFigura 8.11: Paleta do Lancet do pacote ggsci\n\n\n\n\n\n\n\n\n\n\n\nExercício\n\n\n\nVisualizar outras paletas do pacote ggsci, usando a função show_col() do pacote scales. Por exemplo:\nshow_col(pal_jama())\nshow_col(pal_bmj())\nshow_col(pal_aaas())\nshow_col(pal_simpsons())\n\n\n\n\n8.4.6.2.2 Pacote RColorBrewer\nExistem outros pacotes, como o RColorBrewer, que oferecem paletas visualmente agradáveis e podem facilmente ser exploradas. Por alguns, é considerado uma ferramenta indispensável para gerenciar cores com R (Holtz 2025). Para visualizar (Figura 8.12) as paletas do pacote RColorBrewer.\n\nlibrary(RColorBrewer)\n\npar(mar=c(2, 4, 2, 3))            # modifica o tamanho das margens\ndisplay.brewer.all()\npar(mar=c(5.1, 4.1, 4.1, 2.1))    # retorna ao tamanho original das margens\n\n\n\n\n\n\n\nFigura 8.12: Paleta do RColorBrewer\n\n\n\n\n\nComo exemplo, será repetido o gráfico da Figura 8.10 com uma paleta de cores do RColorBrewer, Pastel2.\n\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21, \n             color = \"gray20\", \n             size = 4, \n             stroke = 1) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title=\"\",\n       x = \"Comprimento do Recém-nascido (cm)\",\n       y = \"Peso do Recém-nascido (g)\",\n       fill = \"\") +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigura 8.13: Gráfico de dispersão com cores personalizadas usando o RColorBrewer\n\n\n\n\n\n\n\n8.4.6.2.3 Paleta paletteer\nO pacote paletteer no R reúne um grande número de paletas de cores de diversos pacotes do R dedicados a cores. Fornece uma interface simples e consistente para acessar essas paletas, facilitando o trabalho. Oferece mais de 2000 paletas de cores de vários pacotes do R, como ggthemes, wesanderson, lisa, scico, entre outros. Tudo acessível por uma interface simples e poderosa, facilitando a criação de visualizações bonitas e informativas (Hvitfeldt 2024).\nTodas as paletas podem ser acessadas a partir das três funções paletteer_c(), paletteer_d() e paletteer_dynamic() usando a sintaxe: nome_do_pacote::nome_da_paleta 6.\nPaletas discretas são paletas com um número fixo de cores. Elas são úteis para visualizar dados categóricos. Por exemplo, uma paleta que vai do vermelho ao laranja, do verde ao preto é uma paleta discreta.\nExemplo com a paleta nbapalettes::supersonics_holiday, mapeando os pontos em um tamanho maior (size = 7) para chamar atenção das cores.\n\nlibrary(paletteer)\n\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN, fill = sexo)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21, \n             color = \"gray20\", \n             size = 6, \n             stroke = 1.5) +\n  scale_fill_paletteer_d(\"nbapalettes::supersonics_holiday\") +\n  labs(title=\"\",\n       x = \"Comprimento do Recém-nascido (cm)\",\n       y = \"Peso do Recém-nascido (g)\",\n       fill = \"\") +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\nFigura 8.14: Gráfico de dispersão com cores personalizadas usando o paletteer\n\n\n\n\n\nPara uma visualização rápida de algumas paletas com o paleteteer, pode-se digitar em um script do RStudio ou do Positron o seguinte comando:\n\npaletteer::paletteer_d(\"lisa::FridaKahlo\")\n\n&lt;colors&gt;\n#121510FF #6D8325FF #D6CFB7FF #E5AD4FFF #BD5630FF \n\n\n\npaletteer::paletteer_d(\"nbapalettes::supersonics_holiday\")\n\n&lt;colors&gt;\n#D50032FF #F6BE00FF #00573FFF #010101FF \n\n\nNo console, aparecerão as cores com os códigos hexadecimais. Se as cores não estiverem visíveis como aqui, então acesse o website HTML Coloe Codes, onde facilente é feita essa conversão.\nIsto é apenas o caminho, existem uma enorme quantidade de paletas (mais de 2500 paletas!) e o paletteer é uma espécie de facilitador para se ter acesso a elas. Escolher as cores para um gráfico é uma tarefa desafiadora e demorada, o que geralmente leva à insatisfação.Em um dos seus sites educacionais Yan Holtz disponibiliza um localizador de paletas de cores que torna este trabalho mais palatável.\n\n\n\n\n8.4.7 Facetamento\nNa Seção 8.4.6, foi mostrado como comparar grupos, através da cor, usando as estéticas fill ou color 7. Outra técnica para diferenciar grupos em um gráfico é o facetamento. O facetamento cria gráficos dividindo os dados em subconjuntos e exibindo o mesmo gráfico para cada subconjunto (Figura 8.15) . Para facetar um gráfico, basta adicionar uma especificação de facetamento com a função facet_wrap(), que recebe o nome de uma variável categórica precedido pelo sinal gráfico til (~).\nComo exemplo prático, será aproveitado o código que gerou a Figura 8.14 com pequenas alterações 8 e aplicação do facet_wrap().\n\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21,\n             fill = \"tomato\",\n             color = \"gray20\", \n             size = 4, \n             stroke = 1) +\n  labs(x = \"Comprimento do Recém-nascido (cm)\", \n       y = \"Peso do Recém-nascido (g)\") +\n  theme_bw(base_size = 13) +\n  facet_wrap(~sexo)\n\n\n\n\n\n\n\nFigura 8.15: Gráfico de dispersão com facetamento por categoria\n\n\n\n\n\nO facetamento (Figura 8.15) permite verificar que a relação entre o comprimento e o peso dos recém-nascidos é nitidamente linear e semelhante entre os sexos.\n\n\n8.4.8 Reta de Regressão\nA função geom_smooth() é uma forma geométrica do ggplot2 usada para visualizar tendências ou padrões entre duas variáveis numéricas. Ele adiciona uma linha suavizada ao gráfico, que ajuda a entender a relação entre os dados , especialmente quando há muitos pontos ou quando a relação não é linear. O geom_smooth() ajusta uma curva aos dados, usando métodos estatísticos. A regressão linear usa method = “lm” 9. Por padrão, exibe o intervalo de confiança (veja Capítulo 12), que mostra a incerteza da estimativa da reta. Esta técnica ajuda a identificar padrões que não podem ser visíveis apenas com os pontos brutos.\nO código da figura Figura 8.10 será tomado como base sem a divisão por sexo 10, com modificações, para gerar a reta de regressão.\n\nlibrary(ggsci)\nggplot(dadosRNT100, \n       aes(x = compRN, y = pesoRN)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             shape = 21,\n             fill = \"tomato\",\n             color = \"gray20\", \n             size = 4, \n             stroke = 1) +\n  geom_smooth(method = \"lm\", \n              se =TRUE,\n              color= \"darkred\") +\n  scale_fill_lancet(alpha = 0.6) +\n  xlab(\"Comprimento do Recém-nascido (cm)\") +\n  ylab(\"Peso do Recém-nascido (g)\") +\n  theme_bw(base_size = 13)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 8.16: Gráfico de dispersão com reta de regressão\n\n\n\n\n\nO gráfico da Figura 8.16, mostra um ajuste dos pontos a uma reta, com inclinação ascendente, ou seja uma correlação positiva, à medida que o comprimento do recém-nascido aumenta, aumenta o seu peso ao nascer. Pela forte inclinação da reta. pressupoe-se que esta correlação é alta.\nA distância dos pontos à reta é o erro ou resíduo. A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada (veja também Capítulo 18).",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-hist",
    "href": "08-graficos.html#sec-hist",
    "title": "8  Gráficos",
    "section": "8.5 Histograma",
    "text": "8.5 Histograma\nO histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos.\nNo histograma, as frequências observadas são representadas por intervalos de classes de ocorrência que estão no eixo x e a altura das barras, representando a frequência de cada intervalo, no eixo y. A área de cada barra é proporcional à porcentagem de observações de cada intervalo. O geom_histogram() é a geometria para a construção de um histograma. Aqui, há necessidade apenas do eixo x, pois existe uma única variável. A execução do comando retorna a distribuição dessa variável.\nOs dados para plotar um histograma, serão provenientes do dataframe dados (Seção 8.2), utilizando um filtro para as gestações a termo, designado como dadosRNT.\n\nset.seed(123)\ndadosRNT &lt;- dados %&gt;% \n  filter(ig &gt;= 37 & ig &lt; 42) \n\nSerá construído um histograma simples da variável pesoRN (peso dos recém-nascidos a termo), usando os eguinte código:\n\nggplot(dadosRNT, aes(x=pesoRN)) + \n  geom_histogram()+\n  labs(x = \" Peso dos Recém-Nascidos (g)\", \n       y = \"Frequência\")  +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.17: Histograma simples\n\n\n\n\n\nA aparência do histograma da Figura 8.17 permite ter uma idéia da distribuição e simetria dos dados, mas não está com um aspecto agradável, amigável. Mesmo que ele expresse corretamente a sua mensagem, essa mensagem pode ser prejudicada por uma má aparência.\nO histograma recebe uma variável numérica (no caso, o peso dos recém-nascidos) e a divide em vários “compartimentos”, os intervalos, representados pelas barras. A escolha do tamanho (amplitude) do intervalo é de extrema importância para a aparência do histograma.\nO geom_histogram() tem um argumento, denominado binwidth que permite alterar a amplitude do intervalo. O binwidth é um intervalo e sua unidade é igual a da variável que se está “histogramando”. No exemplo, foi usado o peso do recém-nascido (g). Se o objetivo são intervalos de 200 em 200 gramas, o binwidth = 200. Uma outra maneira, é usar bins que agrupa em intervalos de mesmo tamanho. Por exemplo bins = 15, o geom_histogram() dividirá em 15 intervalos iguais, gerando um histograma semelhante ao da Figura 8.18. Junto com a alteração dos intervalos, vamos modificar a cor de preenchimento (fill) e bordas (color) das barras (Figura 8.18 ).\n\nggplot(dadosRNT, aes(x=pesoRN)) + \n  geom_histogram(binwidth = 200,\n                 fill = \"chartreuse\",\n                 color = \"darkgreen\")+\n  labs(x = \"Peso dos Recém-Nascidos (g)\", \n       y = \"Frequência\")  +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.18: Histograma simples modificado\n\n\n\n\n\nCom frequência se observa um histograma com curva normal sobreposta (Figura 8.19) para facilitar a comparação dos dados com a distribuição normal. Isso pode ser conseguido com um código que usa função stat_function() para a construção da curva normal, baseada nos dados (média e desvio padrão da variável pesoRN) e a função after_stat(density), colocada na estética do histograma, no eixo y, para substituir a frequência pela densidade de probabilidade (veja Seção 9.6). O restante do código somente estabelece que a linha da curva será tracejada (linetype = “dashed”), de cor vermelha (color = “red”) e com tamanho 1 (linewidth = 1).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nggplot(dadosRNT) + \n  geom_histogram(aes(x = pesoRN, \n                     y = after_stat(density)),\n                 binwidth = 200,\n                 fill = \"chartreuse\",\n                 colour = \"darkgreen\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dadosRNT$pesoRN),\n                            sd = sd(dadosRNT$pesoRN)),\n                linetype = \"dashed\",\n                linewidth = 1,\n                color = \"red\") +\n  labs(x = \"Peso do Recém-Nascido (g)\", \n       y = \"Densidade de Probabilidade\")  +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.19: Histograma com curva normal sobreposta\n\n\n\n\n\nNa Figura 8.19, se observa que os pesos dos recém-nascidos se ajustam razoavelmente à curva normal.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-bxp",
    "href": "08-graficos.html#sec-bxp",
    "title": "8  Gráficos",
    "section": "8.6 Boxplot",
    "text": "8.6 Boxplot\nO boxplot é uma representação gráfica de um resumo eficaz, de fácil compreensão, de uma ou mais varáveis numéricas. Fornece uma análise visual da posição, dispersão, simetria, caudas e valores discrepantes (outliers) do conjunto de dados (Figura 8.20).\n\nPosição – Em relação à posição dos dados, observa-se a linha central do retângulo (a mediana ou segundo quartil).\nDispersão – A dispersão dos dados pode ser representada pelo intervalo interquartil (IIQ), tamanho da caixa, que é a diferença entre o terceiro quartil (3ºQ) e o primeiro quartil (1ºQ), ou ainda pela amplitude que é calculada da seguinte maneira: valor máximo – valor mínimo. Embora a amplitude seja de fácil entendimento, o intervalo interquartil é uma estatística mais robusta para medir variabilidade uma vez que não sofre influência de outliers.\nSimetria – Um conjunto de dados que tem uma distribuição simétrica, terá a linha da mediana no centro do retângulo. Quando a linha da mediana está próxima ao primeiro quartil, os dados são assimétricos positivos e quando a posição da linha da mediana é próxima ao terceiro quartil, os dados são assimétricos negativos. Vale lembrar que a mediana é a medida de tendência central mais indicada quando os dados possuem distribuição assimétrica, uma vez que a média aritmética é influenciada pelos valores extremos.\n\nCaudas – As linhas que vão do retângulo até aos outliers podem fornecer o comprimento das caudas da distribuição.\n\nValores atípicos (Outliers) – Os valores atípicos indicam possíveis valores discrepantes. No boxplot, as observações são consideradas atípicas quando estão abaixo ou acima dos limites superior e inferior. O limite de detecção de valores atípicos (outliers) é construído utilizando o intervalo interquartil, dado pela distância entre o primeiro e o terceiro quartil. Sendo assim, os limites inferior e superior de detecção de outlier são dados por:\n\no Limite Inferior: 1ºQ – (1,5 * IIQ);\no Limite Superior: 3ºQ + (1,5 * IIQ). Tanto o limite superior como o inferior são representados por (º).\nos Valores extremos: são valores que estão acima ou abaixo de 3 vezes o IIQ e são representados por (*).\n\n\n\n\n\n\n\n\n\n\nFigura 8.20: Anatomia de um Boxplot\n\n\n\n\n\nOs boxplots são construídos com o geom_boxplot(). Deve-se especificar uma variável quantitativa para o eixo y e uma variável qualitativa para o eixo x (grupo). Se não houver, variável x e tem-se apenas um vetor de valores numéricos, então, ignora-se a variável x.\nPara o exemplo de construção de um boxplot, será usada a variável pesoRN do conjunto de dados dadosRNT00 carregados na Seção 8.3.1.\n\nggplot(dadosRNT100, \n       aes(x = \"\", y = pesoRN)) +\n  geom_boxplot(fill = \"skyblue\", alpha = 0.6) +\n  labs (x = NULL, y = \"Peso do Recém-nascido (g)\") +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.21: Boxplot simples\n\n\n\n\n\nO geom_boxplot() vazio gera um gráfico sem cores. Colocando o preenchimento fill = “skyblue”,tem-se um boxplot de cor azul céu (Figura 8.21).\nNa aparência do boxplot , é clássico o seu formato com os “bigodes” terminando em “T”, como mostra a Figura 8.20, e não um traço simples. Para modificar isso, pode-se criar uma nova camada de barra de erro, usando a função geom_errorbar(), antes de geom_boxblot(). Assim, como o boxplot passa ser a camada mais superficial, ele impede que se visualize a barra de erro na caixa (Figura 8.22), desde que ele seja opaco (remover ou zerar o argumento alpha). A função geom_errorbar() normalmente é usada para barras de erro, no entanto, aqui ela está sendo utilizada com stat = \"boxplot\" 11, o que significa que os cálculos de estatística do boxplot serão aplicados à barra de erro. O argumento width = 0.1 ajusta a largura das barras de erro, tornando-as mais estreitas.\n\nggplot(dadosRNT100, \n       aes(x = \"\", y = pesoRN)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs (x = NULL, y = \"Peso do Recém-nascido (g)\") +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.22: Boxplot simples com bigodes em T\n\n\n\n\n\n\n8.6.1 Múltiplos boxplots\nOs boxplots são bastante úteis quando se compara dois grupos, tornando-se uma ferramenta conveniente para compreender rapidamente as diferenças entre esses grupos. Ao usar os boxplots para comparar grupos, deve-se ter cuidado, pois os resumos podem levar à perda de informação que pode induzir erros de interpretação. Considere os boxplots da Figura 8.23, comparando o comprimentos de recém-nascidos a termo masculinos e femininos, a mediana dos meninos é mais alta do a das meninas. As meninas apresentam a mediana fora do centro das caixas, indicando um certo grau de assimetria. Mesmo sendo possível obter informações importantes sobre os dados, usando um boxplot, não se pode discernir a distribuição subjacente dos pontos de dados individuais dentro de cada grupo ou o número total de observações.\nAs cores dos boxplots serão definidas manualmente dentro do geom_boxplot(). Além disso, será adicionada a função theme(legend.postion= \"none) para remover a legenda, pois ela é redundante neste gráfico, uma vez que os sexos já estão mencionados no eixo x.\n\nggplot(dadosRNT100, aes(x = sexo, y = compRN)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(fill= c(\"goldenrod2\", \"orangered3\")) +\n  labs(x = NULL, y = \"Comprimento dos Recém-Nascidos (cm)\") +\n  theme(legend.postion= \"none\") +  \n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.23: Comparação de dois grupos com boxplots\n\n\n\n\n\nSe necessários mais informações, é possível adicionar jitter12 no boxplot da (Figura 8.24) para torná-lo mais esclarecedor e visualizar melhor a distribuição dos dados.\n\nggplot(dadosRNT100, aes(x = sexo, y = compRN)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(fill= c(\"goldenrod2\", \"orangered3\")) +\n  geom_jitter(color=\"grey30\", size=1.5) +\n  labs(x = NULL, y = \"Comprimento dos Recém-Nascidos (cm)\") +\n  theme(legend.postion= \"none\") +  \n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.24: Boxplots com jitter\n\n\n\n\n\n\n\n8.6.2 Boxplots horizontais\nPara criar boxplots horizontais, adiciona-se a função coord_flip() à função geom_boxplot() para inverter os eixos. Em um boxplot padrão, a variável categórica está no eixo x e a variável numérica no eixo y. Com coord_flip(), as variáveis são invertidas, colocando a variável categórica no eixo y e a numérica no eixo x, resultando no boxplot horizontal da Figura 8.25.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nggplot(dadosRNT100, aes(x = sexo, y = compRN)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(fill= c(\"goldenrod2\", \"orangered3\")) +\n  coord_flip() +\n  labs(x = NULL, y = \"Comprimento dos Recém-Nascidos (cm)\") +\n  theme(legend.postion= \"none\") +  \n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.25: Boxplots horizontais",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#sec-violin",
    "href": "08-graficos.html#sec-violin",
    "title": "8  Gráficos",
    "section": "8.7 Gráfico de violino",
    "text": "8.7 Gráfico de violino\nOs gráficos de violino permitem visualizar a distribuição de uma variável numérica para um ou vários grupos. No ggplot2, são construídos com o geom_violin() e, com frequência, substituem os boxplots, principalmente, quando se tem uma amostra muito grande e usar o jitter no boxplot pode não ser eficaz, pois os pontos podem se sobrepor e tornar a figura inelegível.\nCada “violino” representa uma variável de agrupamento. A forma representa a estimativa de densidade de probabilidade da variável: quanto mais pontos de dados em um intervalo específico, mais largo será o violino para esse intervalo. É muito parecido com um boxplot, mas permite uma compreensão mais profunda da distribuição.\nO gráfico de violino é uma técnica poderosa de visualização de dados, pois permite comparar a classificação de vários grupos e sua distribuição. São particularmente adequados quando a quantidade de dados é grande e é impossível mostrar observações individuais. Para conjuntos de dados pequenos, um boxplot com jitter é provavelmente uma opção melhor, pois realmente mostra todas as informações.\nPara o exemplo prático, será usada uma amostra proveniente do conjunto de dados dados (veja Seção 8.2), com filtrado para as gestações a termo, dadosRNT. Serão utilizadas as variáveis pesoRN e categFumo, tabagismo entre as gestantes, de acordo com a intensidade (não fumante, fumante leve. fumante moderada, fumante pesada) . O objetivo é observar visualmente o impacto do tabagismo sobre os pesos dos recém-nascidos .\nPara construir o gráfico de violino, serão usados os argumentos trim = FALSE, para não aparar as caudas, e draw_quantiles = c(0.25, 0.5, 0.75), para traçar os quartis (Figura 8.26). As cores das categoria foram definidas pelo ggplot2.\nReiterando, função theme(legend.position = \"none\") será colocada para evitar que a legenda das categorias apareça, uma vez que ela é explicita no gráfico.\n\nggplot(dadosRNT, aes(x=categFumo, y=pesoRN,         \n                       fill=categFumo)) + \n  geom_violin(trim = FALSE,\n              draw_quantiles = c(0.25, 0.5, 0.75)) +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Peso do Recém-Nascido (g)\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.26: Gráfico de violino com os quartis\n\n\n\n\n\nUma alteração interessante que pode ser feita no gráfico de violino, é colocar um boxplot, dentro do mesmo (Figura 8.27), faz o efeito do argumento draw_quantiles(), usado na Figura 8.26. Facilita a interpretação e, na opinião do autor, é mais bonito e elegante. O argumento width = 0.5, na função geom_boxplot(), estabelece a largura do boxplot, evitando que o boxplot se estenda para fora do “violino”.\n\nggplot(dadosRNT, aes(x=categFumo, y=pesoRN, fill=categFumo)) + \n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.5) +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Peso do Recém-Nascido (g)\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.27: Gráfico de violino com boxplots\n\n\n\n\n\nA observação da Figura 8.27 mostra uma tendência do peso do recém-nascido diminuir à medida que intensidade do fumo aumenta. O gráfico sugere que esta tendência não é sugnificativa, pois as caixas se sobrepõem.\nPara obter uma versão horizontal da Figura 8.26, chama-se a função coord_flip() 13que permite inverter os eixos x e y e, assim, tornar a interpretação mais intuitiva, mais amigável (?).\nPara interpretar um gráfico de violino, observar o seguinte:\n\nForma do violino, observando a largura em diferentes pontos para entender onde os dados se concentram.\nA linha mediana e a caixa do boxplot associado indicam a mediana e o intervalo interquartil, respectivamente.\nSe o violino é simétrico em torno da mediana, a distribuição dos dados é aproximadamente simétrica.\nSe a parte superior do violino é mais larga, os dados podem ser assimétricos, inclinados para valores maiores.\nEm múltiplas categorias, pode-se comparar rapidamente as distribuições. Diferentes formas e larguras entre as categorias fornecem uma visão clara das variações entre elas.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#gráfico-de-barras",
    "href": "08-graficos.html#gráfico-de-barras",
    "title": "8  Gráficos",
    "section": "8.8 Gráfico de barras",
    "text": "8.8 Gráfico de barras\nO gráfico de barras é uma análogo do histograma, onde as barras, ao contrário deste, são separadas. Os gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas. A função geom_bar() permite delinear o gráfico de barras da Figura 8.28.\nPara os exemplos práticos, será usada uma amostra proveniente do conjunto de dados dados (Seção 8.2), manipulando as mesmas variáveis: categFumo , tabagismo materno, e categIdade , idade materna categorizada. Em outros exemplos de grágicos barras, serão usadas as variáveis fumo (fumante e não fumante), para (número de filhos anteriores) e sexo do recém-nascido.\nO gráfico de barras inicial servirá para para visualizar a prevalência (Seção 22.2.1) de fumo na gestação categorizada pela intensidade do fumo.\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count))))+ \n  labs(x = \"Tabagismo Materno\", \n       y = \"Proporção por categoria\")  +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.28: Gráfico de barras padrão do ggplot2\n\n\n\n\n\nAs cores de preenchimento das barras podem ser alteradas, de acordo com a variável categórica. As cores serão estabelecidas de acordo com o padrão do ggplot2 (Figura 8.29). O gráfico retornará uma legenda, mostrando o que representa cada cor. Ela é desnecessária porque fica explicito, no eixo x, o que cada barra representa. Portanto, é uma boa conduta remover a legenda com a função theme (legend.position = \"none\"), como já visto em outras ocasiões (boxplots e gráfico de violino):\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count)),\n               fill = categFumo))+ \n                 labs(x = \"Tabagismo Materno\", \n                      y = \"Proporção por categoria\")  +\n                 theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.29: Gráfico de barras com as cores das barras estabelecidas pelo ggplot2\n\n\n\n\n\nAs cores padrão do ggplot2 podem ser alteradas, como foi visto na Seção 8.4.6, escolhendo manualmente, ou usando uma paleta, como as apresentadas pelo pacote ggsci, RColorBrewer ou paletteer. No exemplo, será usada a paleta do periódico New England Journal of Medicine (NEJM), Figura 8.30.\n\nlibrary(ggsci)\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count)),\n               fill = categFumo))+ \n  scale_fill_nejm() +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Proporção por categoria\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.30: Gráfico de barras com cores da paleta NEJM\n\n\n\n\n\n\n8.8.1 Proporção ou porcentagem\nNa Figura 8.30, a unidade do eixo y encontra-se como uma proporção y = after_stat(count/sum(count), ou seja, y = frequência por categoria/total de observações.\nÉ possível modificar para porcentagem (Figura 8.31), empregando a função percent_format() do pacote scales. O código é praticamente igual, apenas acrescentar o argumento labels = percent_format(accuracy = 0.1, decimal.mark = “,”) dentro da função scale_y_continuous().\n\nlibrary(ggsci)\nlibrary(scales)\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count)),\n               fill = categFumo))+ \n  scale_fill_nejm() +\n  scale_y_continuous (labels = percent_format (accuracy = 0.1,\n                                               decimal.mark = \",\")) +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Porcentagem por categoria\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.31: Gráfico de barras com porcentagem no eixo y\n\n\n\n\n\n\n\n8.8.2 Controle da largura da barra com width\nPara controlar a largura e o espaço entre as barras, num gráfico de barras no ggplot2, usar o argumento width dentro da função geom_bar(), definindo um valor entre 0 e 1 (ou um valor fixo). Um valor de 1 representa a largura total, ou seja, não haverá espaço entre as barras, como no histograma.\nComo exemplo, será alterado a largura das barras do gráfico da figura Figura 8.30 . Se o objetivo é que as barras sejam mais estreitas e com mais espaço entre elas, deve-se definir um valor para width inferior a 0.9 (padrão). Na Figura 8.32, será usado width = 0.5.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggsci)\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count)),\n               fill = categFumo),\n           width = 0.5)+ \n  scale_fill_nejm() +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Proporção por categoria\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.32: Gráfico de barras com cores da paleta NEJM\n\n\n\n\n\n\n\n\n\n\n\nExercício\n\n\n\nQual a diferença entre geom_bar() e geom_col()? Construir o mesmo gráfico de barras da Figura 8.32, usando a geom_col().\n\n\nResposta 14\n\n\n8.8.3 Gráfico de barras empilhadas\nO gráfico de barras empilhadas é ideal para visualizar a proporção de cada grupo dentro de uma categoria. A altura total da barra representa a contagem total para a variável no eixo x, e as cores dentro da barra mostram a distribuição da segunda variável.\nPara criá-lo, mapear a primeira variável para o eixo x e a segunda variável para a estética fill. A função geom_bar() faz o empilhamento por padrão.\nCom os mesmos dados, usados até nos gráficos de barras (Seção 8.2), agora serão trabalhadas as variáveis categIdade e fumo com o objetivo de ver a proporção de tabagismo por faixa etária. Como aprimoramentos, se pretende colocar as porcentagens de fumantes em cada uma das faixas etária no topo das barras.\nEm primeiro lugar, calcular as proporções de fumante em cada uma das faixas etárias e a posição vertical dos rótulos no eixo y.\n\nproporcoes_fumo &lt;- dados %&gt;%\n  group_by(categIdade, fumo) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  group_by(categIdade) %&gt;%\n  mutate(posicao_y = sum(n) - (0.5 * n),\n         total_faixa = sum(n),\n         proporcao_fumo = n / total_faixa) %&gt;% \n  filter(fumo == \"Fumante\")\nproporcoes_fumo\n\n# A tibble: 3 × 6\n# Groups:   categIdade [3]\n  categIdade   fumo        n posicao_y total_faixa proporcao_fumo\n  &lt;fct&gt;        &lt;fct&gt;   &lt;int&gt;     &lt;dbl&gt;       &lt;int&gt;          &lt;dbl&gt;\n1 &lt; 20 anos    Fumante    35      202.         219          0.160\n2 20 a 35 anos Fumante   235      874.         992          0.237\n3 &gt; 35 anos    Fumante    31      142.         157          0.197\n\n\nApós realizado o cálculo das proporções de fumantes em cada faixa etária, constrói-se o gráfico. Para colocar as porcentagens no gráfico15, será usada a geometria geom_text(), informando essas porcentagens e a localização no eixo x e y, resultando na Figura 8.33.\n\nggplot(dados, aes(x = categIdade, fill = fumo)) +\n  geom_bar(color =\"black\") +\n  scale_fill_manual(values = c(\"gray90\",\"skyblue\")) +\n  labs(x = \"Faixa Etária\", \n       y = \"Frequência\",\n       fill = NULL)  +\n  geom_text(data = proporcoes_fumo,\n            aes(x = categIdade,\n                y = posicao_y,\n                label = scales::percent(proporcao_fumo, accuracy = 0.1)),\n            size = 4,\n            color = \"black\" ) + \n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  labs(x = \"Faixa Etária\",\n       y = \"Frequência\",\n       fill = NULL) +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigura 8.33: Gráfico de barras empilhadas\n\n\n\n\n\n\n\n8.8.4 Gráfico de barras lado a lado\nO gráfico de barras lado a lado (ou agrupado) é útil para comparar diretamente a contagem de cada grupo entre as categorias. As barras de uma mesma categoria são dispostas lado a lado para facilitar a comparação visual. Para criá-lo, se faz de maneira semelhante das barras empilhadas. Mapear a primeira variável para o eixo x e a segunda para a estética fill e adicionar o argumento position = \"dodge\" dentro do geom_bar(). Este argumento diz ao ggplot2 para não empilhar as barras, mas sim colocá-las lado a lado.\nO geom_text () usa position_dodge() para replicar o comportamento das barras. O argumento width = 0.9 é o valor padrão para a largura das barras no ggplot2 e garante um alinhamento perfeito. O gráfico de barras lado a lado é, portanto construído assim:\n\nIncialmente, calcula-se as proporções das categorias e a posição y dos rótulos:\n\n\nprop_fumo &lt;- dados %&gt;%\n  group_by(categIdade, fumo) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  group_by(categIdade) %&gt;%\n  mutate(\n    # Calcula a posição vertical do ponto médio de cada segmento\n    posicao_y = (n) - 18,\n    total_faixa = sum(n),\n    proporcao_fumo = n / total_faixa)\nprop_fumo\n\n# A tibble: 6 × 6\n# Groups:   categIdade [3]\n  categIdade   fumo            n posicao_y total_faixa proporcao_fumo\n  &lt;fct&gt;        &lt;fct&gt;       &lt;int&gt;     &lt;dbl&gt;       &lt;int&gt;          &lt;dbl&gt;\n1 &lt; 20 anos    Fumante        35        17         219          0.160\n2 &lt; 20 anos    Não fumante   184       166         219          0.840\n3 20 a 35 anos Fumante       235       217         992          0.237\n4 20 a 35 anos Não fumante   757       739         992          0.763\n5 &gt; 35 anos    Fumante        31        13         157          0.197\n6 &gt; 35 anos    Não fumante   126       108         157          0.803\n\n\n\nCom esses dados, constrói-se o gráfico:\n\n\nggplot(dados, aes(x = categIdade, fill = fumo)) +\n  geom_bar(position = \"dodge\",\n           color =\"black\") +\n  scale_fill_manual(values = c(\"gray90\",\"skyblue\")) +\n  labs(x = \"Faixa Etária\", \n       y = \"Frequência\") +\n  geom_text(data = prop_fumo,\n            aes(x = categIdade,\n                y = posicao_y,\n                label = scales::percent(proporcao_fumo, accuracy = 0.1)),\n            size = 3.5,\n            color = \"black\" ,\n            position = position_dodge(width = 0.9)) + \n  scale_y_continuous (expand = expansion(add = c(0,0.10))) +\n  labs(x = \"Faixa Etária\",\n       y = \"Frequência\",\n       fill = \"\") +\n  theme_classic(base_size = 12) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigura 8.34: Gráfico de barras lado a lado\n\n\n\n\n\nA Figura 8.34 mostra as porcentagens de fumantes em cada uma das categorias de uma forma bem clara.\n\n\n8.8.5 Gráfico de barra para uma variável numérica discreta\nUma variável numérica discreta é um tipo de variável que assume valores inteiros, resultantes de contagens, e que não podem assumir valores fracionários entre eles. A medição ocorre através da contagem das observações (Seção 2.5). Para a representação gráfica, utiliza-se um gráfico de barras . O resultado é semelhante a um histograma com as barras separadas. Para o exemplo, serão usados os mesmos dados empregados na construção dos gráficos de barras empilhadas e lado a lado, provenientes do conjunto de dados dadosMater.xlsx. Nesses dados, existe a variável para que representa o número de filhos que a gestante teve antes do atual. É uma variável numérica discreta que será mostrada visualmente por gráfico de barras simples (Figura 8.35).\n\n\n\n\n\n\nImportante\n\n\n\nApesar da variável para ser uma variável numérica discreta, para a construção do gráfico ela foi transformada em um fator para informar ao ggplot2 que todos os rótulos do eixo x (nº de filhos anteriores) devem aparecer, inclusive o referente a 7 filhos anteriores, apesar de não existir na amostra.\n\n\n\n# Criar uma tabela com contagem completa, incluindo zeros\ncontagem &lt;- as.data.frame(table(factor(dados$para, levels = 0:11)))\n\n# Calcular proporção de cada barra\ncontagem$prop &lt;- contagem$Freq / sum(contagem$Freq)\n\n# Plotar\nggplot(contagem, aes(x = Var1, y = prop)) +\n  geom_bar(stat = \"identity\", \n           fill = \"tomato\", color = \"gray30\") +\n  geom_text(aes(label = scales::percent(prop, accuracy = 0.1),\n                y = prop + 0.01), size = 3.5, color = \"black\") +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  labs(x = \"Número de filhos anteriores ao atual\", \n       y = \"Proporção\") +\n  theme_classic(base_size = 13)\n\n\n\n\n\n\n\nFigura 8.35: Gráfico de barras de uma variável discreta\n\n\n\n\n\n\n\n8.8.6 Gráfico de barra de erro\nUm gráfico de barra de erro é uma ferramenta visual que mostra a variabilidade de dados em um ponto específico. Ele consiste em pontos ou barras que representam as médias (ou outras estatísticas) de um conjunto de dados, com linhas verticais (ou horizontais) que indicam o intervalo de confiança, o desvio padrão ou o erro padrão da média. Essas linhas verticais são conhecidas como “barras de erro”. Usado para comparar as médias de diferentes grupos, mostrando a variabilidade dentro de cada grupo. É visto com frequência em pesquisas científicas e publicações para apresentar os resultados experimentais com suas respectivas variabilidades. As barras de erro dão uma ideia geral de quão precisa é uma medição. O cenário para a construção de um gráfico de barra de erro é o tabagismo na gestação e o peso dos recém-nascidos, onde as barras representarão a média do peso ao nascer (g) e as barras de erro com intervalo de confiança de 95% (veja Capítulo 12), calculado usando média \\(\\pm\\) margem de erro, onde a margem de \\(erro = 1.96 × erro \\ padrão\\).\n\nResumo dos dados: Após carregar os dados necessários (dadosRNT), se fará um resumo dos mesmos que informará as respectivas médias, desvios padrão e margens de erro por grupo.\n\n\n  resumo &lt;- dadosRNT %&gt;% \n    group_by(sexo, fumo) %&gt;% \n    summarise(n = n(),\n              media = mean(pesoRN, na.rm = TRUE),\n              dp = sd(pesoRN, na.rm = TRUE),\n              me = 1.96 * dp/sqrt(n),\n              min =min(pesoRN, na.rm = TRUE),\n              max =max(pesoRN, na.rm = TRUE),\n              .groups = 'drop')\n  print(resumo)\n\n# A tibble: 4 × 8\n  sexo      fumo            n media    dp    me   min   max\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino Fumante       122 3162.  464.  82.4  1440  4410\n2 Masculino Não fumante   470 3303.  453.  40.9  1425  4950\n3 Feminino  Fumante       110 2998.  503.  94.1  1715  4620\n4 Feminino  Não fumante   383 3190.  435.  43.6  2090  4485\n\n\n\nConstrução do gráfico (Figura 8.36) , conforme explicado abaixo:\na) O gráfico inicia com a colocação a variável sexo no eixo x e média da variável pesoRN no y e estabelecendo cores diferentes para o fator sexo;\nb) geom_bar(stat = “identity” usa os valores reais da média para a altura das barras, color = “black” estabelece a cor preta para o contorno das barras e position_dodge(0.9) separa as barras lado a lado para cada grupo de fumo;\nc) geom_point (position = position_dodge(0.9) adiciona um ponto sobre cada barra (pode ser útil para destacar a média). É opcional;\nd) geom_errorbar() adiciona a barra de erro acima da média, com base na barra de erro me. Poderia ter sido usado o desvio padrão. O erro está opcionalmente colocado acima da barra, mas poderia ser acima e abaixo ;\ne) labs() define os rótulos dos eixos e da legenda;\nf) coord_cartesian(ylim = c(0, 3500)) limita o eixo y de 0 a 3500, sem cortar dados fora desse intervalo. Entretanto, para reduzir a altura das barras, pode-se cortar dados, por exemplo começar em1000, 1500 ou, mesmo, 2000, uma vez que não existem recém-nascidos, nesta amostra, com menos de 2000 g e o foco é a média e o IC95%;\ng) scale_fill_manual(values = c(\"gray80\", \"darkslategray1\")) estabelece as cores para os níveis de fumo;\nh) scale_y_continuous( breaks = seq(0, 3500, 500) , expand = expansion(add = c(0, 0.05))) - a primeira parte define os rótulos do eixo y de 500 em 500, começando em 0 16, a segunda adiciona um pequeno espaço acima das barras para não cortar os rótulos (adiante será discutido sobre isso);\ni) Por último colocou o tema clássico17 do ggplot2 com fonte maior para melhorar a leitura.\n\n\nggplot(resumo, aes(x=sexo, y=media, fill=fumo)) +      \n  geom_bar(stat=\"identity\", color=\"black\", \n           position=position_dodge(0.9)) +\n  geom_point(position=position_dodge(0.9)) +\n  geom_errorbar(aes(ymin = media, ymax = media+me), width=0.2,\n                position=position_dodge(.9)) +\n  labs(x=\"\", \n       y = \"Peso do Recém-Nascido(g)\",\n       fill = \"\") +\n  coord_cartesian(ylim = c(1500, 3500)) +\n  scale_fill_manual(values = c(\"gray80\", \n                               \"darkslategray1\")) +\n  scale_y_continuous (breaks = seq(1500, 3500, 500),\n                      expand = expansion(add = c(0,0.05))) +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigura 8.36: Gráfico de barras de erro",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#manipulando-outras-partes-dos-gráficos",
    "href": "08-graficos.html#manipulando-outras-partes-dos-gráficos",
    "title": "8  Gráficos",
    "section": "8.9 Manipulando outras partes dos gráficos",
    "text": "8.9 Manipulando outras partes dos gráficos\n\n8.9.1 Mudando o nome dos eixos, o nome e a ordem dos rótulos\nPara modificar o nome dos eixos, utiliza-se, com frequência, as funções xlab() e ylab() como no gráfico da Figura 8.6 . O mesmo trabalho de alteração dos rótulos pode ser feito com a função labs() como no gráfico da figura Figura 8.14 .\nO nome e ordem dos rótulos podem ser modificados, usando a função scale_x_discrete() com os argumentos limits = que coloca os níveis na ordem desejada e labels = que coloca os novos nomes18 na ordem estabelecida pelo argumento limits =.\nObservando, por exemplo, o gráfico da Figura 8.30, onde se usou a paleta do NEJM, verifica-se que os rótulos do eixo x estão como: fumante_leve, fumante_moderada, fumante_pesada e nao_fumante. Esses nomes não estão prontos para publicação e o ideal é que sejam modificados para Leve, Moderado, Pesado e Não, uma vez que o título do eixo x será modificado para Tabagismo Materno. Aproveitando, pode-se modificar a ordem das categorias, colocando, por exemplo, as fumantes pesadas como primeira categoria na Figura 8.37, a seguir as fumantes moderadas, leves e não fumantes para ter uma lógico crescente da intensidade de tabagismo materno.\n\nCálculo das proporções de tabagismo em cada uma das categorias para adicionar ao gráfico, melhorando as informações\n\n\nprop_fumo &lt;- dados %&gt;%\n  group_by(categFumo) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  mutate(\n    # Calcula a posição vertical do ponto médio de cada segmento\n    posicao_y = (n) + 30,\n    total_faixa = sum(n),\n    proporcao_fumo = n / total_faixa)\nprop_fumo\n\n# A tibble: 4 × 5\n  categFumo            n posicao_y total_faixa proporcao_fumo\n  &lt;fct&gt;            &lt;int&gt;     &lt;dbl&gt;       &lt;int&gt;          &lt;dbl&gt;\n1 nao_fumante       1067      1097        1368         0.780 \n2 fumante_leve       157       187        1368         0.115 \n3 fumante_moderada    37        67        1368         0.0270\n4 fumante_pesada     107       137        1368         0.0782\n\n\n\nConstrução do gráfico que resultará na Figura 8.37.\n\n\nggplot(data = dados, aes(x = categFumo, fill = categFumo)) + \n  geom_bar(position =\"dodge\", color =\"black\")+ \n  scale_fill_nejm() +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Frequência\")  +\n  scale_x_discrete(limits = c(\"fumante_pesada\", \n                              \"fumante_moderada\", \n                              \"fumante_leve\",\n                              \"nao_fumante\"),\n                   labels = c(\"Pesado\", \n                              \"Moderado\", \n                              \"Leve\",\n                              \"Não\")) +\n  geom_text(data = prop_fumo,\n            aes(x = categFumo,\n                y = posicao_y,\n                label = scales::percent(proporcao_fumo, accuracy = 0.1)),\n            size = 4,\n            color = \"black\" ) +\n  theme_bw(base_size=13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.37: Gráfico de barras modificado\n\n\n\n\n\n\n\n8.9.2 Título e subtítulo do gráfico\nNem sempre necessários, o título, o subtítulo ou uma nota de rodapé podem ser adicionados ao gráfico através da função labs(), usada anteriormente (por ex. na Figura 8.14) para colocar rótulos nos eixos x e y. Além de argumentos para colocar título e subtítulo, a função labs() tem argumento para nota de rodapé, caption.\nComo exemplo, será plotado um gráfico com boxplots que ilustrem o impacto do tabagismo materno sobre o peso do recém-nascido. Os dados serão provenientes da amostra dadosRNT (Seção 8.7).\nSerão usados todos os argumento da função labs(), e se repetirá o que foi feito na construção do gráfico da Figura 8.37, alterando os nomes dos rótulos do eixo x. O código do gráfico da Figura 8.38 vai ser atribuído a um objeto denominado bxp:\n\nbxp &lt;- ggplot(dadosRNT, aes(x = categFumo, \n                            y = pesoRN,\n                            fill = categFumo)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.1) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Pastel2\")  +\n  stat_summary(fun = \"mean\", \n                colour = \"red\", \n                size = 3, \n                geom = \"point\") +\n  labs(title = \"Tabagismo Materno e Peso do Recém Nascido\", \n       subtitle = \"Maternidade do Hospital Geral de Caxias do Sul, 2008\",\n       x = \"Tabagismo Materno\",\n       y = \"Peso dos Recém-Nascidos (g)\",\n       caption = \"O ponto vermelho é a média de cada grupo\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                     labels = c(\"Não\", \"Leve\", \n                                \"Moderado\", \"Pesado\")) +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n  \nprint(bxp)\n\n\n\n\n\n\n\nFigura 8.38: Boxplots do impacto do tabagismo materno no peso do recém-nascido\n\n\n\n\n\n\n\n8.9.3 Modificação dos limites dos eixos\nO pacote ggplot2 possui uma família de funções scale_ para modificar as propriedades referentes às escalas do gráfico. Como é possível ter escalas de números, categorias, cores, datas, entre outras, é disponibilizada uma função específica para cada tipo de escala.\nCada tipo fundamental é manipulado por uma das três funções construtoras de escala: continuous_scale(), discrete_scale() e binned_scale().\nNo gráfico da Figura 8.38, os pesos dos recém-nascidos estão dispostos em uma escala que varia a cada 1000 g. Para modificar esses limites, pode-se usar a função scale_y_continuous() para ter intervalos de 500 g.\nO gráfico da Figura 8.38 foi designado para um objeto denominado bxp. Isto facilita o trabalho, pois não há necessidade de repetir todo o código que gerou o gráfico, apenas as modificações:\n\nbxp +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\")) +\n  scale_y_continuous(breaks = seq(1000, 5000, 500))\n\n\n\n\n\n\n\nFigura 8.39: Modificação dos limites do eixo y: 500 em 500\n\n\n\n\n\nJunto com a modificação dos limites dos eixo da Figura 8.39, manipulou-se o título e subtítulo, usando a função theme() e foi aumentado o tamanho da fonte, usou-se negrito e a cor do subtítulo passou a ser “darkgreen”.\n\n\n8.9.4 Modificação da expansão\nVoltando aos gráficos de barra, todos, com exceção do gráfico da Figura 8.36, tem algo que incomoda ao autor: abaixo do valor 0 (zero) existe uma expansão, ou seja um espaço abaixo do 0. Isto, visualmente, é desagradável.\nPara que as barras tenham início exatamente no 0 (zero), pode-se empregar a função scale_y_continuous() com o argumento expand = expansion (add = c(0,0.05)), significando que não se expande nada abaixo do 0 e se adiciona 5 unidades para cima, criando uma margem superior. Comparar a Figura 8.40 com a Figura 8.30. O rótulos do eixo y também foram corrigidos.\n\nlibrary(ggsci)\nlibrary(scales)\n\nggplot(data = dados) + \n  geom_bar(aes(x = categFumo, \n               y = after_stat(count/sum(count)),\n               fill = categFumo))+ \n  scale_fill_nejm() +\n  scale_y_continuous (labels = percent_format (accuracy = 0.1,\n                                               decimal.mark = \",\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  labs(x = \"Tabagismo Materno\", \n       y = \"Proporção por categoria\")  +\n  theme_bw(base_size = 13) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 8.40: Gráfico de barras com remoção do espaço abaixo de 0 (zero)",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#ajustando-o-layout-e-as-margens-no-ggplot2",
    "href": "08-graficos.html#ajustando-o-layout-e-as-margens-no-ggplot2",
    "title": "8  Gráficos",
    "section": "8.10 Ajustando o layout e as margens no ggplot2",
    "text": "8.10 Ajustando o layout e as margens no ggplot2\n\n8.10.1 Modificação das margens com a função theme()\nUsando o gráfico da Figura 8.7, repetido aqui e designando-o a um objeto com nome de gdsip:\n\ngdisp &lt;- ggplot(data = dadosRNT100,\n                aes(x = compRN, y = pesoRN)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0),\n             color = \"gray20\",\n             fill =\"steelblue\",\n             shape = 21, \n             alpha = 1,\n             size = 3,\n             stroke =1) +\n  ylab(\"Peso do Recém-nascido (g)\") +\n  xlab(\"Comprimento do Recém-nascido (cm)\")+\n  theme_classic(base_size = 13)\nprint(gdisp)\n\n\n\n\nGráfico de dispersão\n\n\n\n\nO objeto gdisp contém o gráfico de dispersão da Figura 8.7 e pode ser modificado sem ter que digitar todos os comandos novamente. Será usado para exemplicar como promover um aumento das margens, partindo do padrão do tema usado:\n\ntheme_classic()$plot.margin\n\n[1] 5.5points 5.5points 5.5points 5.5points\n\n\nPara aumentar as margens usa-se:\n\ngdisp +\n  theme(plot.margin = margin(t = 80, r = 80, b = 80, l = 80))\n\n\n\n\n\n\n\nFigura 8.41: Gráfico de dispersão com margem reduzida\n\n\n\n\n\nA função margin() define as margens em pontos 19. Um ponto (pt) equivale a 1/72 de polegada, ou aproximadamente 0,35 milímetros. É a mesma unidade usada para definir tamanho de fonte. portanto, quando se observa um valor de 80, significa que o gráfico terá 80 pontos de margem no topo, à direita, à esquerda e na base. Isto dá aproximadamente 28 mm de espaço em cada lado. As letras t, r, b e l equivalem, respectivamente a top, right, bottom e left.\n\n\n\n\n\n\nDica prática\n\n\n\nAo exportar gráficos para PDF ou PNG e quiser controlar o layout com precisão (por exemplo, para publicação), ajustar as margens com margin() é essencial para evitar que elementos fiquem cortados ou apertados demais.\n\n\n\n\n\n\n\n\nField, Andy, Jeremy Miles, e Zoë Field. 2012. «Exploring data with graphs». Em Discovering statistics using R, 117. Sage Publications, Ltd.\n\n\nHoltz, Yan. 2025. «R Color Brewer’s palettes». – the R Graph Gallery. https://r-graph-gallery.com/38-rcolorbrewers-palettes.html.\n\n\nHvitfeldt, Emil. 2024. «Use any color palette with paletteer». The R Graph Gallery. https://r-graph-gallery.com/package/paletteer.html.\n\n\nWickham, Hadley. 2010. «A layered grammar of graphics». Journal of Computational and Graphical Statistics 19 (1). Taylor & Francis: 3–28.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Danielle Navarro, e Thomas Lin Pedersen. 2023. ggplot2: Elegant Graphics for Data Analysis (3e). http://www.new.pmean.com/ggplot2-book/.",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "08-graficos.html#footnotes",
    "href": "08-graficos.html#footnotes",
    "title": "8  Gráficos",
    "section": "",
    "text": "categIdade \\(\\to\\) &lt; 20 anos, 20 a 35 anos e &gt; 35 anos; categFumo \\(\\to\\) Não fumante, Fumante leve: &lt;= 10 cigarros/dia, Fumante moderada: &gt; 10 a &lt; 20 cigarros/dia, Fumante pesada: &gt;= 20 cigarros/ dia↩︎\nGestações com idade gestacional igual ou acima de 37 semanas e abaixo de 42 semanas.↩︎\nO adicionar aqui é literal, pois isto é feito com o sinal (+).↩︎\nA legenda também pode ser colocada em outras partes do gráfico: “top” (superior), “left” (esquerda), “rigth” (direita) e ser removida (“none”).↩︎\nNão acontece nada! O R ignora o código e retorna um gráfico com as sua cor padrão, a preta.↩︎\nA função paletteer_c() é usada para variáveis contínuas; a paletteer_d() para variáveis categóricas e a paletteer_dinamic() é pouco usada, mas serve para paletas que mudam conforme o número de categorias.↩︎\nAlém da cor, os grupos em um gráfico também podem ser diferenciados por meio das estéticas shape e size. Para isso, substituir o mapeamento fill = sexo por shape = sexo ou size = sexo.↩︎\nAlém da função específica para o facetamento, a cor agora é determinada com preenchimento dos pontos ( fill=”tomato”) colocado dentro do geom_point() e houve uma diminuição do tamanho dos pontos (size = 4) .↩︎\nOutros métodos: “loess”, “glm”, “gam”. Para mais informações, consulte a ajuda ?loess, ?gam ou ?glm ou NULL, onde a escolha é automática: usa “loess” para &lt; 1000 pontos e “gam” para &gt; 1000.↩︎\nA Figura 8.15 mostrou que a correlação não parece diferir entre os sexos.↩︎\nO padrão é stat = \"identity\", o que significa que os valores das barras de erro devem ser fornecidos diretamente no conjunto de dados, sem cálculos adicionais.↩︎\nPrestar atenção para o fato de que o jitter é aleatório, por isso em cada execução do código os pontos se distribuem em posições diferentes.↩︎\nConsulte a construção da Figura 8.25↩︎\nA geom_bar() conta as ocorrências e usa a altura para representar essa contagem, enquanto geom_col() usa a altura para representar o valor especificado na estética y . Ambas as funções aceitam o argumento width. Enquanto o geom_bar() com after_start() calcula os valores automaticamente, o geom_col() exige que se forneça os valores de y (as proporções) diretamente.↩︎\nEste acréscimo das porcentagens dentro do gráfico é opcional. Para fazer o mesmo gráfico sem esta informação, não há necessidade dos cálculos das proporções e nem do geom_text() que deve ser removido.↩︎\nModificar se no coord_cartesian() o valor inicial for alterado, por exemplo, 1000, 1500 ou 2000.↩︎\nPode ser qualquer tema, este fica bem por ser bem limpo, sem grades, apenas eixo x e y.↩︎\nPode-se aproveitar aqui para trocar os nomes ou , simplesmente, corrigir acentuação que, às vezes, não foi colocada no dataframe.↩︎\nA margem pode ser definida também em centímetros (cm), usando o mesmo comando, mas especificando que é em “cm”. Por exemplo, para aumentar 2 cm em todos os lados: theme(plot.margin = unit(c(2, 2, 2, 20, \"cm\")↩︎",
    "crumbs": [
      "Parte III - Estatística Descritiva",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gráficos</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html",
    "href": "09-probabilidades.html",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "9.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr, readxl, scales, ggplot2)",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#introdução",
    "href": "09-probabilidades.html#introdução",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.2 Introdução",
    "text": "9.2 Introdução\nA teoria das probabilidades é a base sobre a qual a estatística é desenvolvida. Os jogos de azar deram um grande impulso ao conhecimento da moderna teoria das probabilidades, principalmente, pelo trabalho de Blaise Pascal (1623-1662), em parceria com Pierre de Fermat (1601-1665). Eles foram estimulados por um escritor francês e matemático amador, Antoine Gombaud (1607-1684), conhecido como Chevalier de Méré, que era muito interessado em jogos de azar (Debnath e Basu 2015).\nA Teoria das probabilidades permite que seja possível modelar populações, experimentos ou qualquer situação que possa ser considerada aleatória. Estes modelos possibilitam fazer inferência sobre populações a partir da observação de uma amostra dessa população. Ao usar apenas uma parte da população, inevitavelmente, é cometido um erro, o erro amostral. Este erro amostral pode ser dimensionado pela teoria das probabilidades.\nExistem duas interpretações alternativas de probabilidades: a frequentista e a bayesiana (Menezes 2004). Neste livro, será discutida, basicamente, a definição de probabilidade frequentista. O processo bayesiano de formulação de um modelo probabilístico faz uso do conhecimento subjetivo, estabelecendo uma especificação a priori, combinado com a informação objetiva ou empírica. A teoria bayesiana é a estrutura integradora dessas duas fontes de informação, derivando como resultado a distribuição a posteriori dos parâmetros de interesse. Na Seção 21.2, sobre análise de testes diagnósticos, serão abordados alguns aspectos relacionados à teoria bayesiana em medicina.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#processo-aleatório",
    "href": "09-probabilidades.html#processo-aleatório",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.3 Processo aleatório",
    "text": "9.3 Processo aleatório\nUm processo ou experimento é dito aleatório quando em uma situação se sabe quais os resultados que podem acontecer, mas não se sabe qual resultado particular irá acontecer. Por exemplo, quando uma moeda é lançada, se conhece que a probabilidade de o desfecho cara ocorrer é de 50%, mas se desconhece o que irá ocorrer até que a moeda esteja no chão.\nO número de caras que podem surgir em vários lançamentos da moeda é chamado de variável aleatória, ou seja, uma variável que pode assumir mais de um valor com determinadas probabilidades (Pagano e Kimberly 2000). Da mesma forma, um dado lançado pode mostrar seis faces, numeradas de um a seis, com igual probabilidade de 16,7%. Portanto, quando a probabilidade é associada a todos os conjuntos de valores possíveis de uma variável, diz-se que ela é aleatória. O conjunto de todos os possíveis resultados de um experimento aleatório é denominado espaço amostral.\nNa área da saúde, trabalha-se com uma infinidade de variáveis aleatórias, por exemplo, o número de filhos de uma mulher, o número de mortos diários em uma epidemia, o número de vacinados em uma campanha, etc. Essas variáveis são variáveis aleatórias discretas, pois apenas permitem ser quantificadas por processo de contagem. Por outro lado, o peso ou a altura de uma mulher são ditos variáveis aleatórias contínuas, pois podem assumir qualquer valor real entre uma medida e outra, dependendo da precisão do aparelho usado.\nEm geral, variáveis aleatórias são representadas por letras maiúsculas, como X, Y e Z e sua a probabilidade, por exemplo, pode ser denotada por: \\(P(X)\\).",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#definição-frequentista",
    "href": "09-probabilidades.html#definição-frequentista",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.4 Definição frequentista",
    "text": "9.4 Definição frequentista\nA probabilidade se relaciona a eventos futuros ou que ainda não ocorreram, desta forma a probabilidade pode ser entendida como uma medida de incerteza em relação ao evento. A probabilidade de um evento ocorrer, em determinadas circunstâncias, pode ser definida como a proporção de vezes que o evento é observado quando o experimento é repetido um número infinitamente grande de vezes (Menezes 2004). Pode-se dizer que a visão frequentista define a probabilidade como uma frequência de longo prazo.\nA chamada Lei dos Grandes Números diz que à medida que múltiplas observações são coletadas, a proporção observada de ocorrências de um determinado desfecho, após n ensaios, converge para a probabilidade real P desse desfecho. Ou seja, quanto mais vezes for repetido uma experiência, a melhor estimativa de probabilidade tende a ocorrer. Suponha que seja lançada uma moeda honesta repetidas vezes. Por definição, essa é uma moeda que tem \\(P(cara)=0,5\\). O que se observaria? O autor fez 20 lançamentos seguidos com uma mesma moeda e obteve o seguinte resultado, onde 1 = cara (Figura 9.1):\n\n\n\n\n\n\n\n\nFigura 9.1: Vinte lançamentos seguidos de uma moeda\n\n\n\n\n\nNeste caso, 10 (50%) desses lançamentos deram cara. Agora, suponha que foram feitos registros do número de caras (\\(n_1\\)) dos primeiros lançamentos (N) e calculadas as proporções de caras (\\(n_1⁄N\\)) todas as vezes. O resultado está na Figura 9.2.\n\n\n\n\n\n\n\n\nFigura 9.2: Proporção em 20 lançamentos de moeda\n\n\n\n\n\nObserva-se, nessa sequência, que a proporção de caras flutua muito, variando de 0,17 a 0,75. Se o número de lançamentos for aumentando tem-se a sensação de que a proporção se aproxima da “correta”. Por exemplo, com 100 jogadas, obteve-se 53 caras (0,53); com 150 jogadas, 79 (0,53) e com 200 jogadas, 111 (0,56). Quando N se aproximar do infinito (\\(N \\to\\infty\\)) a proporção de caras convergirá para 0,50. A definição frequentista de probabilidade segue essa definição. Ninguém consegue um número infinito de lançamentos de moedas, mas um computador pode simular milhares de lançamentos. A Figura 9.3 mostra o que acontece com a proporção \\(n_1⁄N\\) à medida que N aumenta em lançamentos de moedas. As simulações foram repetidas 4 vezes somente para ter certeza de que o que aconteceu não foi obra do acaso.\n\n\n\n\n\n\n\n\nFigura 9.3: Proporção à medida que N aumenta em lançamentos de moedas\n\n\n\n\n\nEmbora nenhuma das simulações tenha realmente terminado com um valor exato de 0,5, elas se aproximaram, oscilando muito pouco em torno desse valor.\n\n9.4.1 Aplicando a visão frequentista no dia a dia\nA definição frequentista também pode ser aplicada no cotidiano. Utilizando a altura de 1368 mulheres, uma medida numérica contínua, incluída no conjunto de dados dadosMater.xlsx (veja Seção 5.6). Essas alturas serão selecionadas e colocadas em um objeto, denominado dados.\n\n dados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nUsando a função summary(), será feito um resumo da variável altura:\n\nsummary(dados$altura)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.400   1.550   1.600   1.598   1.650   1.850 \n\n\nA mediana da altura das gestantes é 1.6 m. Em um longo conjunto de sorteios, a probabilidade de uma mulher ter altura acima devalor é 50%. O percentil 75 (3º quartil) é igual a 1.65 m, a probabilidade de estar acima deste valor, portanto, é 25%. É possível encontrar a probabilidade de a altura estar acima, abaixo ou entre quaisquer valores. Quando se faz a mensuração de uma variável contínua, fica-se limitado ao método usado. Portanto, quando se diz que uma mulher tem 160 cm, significa dizer que está entre 159,5 e 160,5 cm, dependendo da precisão do instrumento de medição. Dessa maneira, o interesse está na probabilidade de a variável aleatória assumir valores entre certos limites.\nA probabilidade de encontrar um valor exatamente igual à média (159.8) cm é quase igual a zero. Como se verá a seguir, isto pode ser verificado, no R, com bastante facilidade,calculando a distância que esta medida está da média em número de desvios padrão (escore Z):\n\nZ &lt;- (1.60 - mean(dados$altura))/sd(dados$altura)\nZ\n\n[1] 0.03103551\n\n\nObserve que o valor de 1,60 m está muito próximo da média e isto é um indicativo de que essa variável tem uma distribuição praticamente simétrica. Sabendo a distância, em números de desvios padrão, que 1,60 m está da média, qual a probabilidade de encontrar, na maternidade do HGCS 1, uma parturiente que tenha exatamente esta altura?\nPara responder a essa pergunta, será usada a função pnorm() (veja Seção 9.7.2) que utiliza o escore Z, a média e o desvio padrão para encontrar essa proporção que, multiplicada por 100, fornece a percentagem.\n\n p &lt;- pnorm (Z, mean(dados$altura),sd(dados $altura))\n p\n\n[1] 7.387473e-127\n\n\nO R por padrão retorna números grandes como notação científica. O resultado dessa operação é um número tão grande que para escrevê-lo sem este tipo de notação, seriam necessários 127 dígitos decimais. O resultado não caberia em apenas uma linha. Ficaria assim, suprimindo a notação científica 2:\n\n options(scipen =999)\n p\n\n[1] 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000007387473\n\n options(scipen = 0)\n\nOu seja, um número tão próximo de zero que poderia muito bem ser zero!",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#propriedades-das-probabilidades",
    "href": "09-probabilidades.html#propriedades-das-probabilidades",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.5 Propriedades das probabilidades",
    "text": "9.5 Propriedades das probabilidades\nAs seguintes propriedades simples decorrem da definição de probabilidade.\nSendo E um evento aleatório, a \\(P[E]\\) está entre 0 e 1, ou seja \\(0\\le P[E]\\le 1\\). Quando o evento certamente não ocorre, a probabilidade é 0, quando sempre ocorre a probabilidade é 1. Quando a probabilidade for igual a 0,50 tem-se máxima incerteza.\n\nRegra de adição (regra do “ou”)\n\nDois eventos A e B são mutuamente exclusivos, ou seja, quando A acontece, B não pode acontecer. Então, a probabilidade de que um ou outro aconteça é a soma de suas probabilidades. Por exemplo, um dado lançado pode mostrar um ou dois, mas não ambos. A probabilidade de mostrar um ou dois é igual a \\(1/6 + 1/6 = 1/3\\).\n\\[\nP[A ou B]=P[A]+P[B]\n\\]\nSe A e B não são mutuamente exclusivos, ou seja, quando A acontece pode também ocorrer B. Por exemplo, o nascimento de uma menina pode ser concomitante com o fato de ser branca.\n\\[\nP[A ou B]=P[A]+P[B]-P[A \\space e \\space B]\n\\]\n\nRegra de multiplicação (regra do “e”)\n\nSuponha que dois eventos (A e B) sejam independentes, ou seja, saber que um aconteceu não nos diz nada sobre se o outro aconteceu. Então, a probabilidade de que ambos aconteçam é o produto de suas probabilidades. Por exemplo, suponha que jogamos duas moedas. Uma moeda não influencia a outra, portanto os resultados dos dois lançamentos são independentes e a probabilidade de ocorrerem duas caras é 050 × 0,50 = 0,25.\n\\[\nP[A \\quad e\\quad B]=P[A]×P[B]\n\\]\nSe os eventos são dependentes, a probabilidade que ambos aconteçam é igual a:\n\\[\nP[A \\quad e \\quad B]=P[A]×P[B \\rvert A]\n\\] Com essas propriedades simples e outras mais complexas, é possível construir algumas ferramentas matemáticas extremamente poderosas, mas isso não faz parte do objetivo deste livro e não se entrará em detalhes.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#sec-distprob",
    "href": "09-probabilidades.html#sec-distprob",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.6 Distribuição de Probabilidades",
    "text": "9.6 Distribuição de Probabilidades\nUm conjunto de eventos que são mutuamente excludentes e que inclui todos os eventos que podem acontecer, é chamado de exaustivo. A soma de suas probabilidades é 1. O conjunto dessas probabilidades constitui uma distribuição de probabilidade.\nExistem diversos modelos probabilísticos que procuram descrever vários tipos de variáveis aleatórias discretas ou contínuas. Estas distribuições também são chamadas de modelos probabilísticos estocásticos que são definidas por duas funções matemáticas: a função de probabilidade (fp) para variáveis discretas, que atribui a cada valor a sua probabilidade de ocorrência (P(X=x)) e função densidade de probabilidade (fdp) para variáveis contínuas.\nA função de probabilidade é a função que atribui probabilidades a cada um dos possíveis valores da variável aleatória discreta, usando, em geral, as frequências relativas, apresentadas em uma tabela de frequência. O modelo de Bernoulli ou Binomial e o modelo de Poisson são exemplos de modelo probabilístico de variáveis discretas.\nA função densidade de probabilidade é a função que atribui probabilidade a qualquer intervalo de número reais, ou seja, um conjunto de valores não enumerável (infinito). Não é possível atribuir probabilidades para um determinado valor, é possível apenas para um intervalo. Por exemplo, o peso dos recém-nascidos. Para atribuir probabilidade a intervalos de valores é utilizada uma função e as probabilidades são representadas por áreas. Existem diversos modelos contínuos de probabilidade, mas o mais importante deles, é o modelo normal, também conhecido como modelo gaussiano.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#sec-normal",
    "href": "09-probabilidades.html#sec-normal",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.7 Distribuição Normal",
    "text": "9.7 Distribuição Normal\nO modelo probabilístico normal ou gaussiano é extremamente importante em estatística, pois serve como um fundamento para técnicas de inferência. Variáveis como os pesos dos recém-nascidos a termo, as alturas das mulheres adultas, a renda familiar em reais e muitas outras variáveis, na natureza, se ajustam ao modelo da distribuição normal.\nO modelo de distribuição normal sempre descreve uma curva simétrica, unimodal e em forma de sino (Figura 9.4).\n\n\n\n\n\n\n\n\nFigura 9.4: Curva Normal\n\n\n\n\n\nUma distribuição normal é descrita por meio de dois parâmetros: a média da distribuição \\(\\mu\\) e o desvio padrão da distribuição \\(\\sigma\\). Em função dessa informação, observe como a distribuição normal funciona se esses parâmetros forem alterados.\nComo é fácil prever, alterar a média desloca a curva de sino para a esquerda ou para a direita, enquanto a alteração do desvio padrão estende ou achata a curva, ou seja, muda a dispersão da distribuição.\nA Figura 9.5, mostra a distribuição normal com média 0 e desvio padrão 1, na curva à direita, a distribuição normal com média 1.5 e desvio padrão 1. Sobrepondo-se à curva da esquerda observa-se uma curva mais achatada (verde) que tem média 0 e desvio padrão 1.5. Observa-se, como mencionado, que modificando os parâmetros da curva, altera-se a posição ou o formato da mesma.\n\n\n\n\n\n\n\n\nFigura 9.5: Curvas normais com modificação dos parâmetros\n\n\n\n\n\n\n9.7.1 Características da distribuição normal\nA curva normal apresenta as seguintes características:\n\nA média e o desvio padrão descrevem exatamente uma distribuição normal, eles são chamados de parâmetros da distribuição. Se uma distribuição normal tem média \\(\\mu\\) e desvio padrão \\(\\sigma\\), pode-se escrever a distribuição como \\(N (\\mu,\\sigma)\\). As três distribuições dos gráficos da Figura 9.5 podem ser escritas como:\n\nCurva azul \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1)\\)\nCurva verde \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1.5)\\)\nCurva vermelha \\(\\to\\) \\(N(\\mu = 1.5,\\sigma = 1)\\)\n\nNa distribuição normal, a média, a mediana e a moda coincidem.\nA curva normal é simétrica em torno da média (\\(\\mu\\)).\nAs extremidades da curva, em ambos os lados da média, se estendem cada vez mais próximas do eixo x (abscissa) sem jamais tocá-lo. É assintótica.\nOs pontos de inflexão da curva são \\(\\mu - \\sigma\\) e \\(\\mu + \\sigma\\).\nA área total sob a curva é 1 ou 100%.\n\n\n\n9.7.2 Distribuição normal padronizada\nCada variável aleatória contínua tem a sua média e seu desvio padrão e, portanto, a sua curva normal correspondente.\nPara facilitar a comparação entre variáveis, foi criado o conceito de curva normal padronizada, que é uma curva normal com média 0 e desvio padrão 1. A distribuição normal padrão também pode ser chamada de distribuição normal centrada ou reduzida.\nPara calcular probabilidades associadas a distribuição normal, costuma-se converter a variável aleatória original X, em unidades reduzidas ou padronizadas, denominadas de escore Z ou escore padrão. Essa transformação é realizada pela equação que indica o número de desvios padrão envolvidos no afastamento do valor x em relação à média da população:\n\\[\nZ =\\frac{x-\\mu}{\\sigma}\n\\] onde:\n\nZ \\(\\to\\) escore Z\n\nx \\(\\to\\) valor qualquer da variável aleatória X\n\n\\(\\mu\\) \\(\\to\\) média da variável X\n\n\\(\\sigma\\) \\(\\to\\) desvio padrão da variável X\n\nQualquer distribuição de uma variável aleatória normal pode ser padronizada, usando o escore Z. Isto permite que se calcule a probabilidade de se encontrar determinados intervalos de valores (Gonzalez 2021).\nComo exemplo, se retornará à altura das mulheres. É, praticamente, impossível saber o valor da média populacional, por isso. costuma-se usar a média aritmética como um estimador da média populacional. Dessa forma, a variável dados$altura poderá utilizada com estimativa da média populacional. Em primeiro lugar, se construirá um tibble de nome resumo:\n\n resumo &lt;- dados %&gt;% \n   dplyr::summarise(n = n(),\n                    media = mean(altura, na.rm = TRUE),\n                    dp = sd(altura, na.rm = TRUE),\n                    min = min(altura, na.rm = TRUE),\n                    max = max(altura, na.rm = TRUE))\n resumo\n\n# A tibble: 1 × 5\n      n media     dp   min   max\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1368  1.60 0.0655   1.4  1.85\n\n\nAssim, pode-se verificar quantos desvios padrão uma mulher, pertencente a essa amostra, com 1,725m está afastada da média:\n\nZ &lt;- (1.725 - resumo$media)/resumo$dp\nround(Z, 2)\n\n[1] 1.94\n\n\nEsta mulher está distante praticamente 2 desvios padrão acima da média da sua população. Portanto, ela é considerada alta. Por que?\nPara responder a essa pergunta, há necessidade de calcular a probabilidade de encontrar uma mulher com esta altura, nesta população. Primeiro, calcula-se a probabilidade de encontrar uma mulher com esta altura, nesta amostra. No R, existem as funções dnorm(), pnorm() e qnorm(), que permitem calcular a densidade de probabilidade, a distribuição cumulativa e a função quantílica da distribuição normal para um conjunto de valores. Além dessas, há a função rnorm() que permite obter observações aleatórias que seguem uma distribuição normal (Jain 2022).\n\n9.7.2.1 Função pnorm()\nA função pnorm() fornece a Função de Distribuição Cumulativa (CDF) da distribuição Normal, que é a probabilidade de que a variável X contenha um valor menor ou igual a x.\nArgumentos:\n\nq \\(\\rightarrow\\) vetor de quantis\n\nmean \\(\\rightarrow\\) média\n\nsd \\(\\rightarrow\\) desvio padrão\n\nlower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são \\(P(X\\le x)\\), caso contrário \\(P(X &gt; x)\\)\n\nSe for usado \\(mean = 0\\) e \\(sd = 1\\), o valor de q = Z, caso contrário, toma-se os valores da média, o desvio padrão da população e o valor de x. Com esta função, é possível responder a pergunta feita anteriormente em relação a probabilidade de encontrar uma mulher com mais de 1,725m, equivalente a 1.94 desvios padrão acima da média, em uma população com média = 1.5979678 e desvio padrão = 0.0654787.\n\np &lt;- pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\np\n\n[1] 0.02618654\n\n\nOu, usando os valores:\n\npnorm(1.725, mean = resumo$media, sd = resumo$dp, lower.tail = FALSE)\n\n[1] 0.02618654\n\n\nObserva-se que, nesta amostra, apenas 2.6% das mulheres têm acima de 1,725m, razão de ser considerada uma mulher alta. Ou seja, é pouco provável encontrar mulheres acima dessa altura, nesta amostra.\nPara representar graficamente essa pequena probabilidade, será construída uma curva com essa pequena área sombreada, colorida em azul claro. Para isso, uma função própria, denominada normal_intervalo_sombreado() plotará a figura. Os argumentos dessa função são: a = limite inferior do intervalo; b = limite superior do intervalo; mean = média (padrão = 0); sd = desvio padrão (padrão = 1). Ela pode ser obtida aqui para ser baixada em seu diretório de trabalho para uso posterior 3. Para carregar a função para uso usa-se a função nativa source() que serve para executar o código contido em um arquivo com a extensão .R.\nA Figura 9.6 representa com clareza esta pequena probabilidade.\n\n\n\n\n\n\n\n\nFigura 9.6: Probabilidade de encontrar mulheres com mais de 1,725m\n\n\n\n\n\n\n\n9.7.2.2 Função qnorm()\nA função qnorm() permite encontrar o quantil q para qualquer probabilidade p. Portanto, a função qnorm é o inverso da função pnorm().\nArgumentos:\n\np \\(\\to\\) vetor de probabilidades\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nNo exemplo anterior, a probabilidade de se encontrar mulheres, na maternidade, com mais de 1,725m foi de 2.6%. Poderia ser calculado com a função qnorm() qual o escore Z correspondente:\n\nqnorm(p, mean = 0, sd = 1, lower.tail = FALSE)\n\n[1] 1.940054\n\n\n\n\n9.7.2.3 Função dnorm()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição normal dada uma certa variável aleatória X, uma média populacional \\(\\mu\\) e o desvio padrão populacional \\(\\sigma\\).\nArgumentos:\n\nx \\(\\to\\) vetor de quantis\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nEmbora x represente a variável independente da pdf para a distribuição normal, também é útil pensar em x como um escore Z. Por exemplo, a densidade de probabilidade quando x = 0 é igual:\n\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\nPara se construir uma curva de densidade de probabilidades normal ( \\(X \\sim N(μ=0,σ=1)\\)), basta aplicar a função dnorm() a uma sequência contínua de escores Z. O vetor de escores Z é obtido com a função seq(), como mostrado a seguir:\n\nescores_z &lt;- seq(-3,3, by = 0.05)\nescores_z\n\n  [1] -3.00 -2.95 -2.90 -2.85 -2.80 -2.75 -2.70 -2.65 -2.60 -2.55 -2.50 -2.45\n [13] -2.40 -2.35 -2.30 -2.25 -2.20 -2.15 -2.10 -2.05 -2.00 -1.95 -1.90 -1.85\n [25] -1.80 -1.75 -1.70 -1.65 -1.60 -1.55 -1.50 -1.45 -1.40 -1.35 -1.30 -1.25\n [37] -1.20 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65\n [49] -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05\n [61]  0.00  0.05  0.10  0.15  0.20  0.25  0.30  0.35  0.40  0.45  0.50  0.55\n [73]  0.60  0.65  0.70  0.75  0.80  0.85  0.90  0.95  1.00  1.05  1.10  1.15\n [85]  1.20  1.25  1.30  1.35  1.40  1.45  1.50  1.55  1.60  1.65  1.70  1.75\n [97]  1.80  1.85  1.90  1.95  2.00  2.05  2.10  2.15  2.20  2.25  2.30  2.35\n[109]  2.40  2.45  2.50  2.55  2.60  2.65  2.70  2.75  2.80  2.85  2.90  2.95\n[121]  3.00\n\n\nAgora, usando a função dnorm(), será construído conjunto de valores de densidade de probabilidade correspondentes aos escores Z obtidos anteriormente:\n\nvalores_d &lt;- dnorm(escores_z, mean = 0, sd = 1)\n\nEstes valores serão plotados para construir a curva normal (Figura 9.7):\n\n\n\n\n\n\n\n\nFigura 9.7: Função densidade de probabilidade\n\n\n\n\n\nOs argumentos básicos a serem informados da função axis() são: side=, at= e labels=. Esses argumentos determinam qual eixo será preenchido, qual a posição dos valores no eixo e a sequência de valores a ser preenchida, respectivamente. O argumento side= recebe valores que vão de 1 a 4: 1 = eixo inferior, 2 = eixo lateral esquerdo, 3 = eixo superior, 4= eixo lateral direito. Ou seja, partindo do eixo inferior (eixo x), os valores aumentam até 4 seguindo o sentindo horário para os quatros lados do gráfico. No exemplo, foi modificado o eixo x, logo side = 1. O argumento at = estabelece os pontos (densidades de probabilidade) do eixo x que receberão os rótulos, especificados no argumento label =.\nComo se pode ver, dnorm() fornece a “altura” do pdf da distribuição normal em qualquer escore Z que se forneça como argumento.\n\n\n9.7.2.4 Função rnorm()\nA função rnorm() gera n números aleatórios com distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\sigma\\).\nArgumentos:\n\nn \\(\\to\\) número de observações a serem geradas\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nCom esta função é possível, por exemplo, gerar 10 observações de uma distribuição normal:\n\nrnorm(10)\n\n [1] -0.82948264 -2.07087155  1.61373891 -0.65597468 -1.92658708  0.05687942\n [7] -0.33220699 -0.80301895  0.31804927 -2.31707086\n\n\nNo entanto, deve-se notar que, se uma “semente” (seed) não for especificada, a saída não será reproduzível, ou seja, cada vez que o comando for executado, retornará um novo conjunto de observações:\n\nrnorm(10)\n\n [1]  0.123937772 -1.768075089  0.143984241  1.103435514 -0.258556163\n [6] -0.783556573 -1.502900318 -1.932686151 -0.006632923 -1.344596996\n\n\nCada vez que este comando for reproduzido, retornará uma nova série de 10 números diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada, no exemplo, pela função rnorm(). O valor do número (“semente”) não é importante, é apenas um identificador. Para ilustrar, será construído dois conjuntos de 10 números que serão recebidos pelos objetos x e y. Para gerar o conjunto de números x, será usado o número 123 como “semente”. A “semente” funciona como uma espécie de marca. Para o y não será usado a função set.seed():\n\nn &lt;- 10\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny &lt;- rnorm(n)\ny\n\n [1]  1.2240818  0.3598138  0.4007715  0.1106827 -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n\nComparando os conjuntos com a função identical() do R base, observa-se que os conjuntos são diferentes:\n\nidentical(x, y)\n\n[1] FALSE\n\n\nAgora, repetindo os mesmos comandos, mas usando antes a mesma “semente”, observa-se que os conjuntos são idênticos.\n\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nset.seed (123)\ny &lt;- rnorm(n)\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nidentical(x, y)\n\n[1] TRUE\n\n\nOutros usos da função rnorm():\nA função rnorm()será usada para gerar três vetores diferentes de números aleatórios de uma distribuição normal.\n\nset.seed(1234)\nn10 &lt;- rnorm(10, mean = 0, sd = 1)\nn100 &lt;- rnorm(100, mean = 0, sd = 1)\nn10000 &lt;-  rnorm(10000, mean = 0, sd = 1)\n\nEm sequência, serão construídos histogramas (Figura 9.8), onde se pode observar que, aumentando o número de observações, tem-se gráficos que irão progressivamente se aproximando da verdadeira função de densidade normal.\n\n\n\n\n\n\n\n\nFigura 9.8: Histogramas construídos com amostras geradas pela função rnorm\n\n\n\n\n\nObserve também que à medida que n aumenta, a distribuição dos dados caracteriza-se como uma distribuição normal. Na Seção 11.2.2, este assunto voltará à cena.\n\n\n\n9.7.3 Regra Empírica 68-95-99.7\nA regra empírica diz que, se uma população de um conjunto de dados tem uma distribuição normal com média 0 e desvio padrão 1 (\\(X\\sim N(\\mu=0,\\sigma=1)\\)) pode-se afirmar que aproximadamente, 68%, 95% e 99,7% dos valores encontram-se, respectivamente, dentro de \\(\\pm\\) 1, 2 e 3 desvio padrão acima e abaixo média.\nEssa regra pode ser usada para descrever uma população e ajudar a decidir se uma amostra de dados veio de uma distribuição normal. Se uma amostra é grande o suficiente e a observação do histograma tem um formato parecido com um sino, é possível verificar se os dados seguem as especificações 68-95-99,7%. Se sim, é razoável concluir que os dados vieram de uma distribuição normal.\nExemplo:\nCom a função rnorm(), será gerado um vetor de 1000 números e um histograma com curva normal sobreposta:\n\n\n\n\n\n\n\n\nFigura 9.9: Histograma com curva normal sobreposta\n\n\n\n\n\nNo histograma (Figura 9.9), a probabilidade entre os escores Z - 1 e + 1 (entre as duas linhas tracejadas A e B) é igual a aproximadamente 68%. Pode-se calcular isto facilmente, usando a função pnorm():\nProbabilidade abaixo de z = 1, abaixo do ponto B:\n\nB &lt;- pnorm (1, 0, 1)\nB &lt;- round(B, 3)*100\nB\n\n[1] 84.1\n\n\nProbabilidade abaixo de z = -1, abaixo do ponto A:\n\n A &lt;- pnorm (-1, 0, 1)\n A &lt;- round(A, 3)*100\n A\n\n[1] 15.9\n\n\nLogo , a área abaixo da curva entre A e B é igual a:\n\n prob &lt;- B - A\n prob\n\n[1] 68.2\n\n\n\n\n9.7.4 Calculando probabilidades em uma distribuição normal\n\n\n\n\n\n\nExemplo 1\n\n\n\nA variável dados$altura, mostrada na Seção 9.4.1, tem uma média (1.598, mediana de (1.6) e um coeficiente de variação (CV) muito pequeno de 4.1%. Isso caracteriza uma variável com uma distribuição praticamente simétrica. Usando esses dados (\\(X\\sim N(\\mu=1,598,\\sigma=0,065)\\)), pode-se calcular probabilidades, dadas pela área sob a curva.\nBaseado nessas informações, qual a probabilidade de se encontrar mulheres com altura entre 1,47 e 1,73 m?\n\n\nResposta:\n\n# Dados\n mu &lt;- 1.598\n sigma &lt;- 0.065\n linf &lt;- 1.47\n lsup &lt;- 1.73\n \n # Solução\n z1 &lt;-  (linf - mu)/sigma\n z2 &lt;-  (lsup - mu)/sigma\n\n p1 &lt;- pnorm(linf, mu, sigma)\n p2 &lt;- pnorm(lsup, mu, sigma)\n \n p2 - p1\n\n[1] 0.9543975\n\n\n\n\n\n\n\n\n\n\nFigura 9.10: Probabilidade de encontrar mulheres entre 1,47 e 1,725m\n\n\n\n\n\nA probabilidade de alturas entre 1,47m e 1,73m é igual a 95,4%, como mostrado na Figura 9.10.\n\n\n\n\n\n\nExemplo 2\n\n\n\nOs dados de uma pesquisa mostram informações sobre o tempo de cirurgia para reconstrução do ligamento cruzado anterior (LCA). A distribuição de probabilidades se ajusta à distribuição normal com o tempo médio de cirurgia de 129 minutos com um desvio padrão de 14 minutos.\n\nQual a probabilidade de uma cirurgia de reconstrução do LCA requerer um tempo menor do que 100 minutos?\nSe uma cirurgia demorar 160 minutos, o que se conclui em relação a essa informação?\n\n\n\nResposta 1:\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 100\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.01915938\n\n\n\n\n\n\n\n\n\n\nFigura 9.11: Probabilidade do tempo de cirurgia de LCA menor ou igual a 100 minutos\n\n\n\n\n\nDe acordo com a distribuição, 1,92%% das cirurgias irão demandar quantidade de tempo menor do que 100 minutos (Figura 9.11).\nResposta 2:\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 160\n\n # Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = FALSE)\n p\n\n[1] 0.01340457\n\n\n\n\n\n\n\n\n\n\nFigura 9.12: Probabilidade do tempo de cirurgia de LCA maior ou igual a 160 minutos\n\n\n\n\n\nDe acordo com a distribuição, 1,34% das cirurgias irão demandar quantidade de tempo \\(\\ge 160\\) minutos (Figura 9.12). Ou seja, é uma probabilidade muito pequena!\n\n\n\n\n\n\nExemplo 3\n\n\n\nSupondo que em uma determinada ilha hipotética existam duas populações etnicamente diferentes onde as mulheres têm as seguintes medidas de altura: população 1 tem μ = 160 cm e σ = 6,6 cm e a população 2 tem μ = 140 cm e σ = 6,6 cm. As alturas de ambas as populações têm distribuição normal. Essas duas populações têm o mesmo aspecto físico, podendo ser distinguidas apenas geneticamente.\n\nQual a probabilidade de uma mulher com 150 cm pertencer a população 1?\nQual a probabilidade de uma mulher com 150 cm pertencer a população 2?\n\n\n\nResposta 1:\n\n# Dados\n mu &lt;- 160\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.06486702\n\n\n\n\n\n\n\n\n\n\nFigura 9.13: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 160 cm\n\n\n\n\n\nNa população 1, 6.5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 9.13). Em outras palavras, existe pouca probabilidade dessa mulher pertencer à população 1.\nResposta 2:\n\n# Dados\n mu &lt;- 140\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n \n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.935133\n\n\n\n\n\n\n\n\n\n\nFigura 9.14: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 140 cm\n\n\n\n\n\nNa população 2, 93,5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 9.14). Concluindo, ela pode pertencer a qualquer uma das populações. Pode ser uma mulher alta da população 2 ou uma “baixinha” da população 1!",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#distribuição-binomial",
    "href": "09-probabilidades.html#distribuição-binomial",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.8 Distribuição Binomial",
    "text": "9.8 Distribuição Binomial\nA distribuição normal padrão é apenas um dos exemplos de distribuição de probabilidade. Uma boa parte das situações se ajustam a ela. Entretanto, diversas situações reais muitas vezes se aproximam de outras distribuições estocásticas definidas por algumas hipóteses. Daí a importância de se conhecer e manipular algumas destas distribuições. Entre elas, a distribuição binomial.\nQuando um experimento aleatório resulta em um de dois, mutuamente exclusivos, desfechos, tais como vivo/morto, positivo/negativo, sim/não, masculino/feminino é denominado de Ensaio de Bernoulli. Recebeu esta denominação em homenagem ao matemático suíço, Jacob Bernoulli (1654-1705), considerado fundador do cálculo e da teoria da probabilidade (Robertson e O’Connor 2022).\nA distribuição de frequências que descreve as proporções de um ensaio de Bernoulli, chama-se Distribuição Binomial. A probabilidade binomial dá a probabilidade de determinado desfecho ocorrer em determinado número de ensaios independentes. Uma sequência de ensaios de Bernoulli forma um Processo de Bernoulli.\nA distribuição binomial é importante para variáveis discretas. Existem poucas condições que precisam ser atendidas para distribuição binomial:\n\nCada ensaio resulta em um de dois desfechos, mutuamente exclusivos, denominados, arbitrariamente, de sucesso e fracasso;\n\nA probabilidade de sucesso é fixa, igual a p, constante em cada ensaio, e a probabilidade de fracasso é igual a 1 – p;\nO número de repetições n em um ensaio é fixo.\n\nOs ensaios são independentes\n\nA distribuição binomial é na verdade uma família de distribuições, cujos membros são definidos pelos valores de n e p (parâmetros da distribuição binomial).\nA probabilidade de sucesso 4, em uma distribuição binomial, é dada pela fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nonde n = ensaios, x = sucessos, p = probabilidade de um sucesso e C representa o número possível de combinações em um ensaio.\nO número de combinações, C de x sucessos entre n repetições podem ser computado pela fórmula:\n\\[\nC = \\frac{n!}{x!(n - x)!}\n\\]\nou, no R, com a função choose (n, x).\nO modelo de distribuição binomial trata de encontrar a probabilidade de sucesso de um evento que tem apenas dois resultados possíveis em uma série de experimentos. Usando dados de uma distribuição binomial, é possível calcular os valores esperados de uma variável aleatória conforme ela passa por tentativas independentes. Em outras palavras, é possível prever o número exato de caras ou coroas que se deve esperar ao jogar uma moeda um certo número de vezes.\nTambém, pode-se usar a probabilidade binomial cumulativa para encontrar a probabilidade de obter um determinado intervalo de resultados. Por exemplo, saber a probabilidade do nascimento de até três meninos em 10 nascimentos consecutivos quando a probabilidade de nascer um menino é 0,50.\nO R tem quatro funções embutidas para gerar distribuição binomial. Ela são descritas a seguir.\n\n9.8.1 Funções da distribuição binomial\n\n9.8.1.1 Função pbinom()\nEsta função retorna o valor da função de densidade cumulativa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob).\nArgumentos:\n\nq \\(\\to\\) vetor de quantis\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, qual é a probabilidade de nascer até três meninos em cinco nascimentos, sabendo que a probabiliade de nascer um menino é igual a 0.50?\n\npbinom (3, 5, 0.50)\n\n[1] 0.8125\n\n\nIsso corresponde a soma das probabilidades de nascer nenhum menino, um menino, dois meninos e três meninos (Figura 9.15). Isto é calculado pela equação \\(P(X = x)\\), vista anteriormente.\nColocando no R:\n\nn = 5\np = 0.50\nx &lt;- 0:5\n# Probabilidades de meninos \nFx &lt;- (factorial(n)/(factorial(x)*factorial(n-x)))* p^x *(1-p)^(n-x)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n[1] 0.8125\n\n\n\n\n\n\n\n\nFigura 9.15: Distribuição binomial, mostrando a P (x &lt; 4) com n = 5 e p = 0.50\n\n\n\n\n\n\n\n9.8.1.2 Função qbinom()\nEsta função retorna o valor da função de densidade cumulativa inversa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). Com o uso desta função, podemos descobrir o quantil da distribuição binomial.\nArgumentos:\n\np \\(\\to\\) probabilidade ou vetor de probabilidades\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, quantos meninos nascerão em 5 partos com 81.25% de probabilidade cumulativa?\n\nqbinom (0.8125, size = 5, prob = 0.50)\n\n[1] 3\n\n\n\n\n9.8.1.3 Função rbinom()\nA função rbinom() permite extrair n observações aleatórias de uma distribuição binomial. Os argumentos da função são descritos abaixo:\nArgumentos:\n\nn \\(\\to\\) número de observações aleatórias a ser gerado\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nPara fazer uma simulação de 1000 amostras, aleatoriamente, de tamanho 5 e com probabilidade de nascer menino igual a 0,50, usa-se 5:\n\nset.seed(23)\nmenino &lt;- rbinom(n = 1000, size = 5, prob = 0.5)\n\nCada amostra de n = 5 exibe o número de meninos nascidos. Pode-se fazer a média que representa o valor esperado do número de sucessos (nascimento de menino, no exemplo) em um conjunto de ensaios independentes:\n\nmean(menino)\n\n[1] 2.515\n\n\nQuanto maior o número de variáveis aleatória criadas, mais próximo a média do número de sucessos estará do número esperado de sucessos que é igual ao número de sucessos vezes a probabilidade de sucesso em cada ensaio (5 x 0,50 = 2,5).\nEstranho, não é? Dois meninos e meio, em média por ensaio! É, a média é assim, uma estimativa, expectativa matemática! Não é real…\n\n\n9.8.1.4 Função dbinom()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição binomial dada uma determinada variável aleatória X, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). A função tem a seguinte sintaxe:\nArgumentos:\n\nx \\(\\to\\) vetor de números\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nA função é usada para encontrar a probabilidade de um determinado valor para dados que seguem a distribuição binomial, ou seja, encontra \\(P(X=x)\\), probabilidade de x sucessos em tentativas de tamanho (size) n quando a probabilidade (p) de sucesso é prob. Obtém o mesmo resultado da fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nPor exemplo, no nascimento de uma criança, as duas possibilidades, menino ou menina, são mutuamente excludentes e esses são os únicos eventos que podem acontecer. A probabilidade de nascimento de menino, como visto, é 0,50, qual seria a probabilidade de nascerem 4 meninos em 5 partos consecutivos não gemelares (Figura 9.16)?\n\ndbinom(4, size = 5, prob = 0.50)\n\n[1] 0.15625\n\n\nAs probabilidades de nascerem meninos em 5 nascimentos são:\n\nFx &lt;- dbinom(0:5, 5, 0.50)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n\n\n\n\n\n\nFigura 9.16: Distribuição binomial para P (x = 4) com n = 5 e p = 0,50\n\n\n\n\n\n\n\n\n9.8.2 Média e desvio padrão da distribuição binomial\nQuando o número de repetições é grande, geralmente há necessidade de resumir as probabilidades. A distribuição binomial pode ser descrita por sua média e variância.\nA média é o valor médio da variável aleatória em um longo número de repetições. É também chamada de valor esperado ou expectativa. A expectativa de uma variável aleatória X, geralmente, é denotada por \\(E(X)\\) e obtida pela multiplicação do número de ensaios independentes (n) pela probabilidade (p) de sucesso em cada ensaio:\n\\[\n\\mu = E(X) = n \\times p\n\\]\nPortanto, a expectativa (esperança) de nascimento de meninos em 5 partos é \\(E(X)=5 \\times 0,50 = 2,5\\), como visto na função rbinom(). Observe que o valor esperado de uma variável aleatória discreta não tem um valor que a variável aleatória pode realmente assumir.\nPor exemplo, para o número médio de meninos em um parto, ou não se tem menino ou se tem 1 menino, cada uma possibilidade com probabilidade de 0,50 e o valor esperado é (0 × 0,50) + (1 × 0,50) = 0,50. O número de meninos deve ser 0 ou 1, mas o valor esperado é a metade, a média que se obteria no longo prazo.\nA variância de uma variável aleatória discreta X é igual a\n\\[\n\\sigma^2=var(X) = n\\times p \\times (1-p)\n\\]\nConsequentemente, o desvio padrão é igual a\n\\[\n\\sigma = \\sqrt{var(X)} = \\sqrt{n\\times p \\times (1-p)}\n\\]\nPara o exemplo de 5 nascimentos, a média foi de 2,5 meninos e o desvio padrão\n\\[\n\\sigma =\\sqrt{5\\times 0.50 \\times (1-0.50)}=\\sqrt{2.5 \\times 0.50}= 1.12\n\\]\nPortanto, se espera que ocorram em média 2,5 (\\(\\sigma\\) = 1,12) nascimentos de meninos em 5 partos.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#distribuição-de-poisson",
    "href": "09-probabilidades.html#distribuição-de-poisson",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "9.9 Distribuição de Poisson",
    "text": "9.9 Distribuição de Poisson\nA distribuição de Poisson é utilizada para descrever a probabilidade do número de ocorrências em um intervalo contínuo (de tempo ou espaço). No caso da distribuição binomial, a variável de interesse é o número de sucessos em um intervalo discreto (n ensaios de Bernoulli).\nA unidade de medida (tempo ou espaço) é uma variável contínua, mas a variável aleatória, o número de ocorrências, é discreta. Esta distribuição segue as mesmas premissas da distribuição binomial:\n\nas tentativas são independentes;\na variável aleatória é o número de eventos em cada amostra;\na probabilidade é constante em cada intervalo\n\nEla é utilizada para modelar eventos discretos que ocorrem com pouca frequência no tempo ou espaço, por isso é algumas vezes denominada de distribuição de eventos raros. Pode-se usar a distribuição de Poisson como uma aproximação da distribuição Binomial quando n, o número de tentativas, for grande e p ou (1 – p) for pequeno (eventos raros).\nUm bom princípio básico é usar a distribuição de Poisson quando \\(n \\ge 20\\) e \\(n \\times p\\) ou \\(n \\times (1- p)\\) &lt; 5% (Fisher e Van Belle 1993). Nessas condições, a probabilidade que uma variável aleatória X adote um valor x é\n\\[\nP(X = x) = \\frac {e^{-\\lambda} \\times \\lambda^x}{x!}\n\\]\nonde \\(\\lambda\\) (lambda) representa o número de ocorrências de um evento em um intervalo de tempo e é conhecida como parâmetro da distribuição de Poisson e é igual em média a \\(n \\times p\\).\nNo R, essa probabilidade é dada pela função dpois(x, lambda).\nExemplo:\nSuponha que a probabilidade de uma puérpera ter infecção congênita (rubéola) seja igual a 0,0009. Qual seria a probabilidade, em uma população de 6000 gestantes, de que 5 estejam infectadas?\n\np &lt;- 0.0009\nx &lt;- 5\nn &lt;- 6000\nlambda &lt;- n * p\nP &lt;- dpois(x, lambda)\nround (P, 3)\n\n[1] 0.173\n\n\nPortanto, a probabilidade de se encontrar 5 mulheres com infecção congênita é de aproximadamente 17%.\n\n\n\n\n\n\nDebnath, Lokenath, e Kanadpriya Basu. 2015. «A short history of probability theory and its applications». International Journal of Mathematical Education in Science and Technology 46 (1). Taylor & Francis: 13–39.\n\n\nFisher, Lloyd D., e Gerald Van Belle. 1993. «Poisson Random Variables». Em Biostatistics: A Methodology for the Health Sciences, 211–18. New York, NY: John Wiley & Sons.\n\n\nGonzalez, José Carlos Soage. 2021. «Normal distribution in R». R CODER. https://r-coder.com/.\n\n\nJain, Sandeep. 2022. «A Guide to dnorm, pnorm, rnorm, and qnorm in R». GeeksforGeeks. https://www.geeksforgeeks.org/.\n\n\nMenezes, Renée Xavier de. 2004. «Introdução à Probabilidade». Em Métodos Quantitativos em Medicina, editado por Eduardo Massad, Renée Xavier de Menezes, Paulo Sérgio Panse Silveira, e Neli Regina Siqueira Ortega, 151–87. Barueri, São Paulo: Editora Manole Ltda.\n\n\nPagano, Marcello, e Gauvreau Kimberly. 2000. «Theoretical Probability Distributions». Em Principles of Biostatistics, Second Edition, 162. CRC Press.\n\n\nRobertson, Edmund, e John O’Connor. 2022. «Jacob (Jacques) Bernoulli». Maths History. School of Mathematics; Statistics, University of St Andrews. https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "09-probabilidades.html#footnotes",
    "href": "09-probabilidades.html#footnotes",
    "title": "9  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "Hospital Geral de Caxias do Sul, Hospital de Ensino da Universidade de Caxias do Sul, RS↩︎\nPara remover a notação científica, usar a função options (scipen = 999) e, para desfazer essa ação, trocar o 999 por 0.↩︎\nVeja Seção 4.4.1 como se constrói uma função. A função apresentada aqui é mais complexa..↩︎\nSucesso, aqui, não está no sentido de vitória, êxito, triunfo, glória e sim com a conotação de obter o desfecho esperado. Por exemplo, se uma moeda é lançada e se espera obter cara, sucesso significa um resultado igual a cara.↩︎\nDeve ser especificado uma “semente” (seed) antes de executar a função, senão será obtido um conjunto diferente de observações aleatórias a cada execução. Teste para verificar↩︎",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "10-assimetria.html",
    "href": "10-assimetria.html",
    "title": "10  Assimetria e Curtose",
    "section": "",
    "text": "10.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools,\n               dplyr, \n               e1071, \n               flextable,\n               ggplot2, \n               ggpubr, \n               grDevices, \n               moments, \n               readxl, \n               rstatix)",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "10-assimetria.html#dados-usados-neste-capítulo",
    "href": "10-assimetria.html#dados-usados-neste-capítulo",
    "title": "10  Assimetria e Curtose",
    "section": "10.2 Dados usados neste capítulo",
    "text": "10.2 Dados usados neste capítulo\nSerá usada a mesma variável altura de 1368 mulheres do conjunto de dadosdadosMater.xlsx, já mostrado anteriormente (Seção 9.4.1).\n\n10.2.1 Exploração dos dados\nO resumo dos dados pode ser realizado, usando a função summarise() do pacote dplyr. A moda será calculada usando com função Mode() do pacote DescTools.\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nresumo &lt;- dados %&gt;% \n  summarise(n = n(),\n            media = mean(altura, na.rm = TRUE),\n            dp = sd(altura, na.rm = TRUE),\n            mediana = median(altura, na.rm = TRUE),\n            moda = Mode(altura),\n            Q1 = quantile (altura, 0.25),\n            Q3 = quantile (altura, 0.75),\n            CV = dp/media)\nresumo\n\n# A tibble: 1 × 8\n      n media     dp mediana  moda    Q1    Q3     CV\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1368  1.60 0.0655     1.6   1.6  1.55  1.65 0.0410\n\n\nPara a exploração visual dos dados, será construído um histograma com um boxplot sobreposto (Figura 10.1). Para sobrepor o boxplot, foi usado um y = 350 que está acima da frequência máxima. Ajustamos a largura do boxplot com a função width = 13 (método dos acerts e erros). Para colocar o boxplot na horizontal não se usa a função cord_flip() (veja Seção 8.6.2). Aqui o “truque” é fixar o valor de y no boxplot e ajustar a altura com width(). Isto mantém o boxplot deitado no topo do histograma. É uma apresentação interessante para visualzar a simetria dos dados.\n\n# Estruturação do histograma\nhistograma &lt;- ggplot(dados, aes(x = altura)) +\n  geom_histogram(binwidth = 0.04, \n                 fill = \"lightblue\", \n                 color = \"black\")\n# Boxplot sobreposto\nhistograma +\n  geom_boxplot(aes(y = 350), width = 13, fill = \"lightblue\", color = \"black\") +\n  theme_classic(base_size = 13) + \n  scale_y_continuous(limits = c(0, NA))+\n  ylab(\"Frequência\")+\n  xlab(\"Altura (m)\")\n\n\n\n\n\n\n\nFigura 10.1: Histograma da altura das gestantes com boxplot sobreposto.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "10-assimetria.html#assimetria",
    "href": "10-assimetria.html#assimetria",
    "title": "10  Assimetria e Curtose",
    "section": "10.3 Assimetria",
    "text": "10.3 Assimetria\nA assimetria analisa a proximidade ou o afastamento de um conjunto de dados quantitativos em relação à distribuição normal. Mede o grau de afastamento de uma distribuição em relação a um eixo central (geralmente a média).\nQuando a curva é simétrica, a média, a mediana e a moda coincidem, num mesmo ponto, havendo um perfeito equilíbrio na distribuição. Quando o equilíbrio não acontece, isto é, a média, a mediana e a moda recaem em pontos diferentes da distribuição esta será assimétrica; enviesada a direita ou esquerda. podendo-se caracterizar como curvas assimétricas à direita ou à esquerda. Quando a distribuição é assimétrica à esquerda ou assimetria negativa, a cauda da curva localiza-se à esquerda, desviando a média para este lado (Figura 10.2). Na assimetria positiva, ocorre o contrário, a cauda está localizada à direita e da mesma forma a média (Peat e Barton 2014).\n\n\n\n\n\n\n\n\nFigura 10.2: Assimetria\n\n\n\n\n\n\n10.3.1 Avaliação da assimetria\nO R dispões de diversas maneiras para o cálculo do coeficiente de assimetria. O coeficiente de assimetria é um método numérico estatístico para medir a assimetria da distribuição ou conjunto de dados. Ele fala sobre a posição da maioria dos valores de dados na distribuição em torno do valor central.\n\n10.3.1.1 Cálculo do coeficiente de assimetria\nVárias medidas de coeficientes de assimetria amostrais foram propostas. O coeficiente de assimetria pode ser calculado no R, usando a função skewness() do pacote e1071 (Meyer et al. 2019). Esta função usa os seguintes argumentos:\n\nx \\(\\to\\) vetor numérico que contém os valores\nna.rm \\(\\to\\) um valor lógico que indica se os valores NA devem ser eliminados antes que o cálculo prossiga.\ntype \\(\\to\\) número inteiro entre 1 e 3 selecionando um dos algoritmos para calcular assimetria detalhados abaixo.\n\nOs três tipos são os seguintes:\n\nTipo 1, g1 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_1=\\frac{m_3}{m_2^\\frac{3}{2}}\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 3, tem-se:\n\\[\nm_3=\\frac{\\sum(x_i - \\overline{x})^3}{n}\n\\] Para r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m3 &lt;- (sum((dados$altura - (mean(dados$altura)))^3))/resumo$n\n m3\n\n[1] 5.081924e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g1 no R, chega-se ao resultado:\n\n g1 &lt;- m3/(m2)^(3/2)\n g1\n\n[1] 0.1812196\n\n\nUsando a função skewness() do pacote e1071, chega-se ao mesmo resultado:\n\ne1071::skewness(dados$altura, type = 1)\n\n[1] 0.1812196\n\n\n\nTipo 2, G1 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_1=\\frac{g_1 \\sqrt{n(n-1)}}{n-2}\n\\]\nColocando os dados na fórmula na linguagem do R, tem-se:\n\n G1 &lt;- (g1*sqrt((resumo$n*(resumo$n-1))))/(resumo$n-2)\n G1\n\n[1] 0.1814186\n\n\nCalculando com a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 2)\n\n[1] 0.1814186\n\n\n\nTipo 3, b1 \\(\\to\\) É o padrão da função skewness() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_1= \\frac {m_3}{s^3}\n\\]\nonde s é o desvio padrão da amostra. Na linguagem R, tem-se:\n\n b1 &lt;- m3/(resumo$dp)^3\n b1\n\n[1] 0.1810209\n\n\nUsando a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 3)\n\n[1] 0.1810209\n\n\nPara amostras grandes, há muito pouca diferença entre as várias medidas (Joanes e Gill 1998). Todas as três medidas de assimetria são imparciais sob normalidade.\nInterpretação do coeficiente de assimetria\nQuando a \\(assimetria = 0\\), tem-se uma distribuição simétrica e a média, a mediana e a moda coincidem; quando a \\({assimetria} &lt; {0}\\), \\({média} &lt; {mediana} &lt; {moda}\\), a distribuição tem assimetria negativa e quando a \\({assimetria} &gt; {0}\\), \\({média} &gt; {mediana} &gt; {moda}\\), a distribuição tem assimetria positiva.\nA Tabela 10.1 sugere uma forma de interpretar o coeficiente de assimetria (George e Mallery 2020).\n\n\n\n\nTabela 10.1: Interpretação do Coeficiente de Assimetria\n\n\n\nCoeficiente de assimetriaAssimetria-1 a +1leve-1 a -2 e +1 a +2moderada-2 a -3 e +2 a +3importante&lt; -3 ou &gt; +3grave\n\n\n\n\n\nObservando o formato da distribuição no histograma e no boxplot, na Figura 10.1, e no resultado do coeficiente de assimetria, conclui-se que a variável altura tem uma assimetria positiva leve, não preocupante. É possível aceitar essa variável como praticamente simétrica.\n\n\n10.3.1.2 Avaliação da assimetria com o gráfico QQ\nOutra ferramenta gráfica que permite avaliar a simetria dos dados é o gráfico QQ (gráfico quantil-quantil). Ele permite observar se a distribuição se ajusta a distribuição normal. O gráfico QQ é um gráfico de dispersão que compara os quantis 1 da amostra com os quantis teóricos de uma distribuição de referência. Se os pontos do gráfico QQ formarem uma reta, isso indica que os dados têm a mesma distribuição da referência. Se os pontos se afastarem da reta, isso indica que os dados têm uma distribuição diferente da referência. Para construir um gráfico QQ, pode-se usar a função ggqqplot()do pacote ggpubr. Ele apresenta uma linha de referência, acompanhada de uma area sombreada, correspondente ao Intervalo de Confiança de 95% (veja o Capítulo 12):\n\nggqqplot(data = dados, \n         x = \"altura\",\n         conf.int = TRUE,\n         shape = 19,\n         xlab = \"Quantis teóricos\",\n         ylab = \"Altura (m)\",\n         color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 10.3: Gráfico QQ\n\n\n\n\n\nA Figura 10.3 exibe uma reta com IC95% que praticamente se sobrepõe aos pontos. É mais uma informação mostrando que os dados têm uma distribuição simétrica aceitável.\n\n\n10.3.1.3 Pesquisa de valores atípicos\nOs valores atípicos atraem as caudas da dispersão aumentando a possibilidade de assimetria. No boxplot da Figura 10.1, verifica-se a presença de outliers que devem ser avaliados.\nPara examinar os outliers, as estatísticas do boxplot são úteis, pois mostram a quantidade e os respectivos valores. A função boxplot.stats() do pacote grDevices, entregam as estatísticas dos 5 números (min, P25, mediana, P75 e max), o total de observações, o limite inferior e superior do intervalo de confiança de 95% e os valores atípicos (outliers)::\n\nboxplot.stats(dados$altura)\n\n$stats\n[1] 1.42 1.55 1.60 1.65 1.78\n\n$n\n[1] 1368\n\n$conf\n[1] 1.595728 1.604272\n\n$out\n[1] 1.40 1.82 1.80 1.40 1.40 1.85 1.80\n\n\nOutra maneira de identificar os outliers é através da função indentify_outliers() do pacote rstatix:\n\n dados %&gt;% \n   rstatix::identify_outliers(altura)\n\n# A tibble: 7 × 3\n  altura is.outlier is.extreme\n   &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1   1.4  TRUE       FALSE     \n2   1.82 TRUE       FALSE     \n3   1.8  TRUE       FALSE     \n4   1.4  TRUE       FALSE     \n5   1.4  TRUE       FALSE     \n6   1.85 TRUE       FALSE     \n7   1.8  TRUE       FALSE     \n\n\nAmbas as funções identificaram 7 valores atípicos (acima ou abaixo 1,5 vezes o intervalo interquartil), mas, como mostra a função identify_outliers, eles exercem pouca influência, pois não são extremos, ou seja, acima de três vezes o intervalo interquartil.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "10-assimetria.html#curtose",
    "href": "10-assimetria.html#curtose",
    "title": "10  Assimetria e Curtose",
    "section": "10.4 Curtose",
    "text": "10.4 Curtose\nÉ o grau de achatamento de uma distribuição, em relação a distribuição normal. A curtose indica como o pico e as caudas de uma distribuição diferem da distribuição normal. A assimetria mede essencialmente a simetria da distribuição, enquanto a curtose determina o peso das caudas da distribuição. Portanto, é uma medida dos tamanhos combinados das duas caudas; mede a quantidade de probabilidade nas caudas. A curtose pode ser de três tipos (Figura 10.4):\n\nMesocúrtica \\(\\to\\) quando a distribuição é normal;\nLeptocúrtica \\(\\to\\) quando a distribuição é mais pontiaguda e concentrada que a normal, mostrando caudas pesadas em ambos os lados;\nPlaticúrtica \\(\\to\\) quando a distribuição é mais achatada e dispersa que a normal, com caudas planas.\n\nUma curtose em excesso é uma medida que compara a curtose de uma distribuição com a curtose de uma distribuição normal. A curtose de uma distribuição normal é igual a 3. Portanto, o excesso de curtose é determinado subtraindo 3 da curtose:\n\\[\nExcesso \\space de \\space curtose = curtose - 3\n\\]\nA distribuição normal tem uma curtose de zero e é chamada de mesocúrtica. Uma distribuição com curtose maior que zero (ou três) é mais alta e concentrada que a normal, mostrando caudas pesadas em ambos os lados, e é chamada de leptocúrtica. Uma distribuição com curtose menor que zero é mais achatada e dispersa que a normal, com caudas planas, e é chamada de platicúrtica.\nOs dados que seguem uma distribuição mesocúrtica mostram um excesso de curtose de zero ou próximo de zero. Isso significa que se os dados seguem uma distribuição normal, eles seguem uma distribuição mesocúrtica. A distribuição leptocúrtica mostra caudas pesadas em ambos os lados, indicando grandes valores discrepantes. Uma distribuição leptocúrtica manifesta uma curtose excessiva positiva. Uma distribuição platicúrtica mostra uma curtose excessiva negativa, revela uma distribuição com cauda plana.\n\n\n\n\n\n\n\n\nFigura 10.4: Curtose\n\n\n\n\n\n\n10.4.1 Avaliação da curtose\n\n10.4.1.1 Cálculo do coeficiente de curtose\nO coeficiente de curtose pode ser calculado no R usando a função kurtosis() do pacote e1071. Esta função usa os mesmos argumentos da função skewness(), vista acima. Calcula três tipos de coeficientes:\n\nTipo 1, g2 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_2=\\frac{m_4}{m_2^2} - 3\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 4, tem-se:\n\\[\nm_4=\\frac{\\sum(x_i - \\overline{x})^4}{n}\n\\]\nPara r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m4 &lt;- (sum((dados$altura - (mean(dados$altura)))^4))/resumo$n\n m4\n\n[1] 5.734699e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g2 no R, chega-se ao resultado:\n\ng2 &lt;- (m4/(m2)^2)-3\ng2\n\n[1] 0.1242567\n\n\nUsando a função do pacote e1071, chega-se ao mesmo resultado:\n\n e1071::kurtosis(dados$altura, type = 1)\n\n[1] 0.1242567\n\n\n\nTipo 2, G2 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_2=\\left (\\left (n + 1 \\right )g_2 + 6 \\right )\\frac{\\left (n - 1 \\right)}{\\left ( \\left(n-2 \\right)\\left (n-3 \\right) \\right )}\n\\]\nColocando os dados na fórmula na linguagem do R, tem-se:\n\n G2 &lt;- ((resumo$n+1)*g2 + 6)*(resumo$n-1)/((resumo$n-2)*(resumo$n-3))\n G2\n\n[1] 0.1291109\n\n\nCom a função kurtosis() do pacote e1071:\n\n e1071::kurtosis(dados$altura, type = 2)\n\n[1] 0.1291109\n\n\n\nTipo 3, b2 \\(\\to\\) É o padrão da função kurtosis() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_2=\\frac{m_4}{s^4}-3\n\\] onde s é o desvio padrão da amostra.\nNa linguagem R, tem-se:\n\n b2 &lt;- m4/(resumo$dp)^4 - 3\n b2\n\n[1] 0.1196907\n\n\nCom a função kurtosis():\n\ne1071::kurtosis(dados$altura, type = 3)\n\n[1] 0.1196907\n\n\nNovamente, para amostras grandes, há muito pouca diferença entre as várias medidas, principalmente entre G2 e b2 (Joanes e Gill 1998).\n\n\n10.4.1.2 Interpretação do coeficiente de curtose\nOs coeficientes calculados pela função do pacote e1071 retornam um resultado equivalente ao excesso de curtose. A curva normal tem um excesso de curtose próximo a zero e a curva é dita mesocúrtica. Se o coeficiente for positivo, os dados são leptocúrticos e se for negativo, os dados são platicúrticos. O resultado do exemplo aponta para uma distribuição leptocúrtica, pois existe um pequeno excesso de curtose (g2 = 0.1242567). Os valores que contribuem para a curtose são aqueles fora da região do pico, ou seja, ou outliers. A curva mesocúrtica tem um coeficiente de 3. Portanto, os valores calculados anteriormente referem-se ao excesso de curtose. O resultado da g2 = 0,1242567 pode ser escrito como b2 = 3,1242567. Daí o termo excesso de curtose.\nA função kurtosis() do pacote moments retorna um resultado ao redor de 3, para o coeficiente tipo 1. Para chegar ao mesmo resultado do coeficiente tipo 1 da função do pacote e1071, deve-se subtrair 3 do resultado.\n\nmoments::kurtosis(dados$altura)\n\n[1] 3.124257\n\n\n\n\n\n\n\n\nExercício 1\n\n\n\nCriar um conjunto de dados com distribuição normal com média 0 e desvio padrão 1 e n = 10000 . Verifique a assimetria e a curtose deste conjunto\n\n\nResposta:\n\nConjunto de dados: meusDados\n\n\nset.seed(1234)\nmeusDados &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\nConstrua um histograma (Figura 10.5) com curva normal sobreposta:\n\n\n\n\n\n\n\n\n\nFigura 10.5: Histograma com curva normal\n\n\n\n\n\n\nObserve a skewness e a kurtosis\n\n\ne1071::skewness(meusDados)\n\n[1] 0.008609517\n\ne1071::kurtosis(meusDados)\n\n[1] -0.003450388\n\n\nComo era de se esperar, usando a rnorm(), a distribuição é um exemplo de distribuição normal, \\(skewness \\approx 0\\) e \\(kurtosis \\approx 0\\). Observe que a cada vez que os comandos forem executados, os resultados serão discretamente diferentes. Para evitar isso, deve-se usar set.seed(), veja a seção Seção 9.7.2. Faça o teste!\n\n\n\n\n\n\nGeorge, Darren, e Paul Mallery. 2020. «Descriptive Statistics». Em IBM SPSS Statistics 26 Step by Step: A Simple Guide and Reference, 114–20. New York, NY: Taylor & Francis Group.\n\n\nJoanes, DN, e CA Gill. 1998. «Comparing Measures of Sample Skewness and Kurtosis». Journal of the Royal Statistical Society 47 (1): 183–89.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, Friedrich Leisch, Chih-Chung Chang, Chih-Chen Lin, e Maintainer David Meyer. 2019. «Package “e1071”». The R Journal, 1–67.\n\n\nPeat, Jennifer, e Belinda Barton. 2014. «Descriptive statistics». Em Medical statistics : a guide to SPSS, data analysis, and critical appraisal, 24–51. New York, NY: John Wiley & Sons.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "10-assimetria.html#footnotes",
    "href": "10-assimetria.html#footnotes",
    "title": "10  Assimetria e Curtose",
    "section": "",
    "text": "Sobre os quantis, veja na Seção 6.4.4.↩︎",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html",
    "href": "11-distAmostrais.html",
    "title": "11  Distribuições Amostrais",
    "section": "",
    "text": "11.1 Pacotes necessários para este capítulo\npacman::p_load(dplyr, \n               e1071, \n               ggplot2, \n               knitr, \n               readxl)",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#distribuições-populacional-e-amostral",
    "href": "11-distAmostrais.html#distribuições-populacional-e-amostral",
    "title": "11  Distribuições Amostrais",
    "section": "11.2 Distribuições populacional e amostral",
    "text": "11.2 Distribuições populacional e amostral\nMétricas como a média, a mediana e o desvio padrão são medidas numéricas de resumo. Quando calculadas a partir de dados de uma amostra são denominadas estatísticas amostrais. Por outro lado, as mesmas medidas numéricas de resumo calculadas para dados populacionais são chamadas de parâmetros populacionais (Oliveira Filho 2022).\nUm parâmetro populacional é sempre uma constante, enquanto uma estatística de amostra é sempre uma variável aleatória. Como cada variável aleatória deve possuir uma distribuição de probabilidade, cada estatística de amostra possui uma distribuição de probabilidade. A distribuição de probabilidade de uma estatística de amostra é mais comumente chamada de distribuição amostral. Os conceitos abordados neste capítulo são a base da estatística inferencial (Zar 2014).\n\n11.2.1 Distribuição populacional\nA distribuição populacional é a distribuição de probabilidade derivada das informações sobre todos os elementos de uma população.\nPara fins de raciocínio didático, o conjunto de dados de 1368 observações de puérperas e recém-nascidos da Maternidade-escola do Hospital Geral de Caxias do Sul, RS, será considerado uma população. O gráfico da Figura 10.1, da Seção 10.2.1, mostra a distribuição da altura das puérperas dessa ‘população’. Os parâmetros (\\(\\mu\\) e \\(\\sigma\\)) dessa “população” são:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\n media = mean(dados$altura, na.rm =TRUE)\n round(media, 3)\n\n[1] 1.598\n\n dp = sd(dados$altura, na.rm =TRUE)\n round(dp, 3)\n\n[1] 0.065\n\n\n\n\n11.2.2 Distribuição amostral\nConforme mencionado no início deste capítulo, o valor de um parâmetro da população é sempre constante. Por exemplo, para qualquer conjunto de dados populacionais, há apenas um valor para a média populacional, \\(\\mu\\).\nNo entanto, não se pode dizer o mesmo sobre a média amostral. Amostras diferentes do mesmo tamanho, retiradas da mesma população, produzem valores diferentes da média amostral, \\(\\bar{x}\\). O valor da média amostral, para qualquer amostra, dependerá dos elementos incluídos nessa amostra. Em decorrência, a média amostral é uma variável aleatória. Portanto, como outras variáveis aleatórias, a média amostral possui uma distribuição de probabilidade, que é mais comumente chamada de distribuição amostral da média (Callegari-Jacques 2003).\nOutras estatísticas de amostra, como mediana, moda e desvio padrão, também possuem distribuições amostrais. Em geral, a distribuição de probabilidades de uma amostra é denominada de distribuição amostral.\nUsar a variável altura das puérperas da Maternidade do HGCS como a população de interesse é apenas uma estratégia didática. Raramente, na vida real, é possível obter dados da população inteira. Reunir essa informação costuma ser muito custoso ou impossível. Por essa razão, a prática é selecionar apenas uma amostra da população e a usar para compreender as suas características.\nA função slice_sample() do pacote dplyrextrairá uma amostra 1 de n = 30 da população. As funções mean() e sd() calcularão a média e o desvio padrão, repectivamente:\n\nset.seed(234)\namostra1 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia1 &lt;- mean(amostra1$altura, na.rm =TRUE)\ndp1 &lt;-  sd(amostra1$altura, na.rm =TRUE)\nprint(c(media1, dp1))\n\n[1] 1.59266667 0.06073875\n\n\nSe este processo for repetido várias vezes, a cada amostra aleatória 2, serão gerados médias e desvios padrão diferentes.\n\nset.seed(236)\namostra2 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia2 &lt;- mean(amostra2$altura, na.rm =TRUE)\ndp2 &lt;-  sd(amostra2$altura, na.rm =TRUE)\nprint(c(media2, dp2))\n\n[1] 1.60633333 0.06960397\n\n\nÀ medida que o número de amostras possíveis forem aumentando, elas constituem uma distribuição cuja média, média das médias, \\(\\bar{x}_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\). Essa distribuição, no caso da média, recebe o nome de distribuição amostral das médias.\nAgora, para exemplificar este conceito, serão geradas 5000 amostras e calculada a média de cada uma das amostras de n = 30 que constituirão a distribuição, mostrada no gráfico da Figura 11.1.\n\n# extraindo 5000 amostras\namostras5000 &lt;- rep (0, 5000)\nfor (i in 1:5000) {\n  amostra &lt;- dados %&gt;% dplyr::slice_sample (n = 30) \n  amostras5000 [i] &lt;- mean(amostra$altura)\n}\n\nMedia e desvio padrão das 5000 amostras:\n\nmu &lt;- round (mean (amostras5000), digits = 3)\nsigma &lt;- round (sd (amostras5000), digits = 3)\nprint(c(mu, sigma))\n\n[1] 1.598 0.012\n\n\n\n\n\n\n\n\n\n\nFigura 11.1: Distribuição amostral das médias de 5000 amostras de n = 30\n\n\n\n\n\nSe a média, \\(\\bar{x}_{\\bar{x}}\\), dessas 5000 amostras de n = 30, for comparada com a média populacional, \\(\\mu\\), observa-se que até 3 dígitos decimais não há uma diferença. Entretanto, o desvio padrão é bem menor (0.012) que o da população (0.065).",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "href": "11-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "title": "11  Distribuições Amostrais",
    "section": "11.3 Erros amostrais e não amostrais",
    "text": "11.3 Erros amostrais e não amostrais\nAmostras diferentes selecionadas da mesma população darão resultados diferentes porque contêm elementos diferentes. Isso é evidente nas medias das amostra1 e amostra2, 1.593m e 1.606m, respectivamente, comparadas com a média da população igual a 1.598m .\n\nerro1 &lt;- abs(mean(amostra1$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nerro2 &lt;- abs(mean(amostra2$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nprint(c(erro1, erro2), digits = 2)\n\n[1] 0.0053 0.0084\n\n\nSe outras amostras forem extraídas, o resultado obtido de qualquer amostra geralmente será diferente do resultado obtido da população correspondente. A diferença entre o valor de uma estatística amostral obtida de uma amostra e o valor do parâmetro populacional correspondente, é chamado de erro amostral. Observe que essa diferença representa o erro amostral apenas se a amostra for aleatória e não houver nenhum erro não amostral. Caso contrário, apenas uma parte dessa diferença será devido ao erro amostral.\n\\[\nerro \\quad amostral = \\bar{x}_{i} - \\mu  \n\\]\nÉ importante lembrar que o erro amostral ocorre devido ao acaso. Não é possível evitar o erro amostral. É possível limitar o seu valor através da seleção de uma amostra adequada. Os erros que ocorrem por outros motivos, como erros cometidos durante a coleta, registro e tabulação dos dados, são chamados de erros não amostrais. Esses erros ocorrem, em geral, por causa de erros humanos e não por acaso.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#média-e-desvio-padrão-da-média",
    "href": "11-distAmostrais.html#média-e-desvio-padrão-da-média",
    "title": "11  Distribuições Amostrais",
    "section": "11.4 Média e desvio padrão da média",
    "text": "11.4 Média e desvio padrão da média\nA média e o desvio padrão calculados para a distribuição amostral da média são chamados de média (\\(\\mu_{\\bar{x}}\\)) e desvio padrão (\\(\\sigma_{\\bar{x}}\\)) da média. Na verdade, a média e o desvio padrão da média são, respectivamente, a média e o desvio padrão das médias de todas as amostras do mesmo tamanho selecionadas de uma população. O desvio padrão da média é, comumente, chamado de erro padrão da média (\\(\\sigma_{\\bar{x}}\\)).\nA média amostral, \\(\\bar{x}\\), é chamada de estimador da média da população, \\(\\mu\\). Quando o valor esperado (ou média) de uma estatística amostral é igual ao valor do parâmetro populacional correspondente, essa estatística amostral é considerada um estimador não enviesado, consistente.\nPara a média amostral \\(\\bar{x}\\), \\(\\mu_{\\bar{x}} = \\mu\\). Logo, \\(\\bar{x}\\), é um estimador imparcial de \\(\\mu\\). Esta é uma propriedade muito importante que um estimador deve possuir. No entanto, o desvio padrão da média, \\(\\sigma_{\\bar{x}}\\), não é igual ao desvio padrão, \\(\\sigma\\), da distribuição populacional (a menos que n = 1). O desvio padrão da média amostral é igual ao desvio padrão da população dividido pela raiz quadrada do tamanho amostral:\n\\[\n\\sigma_{\\bar{x}} = \\frac {\\sigma}{\\sqrt{n}}\n\\]\nA dispersão da distribuição amostral da média é menor do que dispersão da distribuição populacional correspondente, como mostrado acima. Em outras palavras, \\(\\sigma_{\\bar{x}} &lt; \\sigma\\). Isso é visível na fórmula do \\(\\sigma_{\\bar{x}}\\) . Quando n é maior que 1, o que geralmente é verdadeiro, o denominador em \\(\\frac {\\sigma}{\\sqrt{n}}\\) é maior que 1. Desta forma, \\(\\sigma_{\\bar{x}}\\) é menor que \\(\\sigma\\). O desvio padrão da distribuição amostral da média diminui à medida que o tamanho amostral aumenta.\nSempre que o n for grande, em geral &gt; 30 (Pagano e Gavreau 2000), pode ser assumido que a distribuição será uma curva normal e que o desvio padrão da amostra (s) é um estimador não enviesado do desvio padrão populacional (\\(\\sigma\\)). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\):\n\\[\nEP_{\\bar{x}} = \\frac {s}{\\sqrt{n}}\n\\]",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#sec-tcl",
    "href": "11-distAmostrais.html#sec-tcl",
    "title": "11  Distribuições Amostrais",
    "section": "11.5 Teorema do Limite Central (TCL)",
    "text": "11.5 Teorema do Limite Central (TCL)\nNa maioria das vezes, a população da qual as amostras são extraídas não é normalmente distribuída. Em tais casos, a forma da distribuição amostral de X é inferida de um teorema muito importante chamado teorema do limite central. De acordo com este teorema para um grande tamanho de amostra (em geral, acima de 30), a distribuição amostral da média é aproximadamente normal, independentemente da forma da distribuição da população (Pagano e Gavreau 2000). Esta aproximação tornar-se-á mais acurada à medida que aumenta o tamanho amostral:\n\na média da distribuição amostral, \\(\\mu_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\);\ndesvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}\\), é igual a \\(\\frac {\\sigma}{\\sqrt{n}}\\);\no erro padrão da média, \\(\\sigma_{\\bar{x}}\\), é sempre menor (Figura 11.2) que o desvio padrão populacional, \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\nFigura 11.2: Erro padrão versus desvio padrão\n\n\n\n\n\n\n11.5.1 Exemplo com uma variável assimétrica\nComo exemplo, será explorada a variável renda, do conjunto de dados dadosMater.xlsx, que representa a renda familiar em salários mínimos (sm). Como foi feito anteriormente, suponha que essa variável seja a “população” de estudo. Ela tem as seguintes medidas resumidoras e de assimetria:\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\")\nresumo &lt;- dados %&gt;% \n  select (renda) %&gt;% \n  dplyr::summarise (media.sm = mean (dados$renda, na.rm = TRUE),\n                    dp.sm = sd(dados$renda, na.rm = TRUE),\n                    mediana.sm = median(dados$renda, na.rm = TRUE),\n                    assimetria = e1071::skewness(dados$renda),\n                    curtose = e1071::kurtosis(dados$renda))\nresumo\n\n# A tibble: 1 × 5\n  media.sm dp.sm mediana.sm assimetria curtose\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1     2.22  1.23       1.92       2.22    8.21\n\n\nO desvio padrão é grande em relação à média, com um coeficiente de variação de 55.1% e uma mediana &lt; média. Estas métricas junto com os coeficientes de assimetria e curtose apontam para a assimetria positiva da variável renda. O gráfico da Figura 11.3 confirma esta afirmação:\n\n\n\n\n\n\n\n\nFigura 11.3: Distribuição assimétrica positiva\n\n\n\n\n\nOs valores da média e do desvio padrão calculados para a distribuição de probabilidade dessa população fornecem os valores dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Esses valores são \\(\\mu\\) =2.22 sm 3 e \\(\\sigma\\) =1.23 sm.\nSe extrairmos múltiplas amostras dessa população, observa-se a modificação do formato da distribuição à medida que aumenta o tamanho amostral, se aproximando progressivamente do modelo normal, com um número grande de amostras.\nExtração de múltiplas amostras(1000)\n\namostras1000 &lt;- rep (0, 1000)\nfor (i in 1:1000) {\n  amostra.sm &lt;- sample (dados$renda, 30) \n  amostras1000 [i] &lt;- mean(amostra.sm)\n}\n\nMedia e desvio padrão das 1000 amostras\n\nmu &lt;- round (mean (amostras1000), digits = 3)\nsigma &lt;- round (sd (amostras1000), digits = 3)\nmd &lt;- round (median(amostras1000), digits = 3)\nprint(c(mu, sigma, md))\n\n[1] 2.231 0.236 2.224\n\n\nAssimetria e curtose\n\nb1 &lt;- e1071::skewness(amostras1000)\nb2 &lt;- e1071::kurtosis(amostras1000)\nprint(c(b1, b2))\n\n[1] 0.4357889 0.2241975\n\n\n\n\n\n\n\n\n\n\nFigura 11.4: Distribuição praticamente normal\n\n\n\n\n\nOu seja, extraindo-se 1000 amostras de n = 30 e calculando as mesmas métricas anteriores, observa-se que, embora a distribuição populacional original seja assimétrica, a distribuição amostral da média se aproxima bastante da distribuição gaussiana (Figura 11.4).\n\n\n\n\n\n\nImportância do Teorema do Limite Central\n\n\n\nO TCL é um dos pilares da inferência estatística (Burattini 2004). Entender a sua importância é como destrancar a porta para aplicar testes, construir intervalos de confiança e fazer previsões com segurança, mesmo quando os dados parecem confusos.\nO TCL afirma que:\n\nA distribuição das médias amostrais tende a ser normal, mesmo que a população original não seja, desde que o tamanho da amostra seja suficientemente grande.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#sec-popamostra",
    "href": "11-distAmostrais.html#sec-popamostra",
    "title": "11  Distribuições Amostrais",
    "section": "11.6 Proporções populacional e amostral",
    "text": "11.6 Proporções populacional e amostral\nO conceito de proporção é o mesmo que o conceito de frequência relativa e o conceito de probabilidade de sucesso em um experimento binomial, discutidos anteriormente, na distribuição binomial.\nA frequência relativa de uma categoria ou classe dá a proporção da amostra ou população que pertence a essa categoria ou classe. Da mesma forma, a probabilidade de sucesso em um experimento binomial representa a proporção da amostra ou população que possui uma determinada característica.\nA proporção populacional, representada por p, é obtida considerando a razão entre o número de elementos em uma população com uma característica específica e o número total de elementos na população. A proporção amostral, denotada por \\(\\hat{p}\\) (pronuncia-se p-chapéu), fornece uma proporção semelhante para uma amostra.\n\\[\np = \\frac{X}{N} \\quad e \\quad \\hat{p}= \\frac{x}{n}\n\\] onde,\n\nN \\(\\to\\) número total de elementos em uma população\nn \\(\\to\\) número total de elementos em uma amostra\nX \\(\\to\\) número de elementos na população que possui determinada característica\nx \\(\\to\\) número de elementos na amostra que possui determinada característica\n\nComo no caso da média, a diferença entre a proporção amostral e a proporção populacional correspondente, determina o erro amostral, assumindo que a amostra é aleatória e nenhum erro não amostral foi cometido. Ou seja,\n\\[\nerro \\quad amostral = \\hat{p} - p\n\\]\nA distribuição amostral de uma proporção é a distribuição das proporções de todas as amostras possíveis de tamanho n retiradas de uma população.\nDe acordo com o Teorema Central do Limite, para amostras suficientemente grandes, a distribuição de \\(\\hat{p}\\) se aproxima de uma distribuição normal, mesmo que os dados originais sejam binomiais (sucesso/fracasso), desde que: \\(np \\geq 5\\) e \\(n(1 - p) \\geq 5\\).\nAssim,\n\\[E(\\hat{p})=\\mu_\\hat{p}\\]\n\\[Var(\\hat{p})=\\sigma^2_\\hat{p}=\\frac{\\hat{p}(1-\\hat{p})}{n}\\] Logo,\n\\[E(\\hat{p})=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nDessa forma, a distribuição amostral de \\(\\hat{p}\\) será:\n\\[\n\\hat{p} \\sim N(\\hat{p}, \\frac{\\hat{p}(1-\\hat{p})}{n})\n\\]\nQuando não conhecemos a proporção populacional p, pode-se usar \\(\\hat{p}\\) como estimativa dessa proporção, desde que as condições acima sejam satisfeitas.\nDessa forma, pode-se calcular probabilidades aproximadas por uma distribuição normal com média \\(μ = n \\times p\\) e \\(σ = \\sqrt{(n×p(1-p))}\\) (veja também Seção 9.8.2).\nConsiderando a “população”, usada neste capítulo, o conjunto de dados dadosMater.xlsx, será verificado a proporção de mulheres fumantes. Inicialmente, a variável fumo, que está como variável numérica, será transformada em fator:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(fumo) %&gt;% \n  mutate(fumo = factor(fumo, \n                       levels = c(1,2), \n                       labels = c(\"Fumante\", \"Não fumante\")))\n\nA proporção de fumantes, frequência relativa (fr) é:\n\nfumo &lt;- with(dados, table(fumo))\nfr.fumo &lt;- prop.table(fumo)\nfr.fumo\n\nfumo\n    Fumante Não fumante \n  0.2200292   0.7799708 \n\n\nA saída retorna que a proporção de fumantes entre as mulheres desse arquivo é 0.22. Esta será considerada a proporção p da ‘população’. Agora, imagine que esse resultado fosse desconhecido. Então, para saber a qual a proporção de fumantes dessa ‘população’, seria necessário extrair uma amostra adequada. Foi selecionada uma amostra de n = 100 da ‘população’ alvo:\n\n set.seed(134)\n amostra.fumo &lt;- dados %&gt;% dplyr::slice_sample(n = 100)\n\nUsando a amostra.fumo, calcula-se a proporção de fumantes:\n\n tabagismo &lt;- with(amostra.fumo, table(fumo))\n fr &lt;- prop.table(tabagismo)\n fp &lt;- fr*100\n\n tab.fumo &lt;- cbind(n = tabagismo,\n                   fr = round(fr, 2),\n                   fp = round(fp, 2))\n tab.fumo\n\n             n  fr fp\nFumante     20 0.2 20\nNão fumante 80 0.8 80\n\n\nA proporção de uma amostra é uma variável aleatória: varia de amostra para amostra de uma forma que não pode ser prevista com certeza. O Teorema Central do Limite se aplica em proporções. À medida que novas amostras forem extraídas, o valor da proporção amostral \\(\\hat{p}\\) se aproxima da proporção populacional p. Na “população” p = 0,22; na amostra de n = 100, \\(\\hat{p}\\) = 0.2. Para amostras grandes, a proporção amostral tem distribuição aproximadamente normal com as seguinte características mencionadas acima em relação a \\(\\mu_\\hat{p}\\) e \\(\\sigma_\\hat{p}\\).\nComo verificar se uma amostra é grande?\nUma amostra é grande se o intervalo\n\\[\n[\\hat{p}-3 \\times \\sigma_\\hat{p} , \\quad \\hat{p}-3 \\times \\sigma_\\hat{p}]\n\\]\nestiver totalmente dentro do intervalo [0,1].\nNa prática, p não é conhecido, portanto, \\(\\sigma_\\hat{p}\\) também não é. Nesse caso, para verificar se a amostra é suficientemente grande, substitui-se o valor de p pelo valor conhecido de \\(\\hat{p}\\). Isso significa verificar se o intervalo\n\\[\n\\hat{p}-3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\\quad \\hat{p}+3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nencontra-se totalmente dentro do intervalo [0,1].\nTransportando os dados da amostra de gestantes, para a fórmula e usando o R para o cálculo, tem-se:\n\np.chapeu &lt;- tab.fumo[1,2]\nn &lt;- tab.fumo[1,1] + tab.fumo[2,1]\n\nli &lt;- p.chapeu - 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nls &lt;- p.chapeu + 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nprint(c(li, ls), digits = 3)\n\n[1] 0.08 0.32\n\n\nComo os limites ficam no intervalo [0, 1], chega-se à conclusão de que a amostra de n = 100 é aceitável para estimar a proporção populacional.\n\n\n\n\n\n\nExercício\n\n\n\nUma amostra com tamanho n = 40 é suficiente?\n\n\nResposta:\n\nn = 40\nli &lt;- p.chapeu - 3*sqrt((p.chapeu*(1-p.chapeu))/n) \nls &lt;- p.chapeu + 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nprint(c(li, ls), digits = 3)\n\n[1] 0.0103 0.3897\n\n\nSim, é aceitável uma amostra de n = 40.\n\n\n\n\n\n\nBurattini, Marcelo N. 2004. «Raciocínio médico e inferência». Em Métodos Quantitativos em Medicina, editado por Eduardo Massad, Paulo SP Silveira, Renèe X de Menezes, e Others, 208–23. São Paulo, SP: Editora Manole.\n\n\nCallegari-Jacques, Sidia M. 2003. «Distribuição amostral das médias». Em Bioestatistica: principios e aplicações, 47–53. Artmed Editora.\n\n\nOliveira Filho, Petronio Fagundes de. 2022. «População e Amostra». Em Epidemiologia e Bioestatística: Fundamentos para a leitura crítica, Segunda Edição, 109–12. Rio de Janeiro: Editora Rubio.\n\n\nPagano, Marcello, e Kimberlee Gavreau. 2000. «The Central Limit Theorem». Em Principles of Biostatistics, Second Edition, 197–98. Pacific Grove, CA: Duxbury.\n\n\nZar, Jerrold H. 2014. «Populations and Samples». Em Biostatistical Analysis, 18–22. Edinburgh: Pearson.",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "11-distAmostrais.html#footnotes",
    "href": "11-distAmostrais.html#footnotes",
    "title": "11  Distribuições Amostrais",
    "section": "",
    "text": "Para que a cada nova amostragem retorne o mesmo conjunto de dados, é usado a função set.seed()(veja Seção 9.7.2.4)↩︎\nObserve que o número da “semente” foi modificado para 236. Poderia ser qualquer outro número, isto garante que o sorteio seja diferente, mas ainda reprodutível.↩︎\nSalários mínimos↩︎",
    "crumbs": [
      "Parte IV - Probabilidades e Distribuições",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html",
    "href": "12-estimacao.html",
    "title": "12  Estimação",
    "section": "",
    "text": "12.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools, \n               dplyr,\n               ggplot2, \n               flextable,\n               knitr,\n               readxl, \n               Rmisc, \n               tidyr)",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#sec-dadoscap12",
    "href": "12-estimacao.html#sec-dadoscap12",
    "title": "12  Estimação",
    "section": "12.2 Dados usados neste capítulo",
    "text": "12.2 Dados usados neste capítulo\nOs dados deste capítulo são provenientes do conjunto de dados dadosMater.xlsx (Seção 5.6), considerando apenas as variáveis altura e pesoRN:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura, fumo, ig, pesoRN) %&gt;% \n  mutate(fumo = factor(fumo, \n                       levels = c(1,2), \n                       labels = c(\"Fumante\", \"Não fumante\")))\n\ndadosRNT &lt;- dados %&gt;% \n  filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(pesoRN)",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#introdução",
    "href": "12-estimacao.html#introdução",
    "title": "12  Estimação",
    "section": "12.3 Introdução",
    "text": "12.3 Introdução\nA estatística inferencial é a parte da estatística que usa os resultados da amostra para tomar decisões e tirar conclusões sobre a população de onde a amostra foi retirada. A estimação e o teste de hipóteses, tomados em conjunto, constituem a inferência estatística.\nEstimação é um procedimento pelo qual um valor ou valores numéricos são atribuídos a um parâmetro populacional com base nas informações de uma amostra. Na estatística inferencial, \\(\\mu\\) é chamada de média populacional e p é chamada de proporção populacional. Existem muitos outros parâmetros populacionais, como mediana, moda, variância e desvio padrão, como observado na Seção 11.2.2.\nSe houvesse possibilidade de realizar um censo (pesquisa incluindo toda a população de interesse), não haveria necessidade dos procedimentos de estimação. Seria equivalente ao que ocorre em uma eleição, basta contar os votos, para declarar os vencedores da eleição. No entanto, em saúde, realizar censo é um procedimento caro, demorado ou virtualmente impossível. Portanto, geralmente é utilizada uma amostra da população e calculada o valor das estatísticas da amostra apropriada. Baseado nessas estatísticas, é atribuído valores ao parâmetro.\nA estatística usada para estimar um parâmetro é chamada de estimador. Assim, a média da amostra, \\(\\bar{x}\\), é um estimador da média da população, \\(\\mu\\); e a proporção da amostra, \\(\\hat{p}\\), é um estimador da proporção da população, p. Estimativa é um valor que a função estimador assume.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "href": "12-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "title": "12  Estimação",
    "section": "12.4 Estimativa Pontual e Intervalo de Confiança",
    "text": "12.4 Estimativa Pontual e Intervalo de Confiança\nA partir do dataframe dados, serão calculados a média e o desvio padrão da variável pesoRN (peso dos recém-nascidos em g) que, para fins didáticos, serão considerados os parâmetros dessa “população”:\n\n mu &lt;- round(mean(dadosRNT$pesoRN, na.rm = TRUE))\n sigma &lt;- round(sd(dadosRNT$pesoRN, na.rm = TRUE))\n print(x = c(mu, sigma))\n\n[1] 3216  462\n\n\nA seguir, será extraída, dessa população, uma amostra de n = 30 1 e calculado os mesmas medidas resumidoras, que se constituirão nas estimativas da amostra:\n\nset.seed (1234)\namostra &lt;- dadosRNT %&gt;% slice_sample(n = 30)\n\n# Média amostral\nx_barra &lt;- round(mean(amostra$pesoRN, na.rm = TRUE))\n\n# Desvio padrão amostral  \ns &lt;- round(sd(amostra$pesoRN, na.rm = TRUE))\n\nprint(c(x_barra, s))\n\n[1] 3222  407\n\n\nO valor de 3222g é a média amostral, \\(\\bar{x}\\), usado como um estimativa da \\(\\mu\\), é denominado de estimativa pontual. Como já mencionado anteriormente, espera-se que cada amostra selecionada produza um valor diferente da estatística amostral. Assim, o valor atribuído a uma média populacional, \\(\\mu\\), com base em uma estimativa pontual depende de qual das amostras está sendo usada. Consequentemente, a estimativa pontual atribui um valor a \\(\\mu\\) que quase sempre difere da mesma.\nPara melhorar a precisão, usa-se uma estimativa de intervalo. Em vez de atribuir um único valor para o parâmetro populacional, é construído um intervalo, acrescentando ou subtraindo um valor, chamado de margem de erro, à estimativa pontual.\nEste procedimento é conhecido como estimação por intervalo e o intervalo construído, estabelecendo um limite inferior e um limite superior em torno da estimativa amostral, é denominado de intervalo de confiança (Altman 2005). Desta forma, é possível afirmar que o intervalo de confiança, provavelmente, contém o parâmetro populacional correspondente (Figura 12.1).\n\n\n\n\n\n\n\n\nFigura 12.1: Intervalo de Confiança\n\n\n\n\n\nA construção do intervalo de confiança depende da obtenção da margem de erro. Este processo necessita de dois fatores:\n\ndo desvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}=\\frac{\\sigma }{\\sqrt{n}}\\), que em decorrência do Teorema do Limite Central, pode ser escrito \\(EP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\\);\ndo nível de confiança (NC) atribuído ao intervalo.\n\nPrimeiro, quanto maior for o desvio padrão de \\(\\bar{x}\\), maior será a margem de erro subtraída e adicionada à estimativa pontual. Consequentemente, o intervalo de confiança se modifica de acordo com a margem de erro. Quanto maior a margem de erro mais amplo o intervalo de confiança.\nEm segundo lugar, a quantidade subtraída e adicionada à estimativa se modifica de acordo o nível de confiança. Para ter uma maior confiança, deve-se aumentar a margem de erro, de acordo com a probabilidade declarada. Quanto maior o nível de confiança, maior a probabilidade. O nível de confiança é mostrado como \\((1 - \\alpha) \\times 100\\)%, onde \\(\\alpha\\) é o nível de significância. Tradicionalmente, o valor de \\(\\alpha\\) é igual a 0,05, mas qualquer outro valor pode ser usado.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "href": "12-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "title": "12  Estimação",
    "section": "12.5 Estimação da média populacional: \\(\\sigma\\) conhecido",
    "text": "12.5 Estimação da média populacional: \\(\\sigma\\) conhecido\nA margem de erro para a estimativa da média populacional, \\(\\mu\\), quando se conhece o desvio padrão populacional,\\(\\sigma\\), e \\(n \\ge 30\\) ou, mesmo que \\(n &lt; 30\\), mas a população de onde amostra foi selecionada tem distribuição normal, é a quantidade que é subtraída ou adicionada ao valor da média da amostra, \\(\\bar{x}\\), para obter o intervalo de confiança para \\(\\mu\\). Desta forma, a margem de erro é igual a:\n\\[\nmargem \\quad de\\quad erro\\quad(me)= z_{(1-\\frac{\\alpha}{2})} \\times \\sigma_{\\bar{x}}\n\\]\nOu,\n\\[\nme = z_{(1-\\frac{\\alpha}{2})} \\times \\frac{\\sigma }{\\sqrt{n}}\n\\]\nLogo, o intervalo de confiança para a média populacional, \\(\\mu\\), para um nível de confiança (1 - \\(\\alpha \\times 100\\))%, é igual a:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  me\n\\]\nSe objetivo é construir um intervalo de confiança de 95%, a última equação passa a ser:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  z_{(0,975)} \\times me\n\\]\nOnde z é o valor crítico para o nível de confiança escolhido, obtido da tabela de distribuição normal padrão, e me é a margem de erro (\\(z_{0,975} \\times erro \\quad padrao\\)). Um intervalo de confiança de 95% significa que a área total sob a curva normal entre dois pontos em torno da média populacional, \\(\\mu\\), é igual a 95%, ou 0,95. A área das caudas é \\(\\alpha\\), ou seja, cada cauda á igual a \\(\\frac{\\alpha}{2}\\) (Figura 12.2)).\n\n\n\n\n\n\n\n\nFigura 12.2: Intervalo de Confiança de 95%\n\n\n\n\n\nPara encontrar o valor de z para um nível de confiança de 95%, primeiro encontram-se as áreas à esquerda desses dois pontos, \\(z_1\\) e \\(z_2\\). Esses dois valores de z serão iguais, mas com sinais opostos. A área total sob a curva é igual a 1. A área entre \\(z_1\\) e \\(z_2\\) é igual a \\(1 - \\alpha = 0,95\\).\nA área a esquerda de \\(z_1\\) é igual a 0,025 e a área a esquerda de \\(z_2\\) é igual a 1 – 0,025 = 0,975. No R, os valores \\(z_1\\) e \\(z_2\\) podem facilmente ser obtidos com a função qnorm():\n\nprint(c(qnorm(0.025),qnorm(0.975)), 3)\n\n[1] -1.96  1.96\n\n\nDessa maneira, para uma confiança de 95%, é usado um \\(z = 1.96\\), onde:\n\\[\np(-1,96 \\le z \\le 1,96) = 0,95\n\\] Logo,\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\sigma_{\\bar{x}})\n\\] ou\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\frac{\\sigma}{\\sqrt{n}})\n\\]\n\n12.5.1 Cálculo do intervalo de confiança com \\(\\sigma\\) conhecido\nUsando a média dos pesos dos recém-nascidos da amostra (n = 30), \\(\\bar{x}\\)= 3222 g, e o desvio padrão populacional conhecido, \\(\\sigma\\)= 462 g, tem-se que o intervalo de confiança de 95% (IC95%), para o peso dos recém-nascidos a termo na ‘população’ de onde esta amostra é proveniente:\nDados do exemplo para o cálculo\n\n n &lt;- 30\n x_barra &lt;- 3222\n sigma &lt;- 462\n\nCom 95% de confiança a margem de erro é igual a 1,96 vezes o erro padrão da média:\n\n n &lt;- 30\n me &lt;- 1.96 * sigma/sqrt(n)\n round(me,2)\n\n[1] 165.32\n\n\nBasta, agora, adicionar e subtrair a margem de erro da média:\n\n lim_inf &lt;- x_barra - me\n lim_sup &lt;- x_barra + me\n ic95 &lt;- c(lim_inf, lim_sup)\n round(ic95, 1)\n\n[1] 3056.7 3387.3\n\n\nAssim, tem-se uma confiança de 95% de que a verdadeira média, esteja incluída no intervalo. O nome para isso é intervalo de confiança de 95% para a média populacional.\n\n\n12.5.2 Função para calcular IC com \\(\\sigma\\) conhecido\nO cálculo manual é simples, mas enfadonho, nos tempos dos computadores. Em decorrência, como o R não tem uma função para encontrar os intervalos de confiança para a média de dados com distribuição normal quando o desvio padrão da população é conhecido, foi criada uma função para cumprir essa ação. Ela necessita dos seguintes argumentos:\n\nx \\(\\to\\) conjunto de números da amostra\ns \\(\\to\\) desvio padrão populacional\nnc \\(\\to\\) nível de confiança. Padrão: nc = 0.95\n\n\nIC_z &lt;- function (x, s, nc = 0.975)\n{\n  `%&gt;%` &lt;- dplyr::`%&gt;%`\n   n &lt;- length(x)\n   me &lt;- abs(qnorm((1-nc)/2))* sigma/sqrt(n)\n   df_out &lt;- data.frame( tamanho_amostral = n, \n                         media_amostral = mean(x), \n                         margem_erro = me,\n                         'IC limite inferior'=(mean(x) - me),\n                         'IC limite superior'=(mean(x) + me)) %&gt;%\n    tidyr::pivot_longer(names_to = \"Medidas\", values_to =\"valores\", 1:5 )\n  return(df_out)\n}\n\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.95)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           165.\n4 IC.limite.inferior   3056.\n5 IC.limite.superior   3387.\n\n\nEssa função pode ser salva no seu diretório e, quando necessária, pode ser ativada com a função source(), como visto na Seção 4.4.1. Com essa função fica fácil alterar o nível de confiança, por exemplo, para 99%. Isso mudará o Z crítico para:\n\n alpha &lt;- 0.01\n p &lt;- 1-(alpha/2)\n p\n\n[1] 0.995\n\n z_critico &lt;- qnorm(p)\n round(z_critico, 2)\n\n[1] 2.58\n\n\nCom a função IC_z():\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.995)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           237.\n4 IC.limite.inferior   2985.\n5 IC.limite.superior   3458.\n\n\nObservando o IC95% e o IC99%, verifica-se que a amplitude do intervalo aumentou com o crescimento da confiança de 95% para 99%, porque houve um aumento na margem de erro (Figura 12.3).\n\n\n\n\n\n\n\n\nFigura 12.3: Comparação entre IC95% e IC99%\n\n\n\n\n\n\n\n12.5.3 Interpretação do intervalo de confiança\nSe fossem extraídas todas as possíveis amostras de n = 30 da população de recém-nascidos a termo e construído para cada uma delas um intervalo de confiança de 95% em torno de cada média amostral, espera-se que 95% desses intervalos incluirão a média populacional e 5% não incluirão.\nO IC95% informa sobre a precisão com que a média amostral estima a média populacional desconhecida 2.\nNa Figura 12.4, são mostradas 20 amostras diferentes de tamanho n = 30, dessa população. Junto aparecem os intervalos de confiança de 95% construídos em torno dessas amostras. Observa-se que apenas uma amostra (em vermelho) não inclui a média populacional (linha tracejada vertical em azul). Pode-se afirmar com 95% de confiança que se forem extraídas muitas amostras do mesmo tamanho de uma população e construído intervalos de confiança de 95% em torno das médias dessas amostras, 95% desses intervalos de confiança incluirão a média populacional.\n\n\n\n\n\n\n\n\nFigura 12.4: Intervalos de confiança de 95% que mostra 20 replicações simuladas de amostras de n = 30 do peso do recém-nascido. Apenas um intervalo (em vermelho) não inclui a média populacional (linha vertical azul)",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "href": "12-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "title": "12  Estimação",
    "section": "12.6 Estimação da média populacional: \\(\\sigma\\) desconhecido",
    "text": "12.6 Estimação da média populacional: \\(\\sigma\\) desconhecido\nCom amostras pequenas, usar o modelo normal para construir intervalos de confiança, pode gerar um erro, pois os pressupostos do teorema do limite central não são respeitados. Quando o desvio padrão populacional, \\(\\sigma\\), é desconhecido e o tamanho amostral é pequeno (&lt; 30), a estimação da média populacional é feita usando a distribuição t.\n\n12.6.1 Distribuição t\nA distribuição t, desenvolvida por William Sealy Gosset, em 1908, é semelhante à distribuição normal. Como a curva de distribuição normal, a curva de distribuição t é unimodal, simétrica (em forma de sino) em torno da média e nunca encontra o eixo horizontal. A área total sob uma curva de distribuição t é 1 ou 100%. A curva da distribuição t é mais plana do que a curva de distribuição normal padrão. Em outras palavras, ela é mais achatada e mais espalhada. No entanto, conforme o tamanho da amostra aumenta, a distribuição t aproxima-se da distribuição normal padrão.\nO formato de uma curva de distribuição t particular depende do número de graus de liberdade. O número de graus de liberdade (gl) para uma distribuição t é igual ao tamanho da amostra menos um, ou seja, \\(gl=n-1\\), veja Seção 6.5.3.\nO número de graus de liberdade é o único parâmetro da distribuição t. Há uma diferente distribuição t para cada número de graus de liberdade, portanto, a distribuição t se constitui em uma família de distribuições (Figura 12.5).\n\n\n\n\n\n\n\n\nFigura 12.5: Curvas de distribuição t conforme o grau de liberdade comparadas à distribuição normal\n\n\n\n\n\nDa mesma maneira que a distribuição normal padrão, a média da distribuição padrão t é 0. Entretanto, ao contrário da distribuição normal padrão, cujo desvio padrão é 1, o desvio padrão de uma distribuição t é \\(\\sqrt{\\frac{gl}{gl-2}}\\) , para gl &gt; 2, sempre é maior do que 1. Assim, o desvio padrão de uma distribuição t é maior do que o desvio padrão da distribuição normal padrão.\nOs valores de \\({t}_{crítico}\\) podem ser obtidos usando a função qt() que usa os seguintes argumentos:\n\np \\(\\to\\) probabilidade, igual a \\(1 - \\frac{\\alpha}{2}\\), considerando-se bicaudal e \\(1 - \\alpha\\) quando unicaudal;\ndf \\(\\to\\) graus de liberdade;\nlower.tail \\(\\to\\) lógico; se TRUE, informa a probabilidade da cauda inferior. O padrão é TRUE.\n\nAssim, o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é:\n\nalpha  &lt;-  0.05\np &lt;- 1 - (alpha/2)\ngl = 10\nt &lt;- qt(p = p, df = 10, lower.tail = TRUE)\nround(t, digits = 2)\n\n[1] 2.23\n\n\nA área compreendida entre \\(\\pm\\) 2.23$ é igual a 95% (Figura 12.6):\n\\[\np(-2,23\\le t\\le 2,23)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 12.6: Distribuição t com gl = 10, bilateral\n\n\n\n\n\nQuando se considera apenas uma das caudas (unicaudal ou unilateral), o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é\n\nt1 &lt;- qt(p = 0.95, df = 10, lower.tail = TRUE)\nround(t1, digits = 2)\n\n[1] 1.81\n\n\nAssim, a área abaixo de 1.81 é igual a 95% (Figura 12.7).\n\\[\np(t \\le 1,81)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 12.7: Distribuição t com gl = 10, unilateral\n\n\n\n\n\n\n\n12.6.2 Cálculo do intervalo de confiança com \\(\\sigma\\) desconhecido\nSerão utilizados nesta seção, os dados da altura de mulheres, obtidos na Seção 12.2, atribuídos ao objeto dados. Suponha-se que os parâmetros sejam desconhecidos. Para estimar esses parâmetros, selecionou-se uma amostra de n = 30 desse conjunto dados. Tomando essa amostra, calcula-se a sua média e o seu desvio padrão:\n\nset.seed(2345)\namostra1 &lt;- dados %&gt;%\n  slice_sample(n = 30)\n\nx_barra1 &lt;- mean(amostra1$altura, na.rm = TRUE)\ns1 &lt;- sd(amostra1$altura, na.rm = TRUE)\nprint(round(c(x_barra1, s1),3))\n\n[1] 1.599 0.051\n\n\nA maneira mais intuitiva de estimar a média da população com base na amostra, é, simplesmente, calcular a média e o desvio padrão. Entretanto, para uma maior precisão, é sempre importante calcular o intervalo de confiança.\n\n12.6.2.1 Cálculo manual do IC\nQuando o desvio padrão da população (\\(\\sigma\\)) não é conhecido, pode-se usar o seu estimador que é o desvio padrão da amostra (s), respeitando os pressupostos (Motulsky 2010). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\).\n\\[\nEP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\n\\]\nO intervalo de confiança para a \\(\\mu\\) para um nível de confiança (NC) de \\((1 – \\alpha) \\times100\\)% é igual a:\n\\[\nIC_{NC}(\\mu)\\rightarrow x\\pm (t_{({1-\\frac{alpha}{2})} } \\times \\frac {s}{\\sqrt{n}})\n\\]\nQuando o tamanho amostral é grande, o valor de t se aproxima do valor de z, portanto, em situações em que não se conhece o desvio padrão populacional, não há muita diferença se houver uma aproximação de t para z (Tabela 12.1).\n\n\n\n\nTabela 12.1: Comparação dos valores z e t(gl)\n\n\n\nnglzt541.962.571091.962.2330291.962.0450491.962.01100991.961.982001991.961.975004991.961.961,0009991.961.96\n\n\n\n\n\nA amostra1 de n = 30, \\(\\overline x\\) = 1.599m e \\(s\\) = 0.051m. Essas estimativas servirão para o cálculo do intervalo de confiança, usando uma distribuição t bicaudal e um nível de significância \\(\\alpha = 0,05\\).\n\nn1 &lt;-  length(amostra1$altura)\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\n# Graus de liberdade  \ngl &lt;- n1 - 1\n# Valor t crítico  \ntc &lt;-  qt(p, gl, lower.tail = TRUE)\n# Erro padrão\nEP1 &lt;- round(s1/sqrt(n1),3)\nprint(round(c(tc, EP1),3))\n\n[1] 2.045 0.009\n\n\nCom esses dados, calcula-se o intervalo de confiança de 95%:\n\nme1 &lt;- tc*EP1\nlim_inf &lt;- x_barra1 - me1\nlim_sup &lt;- x_barra1 + me1\nic95 &lt;- c(lim_inf, lim_sup)\nround(ic95, 2)\n\n[1] 1.58 1.62\n\n\n\n\n12.6.2.2 Cálculo usando uma função do R\nO R possui algumas funções que calculam o intervalo de confiança para variáveis numéricas, baseadas na distribuição t. Entre elas, a função CI(), incluída no pacote Rmisc (Hope 2022). Esta função tem dois argumentos:\n\nx ⟶ vetor de dados;\nci ⟶ intervalo de confiança a ser calculado\n\n\nIC95 &lt;- CI(amostra1$altura, ci = 0.95)\nround(IC95, 2)\n\nupper  mean lower \n 1.62  1.60  1.58",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "href": "12-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "title": "12  Estimação",
    "section": "12.7 Intervalo de Confiança para uma proporção populacional",
    "text": "12.7 Intervalo de Confiança para uma proporção populacional\n\n12.7.1 Dados para estimar a proporção populacional\nAqui, será utilizada uma amostra aleatória de n = 60 do conjunto de dados dados para estimar a proporção de mulheres fumantes.\n\nset.seed(2346)\ndados60 &lt;-  dados %&gt;% \n  select (fumo) %&gt;% \n  slice_sample(n = 60)\n\nstr(dados60)\n\ntibble [60 × 1] (S3: tbl_df/tbl/data.frame)\n $ fumo: Factor w/ 2 levels \"Fumante\",\"Não fumante\": 2 2 1 2 2 2 2 2 2 1 ...\n\n\n\n\n12.7.2 Cálculo da estimativa pontual da proporção\nNessa amostra, a proporção de fumantes é:\n\ntab &lt;- table(dados60$fumo)\ntab\n\n\n    Fumante Não fumante \n         14          46 \n\ntabFumo &lt;- round (prop.table (tab), 3)\ntabFumo\n\n\n    Fumante Não fumante \n      0.233       0.767 \n\n\n\n\n12.7.3 Cálculo do intervalo de confiança para a proporção\nCálculo manual com aproximação normal\n1ª etapa: verificar a premissa de que quando a proporção populacional é desconhecida a proporção pontual (\\(\\hat p\\)) e o seu complemento (\\(\\hat q = 1 - \\hat p\\)) multiplicados, cada um, por \\(n\\), devem ser maior do que 5.\n\nn &lt;- length(dados60$fumo)\n(tabFumo) * n\n\n\n    Fumante Não fumante \n      13.98       46.02 \n\n\nComo se observa, ambos os valores são maiores do que 5.\n2ª Etapa: O intervalo pode ser estimado pela distribuição normal e é necessário calcular o z_crítico:\n\nalpha &lt;- 0.05\np &lt;-  1 - alpha/2\nzc &lt;- qnorm (p, mean = 0, sd = 1)\nround(zc, 2)\n\n[1] 1.96\n\n\n3ª Etapa: Cálculo do erro padrão da proporção (\\(\\sqrt \\frac {\\hat p \\times \\hat q}{n}\\)) e da margem de erro (veja também a Seção 11.6):\n\n# Extração da proporção amostral do tabFumo\nprop &lt;- tabFumo [1]\n\n# Cálculo do EP amostral\nEP &lt;- sqrt((prop * (1 - prop))/n)\n\n# Cálculo da margem de erro(me)\nme &lt;- zc * EP\n\n# dados necessários para o cálculo do IC95%\nprint(c(prop, me), digits = 3)\n\nFumante Fumante \n  0.233   0.107 \n\n\n4ª Etapa: Intervalo de confiança\n\nic_prop &lt;- c((prop - me), (prop + me))\nround(ic_prop, 3)\n\nFumante Fumante \n  0.126   0.340 \n\n\nCálculo usando uma função\nO chamado Intervalo de Confiança Exato corrigem as deficiências da aproximação normal. O R tem uma função para este cálculo: BinomCI() do pacote DescTools (Signorell et al. 2022). É preferível usar o método de Clopper e Pearson que fornece o IC exato.\nOs argumentos da função BinomCI() são:\n\nx \\(\\to\\) é o número de desfechos, sucessos;\nn \\(\\to\\) é o tamanho da amostra, número de ensaios;\np \\(\\to\\) probabilidade, hipótese nula; se ignorada o padrão é 0,50;\nconf.level \\(\\to\\) nível de confiança, o padrão é 0.95;\nmethod \\(\\to\\) possui vários métodos para calcular intervalos de confiança para uma proporção binomial como: “clopper-pearson” (exact interval), “wilson”, “wald”, “agresti-coull”, “jeffreys”, “modified wilson”, “modified jeffreys”, “arcsine”, “logit”, “witting”, “pratt”. O método padrão é o de “wilson”. Qualquer outro método, há necessidade de solicitar;\nsides \\(\\to\\) hipótese alternativa padrão “two.sided” (bilateral), mas pode ser “right” ou “left” (unilateral a direita ou a esquerda, respectivamente).\n\n\nx &lt;-  tab[1]\nIC &lt;- BinomCI (x, \n               n, \n               conf.level = 0.95, \n               method = \"clopper-pearson\")\nround(IC, 3)\n\n       est lwr.ci upr.ci\n[1,] 0.233  0.134   0.36\n\n\nObserve que existe uma pequena diferença entre os valores da aproximação normal e o exato, com o método de “clopper-pearson”\n\n\n\n\n\n\nAltman, Douglas G. 2005. «Why We Need Confidence Intervals». World J. Surg 29: 554–56.\n\n\nHope, Ryan M. 2022. Rmisc: Ryan Miscellaneous. doi:10.32614/CRAN.package.Rmisc.\n\n\nMotulsky, Harvey. 2010. «The Theory of Confidence Intervals». Em Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking, Second Edition, 96–102. New York, NY: Oxford University Press.\n\n\nSignorell, Andri et al. 2022. DescTools: Tools for Descriptive Statistics. https://cran.r-project.org/package=DescTools.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "12-estimacao.html#footnotes",
    "href": "12-estimacao.html#footnotes",
    "title": "12  Estimação",
    "section": "",
    "text": "Repetindo, é importante lembrar que toda vez que for extraída uma nova amostra, o resultado será um conjunto de números diferentes e, em consequência, a média será diferente. Por isso, se for importante repetir o mesmo resultado, deve-se usar a função set.seed(). Consulte a Seção 9.7.2.4.↩︎\nAnteriormente, mostrou-se a media populacional por uma questão didática. A regra é não se conhecer a média populacional, razão da importância do intervalo de confiança↩︎",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html",
    "href": "13-testeHipoteses.html",
    "title": "13  Teste de Hipóteses",
    "section": "",
    "text": "13.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr,\n               lsr,\n               pwr,\n               readxl,\n               rstatix)",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#sec-dadosth",
    "href": "13-testeHipoteses.html#sec-dadosth",
    "title": "13  Teste de Hipóteses",
    "section": "13.2 Dados para o exemplo",
    "text": "13.2 Dados para o exemplo\nConsidere o mesmo arquivo dadosMater.xlsx, usado várias vezes neste livro e disponível para consulta na Seção 5.6. Após a leitura do arquivo com a função read_excel() do pacote readxl, serão filtrados as gestações a termo (37 a 42 semanas de gestação) e selecionadas as varáveis sexo e pesoRN. Considerando esses dados como uma “população” para fins didáticos, será extraída uma amostra de 200 observações e atribuido o resultado ao objeto dados.\n\n\n\n\n\n\nPergunta motivadora\n\n\n\nExiste uma diferença estatisticamente significativa nos pesos dos recém-nascidos de acordo com o sexo?\n\n\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  dplyr::filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(sexo, pesoRN) %&gt;%\n  mutate(sexo = factor(sexo, \n                       levels = c(1,2), \n                       labels = c(\"Masculino\", \"Feminino\"))) %&gt;% \n  slice_sample(n = 200)\n\n\n13.2.1 Exploração e transformação dos dados\nInicialmente, para ter uma visão da estrutura dos dados, usa-se:\n\nstr(dados)\n\ntibble [200 × 2] (S3: tbl_df/tbl/data.frame)\n $ sexo  : Factor w/ 2 levels \"Masculino\",\"Feminino\": 2 2 2 1 2 1 1 2 1 2 ...\n $ pesoRN: num [1:200] 2685 2665 3445 2340 3440 ...\n\n\nEste conjunto de dados fica restrito, portanto, a 200 casos, contendo duas variáveis sexo e pesoRN, necessárias neste capítulo e assim resumidas:\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr::summarise (n = n(),\n                    media = mean(pesoRN, na.rm = TRUE),\n                    dp = sd(pesoRN, na.rm = TRUE))\nresumo\n\n# A tibble: 2 × 4\n  sexo          n media    dp\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino   100 3259.  462.\n2 Feminino    100 3153.  444.\n\n\nEsta amostra de 100 meninos e 100 meninas, informa que os meninos têm, em média, 3259 g ao nascer e as meninas 3153 g. Esta diferença de peso entre os sexos pode ter ocorrido devido ao acaso. Portanto, há necessidade de realizar um teste de hipóteses para tomar uma decisão sobre o parâmetro populacional. Esta diferença é grande o suficiente para rejeitar a hipótese de igualdade entre os pesos e concluir que existe uma diferença real entre eles?",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#introdução",
    "href": "13-testeHipoteses.html#introdução",
    "title": "13  Teste de Hipóteses",
    "section": "13.3 Introdução",
    "text": "13.3 Introdução\nNo capítulo anterior, foi discutido aspectos relacionados à estimação, que se constitui, junto com o teste de hipótese, em procedimentos básicos da estatística inferencial. Em um teste de hipóteses, testa-se uma teoria ou crença sobre um parâmetro populacional Kelen, Brown, e Ashton (1988). Na maioria das vezes, como mencionado anteriormente, obtém-se informações a partir de uma amostra em função da impossibilidade ou dificuldade de se conseguir essas informações a partir da população. Portanto, extrapolar ou estender os resultados, obtidos de uma amostra, para a população, significa aceitá-los como representações adequadas da mesma.\nSabe-se que as estimativas amostrais diferem dos valores reais (populacionais) e o objetivo dos testes de hipóteses é estabelecer a probabilidade de essa diferença ser explicada pelo acaso. O teste de hipóteses fornece um sistema referencial para a tomada de decisão sobre a adequação ou não dos dados amostrais serem representativos de uma população. Este sistema referencial é a distribuição de probabilidade do evento observado Menezes e Burattini (2004a).\nInicialmente é importante fazer uma distinção entre hipótese de pesquisa e hipótese estatística. Uma hipótese de pesquisa é uma afirmação que expressa a relação esperada entre as variáveis de um estudo científico. Ela é baseada em uma pergunta de pesquisa e serve para orientar a coleta e análise dos dados. Uma hipótese de pesquisa pode ser confirmada ou refutada pelos resultados do estudo. Um exemplo de hipótese de pesquisa é: “O tabagismo durante a gestação interfere sobre o peso dos conceptos”. Uma hipótese de pesquisa corresponde àquilo que se quer acreditar sobre o mundo. Uma hipótese estatística é uma afirmação relacionada aos parâmetros de uma população. Baseia-se em uma hipótese de pesquisa e serve para testar a validade da mesma usando técnicas estatísticas. Uma hipótese estatística pode ser aceita ou rejeitada com um certo nível de confiança. A hipótese estatística deve ter uma relação clara com as hipóteses de pesquisa Por exemplo: “A média de peso dos recém-nascidos de mães fumantes é menor do que o das não fumantes”; “A média de peso dos recém-nascidos masculinos é igual ao peso dos recém-nascidos femininos”, ou ainda, “A média de peso dos recém-nascidos masculinos é diferente do peso dos recém-nascidos femininos”. Todos esses exemplos são legítimos de uma hipótese estatística porque são afirmações sobre um parâmetro populacional e estão significativamente relacionados à hipótese de pesquisa.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#hipótese-nula-e-alternativa",
    "href": "13-testeHipoteses.html#hipótese-nula-e-alternativa",
    "title": "13  Teste de Hipóteses",
    "section": "13.4 Hipótese nula e alternativa",
    "text": "13.4 Hipótese nula e alternativa\nEm função da hipótese de pesquisa, mencionada anteriormente, foram gerados os dados do exemplo. A hipótese de pesquisa corresponde ao que se quer acreditar, “o sexo interfere no peso dos neonatos”. Para refutar ou não essa afirmação constrói-se um teste de hipótese para verificar se ela é compatível ou não com os dados disponíveis Guyatt et al. (1995).\nNo teste de hipóteses (TH), existem dois tipos de hipóteses, definidas como:\nHipótese nula(\\(H_{0}\\)): hipótese que afirma a não existência de diferença entre os grupos e, portanto, a diferença observada é atribuível ao acaso. É a hipótese a ser testada, aquela que se busca afastar, demonstrando que é, provavelmente 1, falsa, não válida. É denotada como:\n\\[\nH_{0}: \\mu_{1}= \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2}=0\n\\]\nHipótese alternativa (\\(H_{1} \\quad ou \\quad H_{a}\\)): é a hipótese contrária, como o nome diz, alternativa à \\(H_{0}\\). Representa a posição de uma nova perspectiva, a conclusão que será apoiada se \\(H_{0}\\) for rejeitada. Ela supõe que realmente exista uma diferença entre os grupos. É a hipótese que o pesquisador pretende comprovar. É denotada, em geral, simplesmente como havendo uma diferença entre os grupos, sem indicar uma direção, hipótese bilateral ou bicaudal:\n\\[\nH_{1}: \\mu_{1} \\neq  \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} \\neq  0\n\\]\nOu, se houver uma suspeita, através de um conhecimento prévio, apontar uma direção para a diferença, ou seja, usar uma hipótese unilateral ou monocaudal. Neste caso existe duas possibilidade:\n\nUnilateral à direita:\n\n\\[\nH_{1}: \\mu_{1} &gt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &gt; 0\n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\le \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\le 0\n\\] 2)\n\nUnilateral à esquerda:\n\n\\[\nH_{1}: \\mu_{1} &lt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &lt; 0   \n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\ge \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\ge 0\n\\]\nA \\(H_{0}\\) e \\(H_{1}\\) são opostas e mutuamente exclusivas. No teste de hipótese calcula-se a probabilidade de obter os resultados encontrados caso não haja efeito na população, ou seja, caso a \\(H_{0}\\) seja verdadeira. Portanto, o TH é um teste de significância para a \\(H_{0}\\).\n\n13.4.1 Exemplo\nVoltando à hipótese de pesquisa, usando os dados da Seção 13.2, as hipóteses estatísticas seriam escritas da seguinte maneira, considerando uma hipótese alternativa bilateral.\n\\[\nH_{0}: \\mu_{peso_{masc}} = \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}}=0\n\\]\n\\[\nH_{1}: \\mu_{peso_{masc}} \\neq \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}} \\neq 0\n\\]",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "href": "13-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "title": "13  Teste de Hipóteses",
    "section": "13.5 Escolha do teste estatítico e regra de decisão",
    "text": "13.5 Escolha do teste estatítico e regra de decisão\n\n13.5.1 Teste estatístico\nUsa-se um teste estatístico para testar as hipóteses estabelecidas. Este depende do tipo de distribuição da variável, por exemplo, teste z, teste t, teste F, qui-quadrado (\\(\\chi^2\\)). Cada teste fornece um valor para dirigir a decisão de rejeitar ou não a hipótese nula. Essa decisão depende da magnitude do teste valor. O nome para esse indicador, calculado para orientar a escolha, é estatística de teste. Para fazer isso, há necessidade de determinar qual seria a distribuição amostral da estatística de teste se a hipótese nula fosse realmente verdadeira. Depois de analisar esse valor, decide-se se a hipótese nula está correta ou, caso contrário, ela é rejeitada em favor da alternativa.\nÉ fundamental lembrar que cada teste estatístico tem suas características e seus pressupostos que devem ser analisados para garantir a validade das estatísticas de teste. Para uma boa parte deles, por exemplo, deve-se verificar se os dados se ajustam à distribuição normal (normalidade), a igualdade das variâncias (homocedasticidade), independência entre os grupos, tipo de correlação, etc.\n\n\n13.5.2 Regra de decisão\nRealizado o teste estaístico, para rejeitar ou não rejeitar a \\(H_{0}\\), partindo do pressuposto de que ela é verdadeira, há necessidade de determinar uma regra de decisão que permita uma declaração fundamentada. Essa regra de decisão cria duas regiões, uma região de rejeição e uma região de não rejeição da \\(H_{0}\\), demarcadas por um valor crítico.\nEste valor de referência é determinado pelo nível de significância, \\(\\alpha\\), e deve ser explicitamente mencionado antes de se iniciar a pesquisa, pois é baseado nele que se fundamentam as conclusões da mesma. O nível de significância corresponde a probabilidade de rejeitar uma hipótese nula verdadeira. Quando a hipótese alternativa não tem uma direção definida, a área de rejeição, \\(\\alpha\\), é colocada nas duas caudas (Figura 13.1), superior), dividindo a probabilidade (\\(\\frac {\\alpha}{2}\\)); quando houver indicação prévia de um sentido, a área de rejeição ficará a direita (Figura 13.1), inferior) ou a esquerda dependendo da direção escolhida.\n\n\n\n\n\n\n\n\nFigura 13.1: Regiões bicaudais (acima) e monocaudal à direita (abaixo) de rejeição e não rejeição da hipótese nula\n\n\n\n\n\nQuais valores exatos da estatística de teste deve-se associar à hipótese nula e quais valores exatos devem ser associados à hipótese alternativa? Para encontrar a região de rejeição, deve-se levar em consideração:\n\nA estatística do teste deve ser muito grande ou muito pequena para que a hipótese nula seja rejeitada;\n\nDistribuição da variável de teste, que depende da distribuição da população em estudo e do tamanho da amostra;\n\nNível se significância adotado, em geral, usa-se um \\(\\alpha\\) = 0,05, o que equivale a dizer que a região de rejeição abrange 5% da distribuição.\n\nÉ importante entender bem este último ponto. A região de rejeição corresponde aos valores da estatística de teste para os quais se rejeita a hipótese nula e a distribuição amostral em questão descreve a probabilidade de obtermos um determinado valor da estatística de teste se a hipótese nula for efetivamente verdadeira.\n\n\n\n\n\n\nImportante\n\n\n\nSuponha-se que foi escolhido uma região de rejeição que cobre 10% da distribuição amostral e que a hipótese nula é realmente verdadeira. Qual seria a probabilidade de rejeitar incorretamente a hipótese nula?\nObviamente, a resposta é 10%! E o teste usado teria um nível \\(\\alpha\\) = 0,10. Ou seja, se a hipótese nula é verdadeira e for rejeitada, foi cometido um erro.\n\n\n\n13.5.2.1 Erros de decisão\nComo se observa, ao se tomar uma decisão existe a possibilidade de se cometer erros. O primeiro erro é denominado de erro tipo I e ocorre quando, baseado na regra de decisão escolhida, uma hipótese nula verdadeira é rejeitada. Nesse caso, tem-se um resultado falso positivo. Há uma conclusão de que existe um efeito quando na verdade ele não existe. A probabilidade de cometer esse tipo de erro é \\(\\alpha\\), o mesmo usado como nível de significância no estabelecimento da regra de decisão.\n\\[\nP(rejeitar \\quad H_{0}|H_{0} \\quad verdadeira) = \\alpha\n\\]\nQual o valor de \\(\\alpha\\) que pode representar forte evidencia contra \\(H_{0}\\), reduzindo a possibilidade de erro tipo I?\nO valor de \\(\\alpha\\) escolhido, apesar de arbitrário, deve corresponder a importância do que se pretende demonstrar, quanto mais importante, menor deve ser o valor de \\(\\alpha\\). Nesses casos, não se quer rejeitar incorretamente \\(H_{0}\\) mais de 5% das vezes. Isso corresponde ao nível de significância mais usado de 0,05 (\\(\\alpha = 0,05\\)). Em algumas situações também são utilizados 0,01 e 0,10. Como mencionado, o valor de \\(\\alpha\\) deve ser escolhido antes de iniciar o estudo.\nExiste uma outra possibilidade de erro, denominado de erro tipo II, que ocorre quando a hipótese nula é realmente falsa, mas com base na regra de decisão escolhida, não se rejeita essa hipótese nula. Nesse caso, o resultado é um falso negativo; não se conseguiu encontrar um efeito que realmente existe. A probabilidade de cometer esse tipo de erro é chamada de \\(\\beta\\).\n\\[\nP(não \\quad rejeitar \\quad H_{0}|H_{0} \\quad falsa) = \\beta\n\\]\nNa construção de um teste de hipótese, o erro tipo II é considerado menos grave que o erro tipo I. Entretanto, ele é bastante importante. Tradicionalmente, adota-se o limite de 0,10 a 0,20 para o erro tipo II.\nNa Figura 13.2 estão resumidas as possíveis consequências na tomada de decisão em um teste de hipótese Fletcher, Fletcher, e Fletcher (2014).\n\n\n\n\n\n\n\n\nFigura 13.2: Tomada de decisão e erros\n\n\n\n\n\n\n\n\n13.5.3 Exemplo (continuação)\nContinuando com o exemplo da Seção 13.4.1, aceita-se que os pesos dos recém-nascidos de ambas as amostras tenham distribuição normal e que as variâncias são semelhantes. Apesar de o desvio padrão (\\(\\sigma\\)) da população-alvo ser conhecido (462, 444g), será suposto que ele é desconhecido 2. Portanto, o teste t de amostras independentes será o teste escolhido como o teste estatístico. A hipótese alternativa é bilateral e o \\(\\alpha\\) = 0,05.\nA distribuição t é dependente dos grau de liberdade, que para duas amostras independentes é igual \\(gl=n_1+n_2-2\\). Para os dados em uso, tem-se:\n\n n1 &lt;- resumo$n[1]\n n2 &lt;- resumo$n[2]\n gl &lt;- n1 + n2 - 2\n gl\n\n[1] 198\n\n\nPara o nível de significância escolhido, o valor crítico de t para gl = 198 e uma hipótese alternativa bilateral pode ser obtido da seguinte maneira:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round(qt(p, gl),3)\ntc\n\n[1] 1.972\n\n\nA partir do cálculo do valor crítico de t, podemos estabelecer a regra de decisão para as hipóteses estatísticas:\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\n|t_{calculado}| \\ge |t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\nO teste t pode ser calculado no R, usando a função t_teste() do pacote rstatix. Esta função usa, entre outros, os seguintes argumentos:\n\ndata \\(\\to\\) dataframe contendo as variáveis da formula;\nformula \\(\\to\\) uma fórmula da forma x ~ grupo onde x é uma variável numérica que fornece os valores dos dados e grupo é um fator;\npaired \\(\\to\\) lógico; indicando se o teste é pareado. Padrão é FALSE;\nvar.equal \\(\\to\\) lógico: se TRUE, uma variância combinada é usada; caso contrário, a aproximação de Welch dos graus de liberdade é usada\nalternative \\(\\to\\) two.sided (padrão) ou greater ou less.\n\n\nteste &lt;- rstatix::t_test(data = dados, \n                         formula = pesoRN~sexo, \n                         alternative = \"two.sided\",\n                         detailed = TRUE)\nteste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.    group1 group2    n1    n2 statistic      p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     106.     3259.     3153. pesoRN Mascu… Femin…   100   100      1.66 0.0991\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída do teste mostra uma estatística de teste 3 igual a 1.657. Esta é maior do que o t_crítico = 1.972, consequentemente, rejeita-se a hipótese nula e conclui-se, com uma confiança de 95%, que existe uma diferença estatisticamente significativa no peso dos recém-nascidos entre os sexos. Esta diferença é em média igual a 106 g (IC95%: -20, 233), \\(peso_{meninos} &gt; peso_{meninas}\\).",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#sec-valorp",
    "href": "13-testeHipoteses.html#sec-valorp",
    "title": "13  Teste de Hipóteses",
    "section": "13.6 Valor p do teste",
    "text": "13.6 Valor p do teste\nNas seções anteriores, foi discutido um procedimento onde se encontrou o valor de probabilidade tal que uma dada hipótese nula é rejeitada ou não é rejeitada, de acordo com o nível de significância, \\(\\alpha\\), fixado, pelo pesquisador, no início da pesquisa.\nEssa abordagem do valor de probabilidade, mais comumente chamada de abordagem do valor p, fornece esse valor. Uma vez realizada a pesquisa, o pesquisador calcula a probabilidade de obter um resultado tão ou mais extremo que o observado, uma vez que a hipótese nula é verdadeira. O valor p também é conhecido como nível descritivo do teste Menezes e Burattini (2004b).\nO objetivo de um teste estatístico é transformar em probabilidade a magnitude do desvio verificado em relação ao valor esperado, fornecendo o valor p. A partir daí pode-se, também, definir a regra de decisão, usando esse valor p. Toma-se o valor predeterminado (em geral, 0,05) de \\(\\alpha\\) e, então, compara-se o valor p com \\(\\alpha\\) e toma-se a decisão. Usando essa abordagem, rejeita-se a \\(H_{0}\\) se o valor p &lt; \\(\\alpha\\) e não se rejeita se o valor p &gt; \\(\\alpha\\). Costuma-se dizer que se o valor p &lt; \\(\\alpha\\), o resultado é significativo e não significativo quando p &gt; \\(\\alpha\\).\nUma boa parte dos pesquisadores, principalmente no início da carreira, ficam empolgados pelo conhecimento do valor p. Entretanto, deve ser sempre lembrado que encontrar o valor p não é o único foco da pesquisa. O foco deve estar dirigido ao tamanho do efeito (effect size). O valor p obtido pelo teste estatístico, vai informar apenas sobre a probabilidade de se cometer erro ao rejeitar ou não rejeitar a hipóteses nula.\n\n13.6.1 Exemplo (continuação)\nO teste realizado, t_test(), fornece o valor p = 0.0991. Este valor é menor do que \\(\\alpha\\) e leva as mesmas conclusões da Seção 13.5.3.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#poder-do-teste",
    "href": "13-testeHipoteses.html#poder-do-teste",
    "title": "13  Teste de Hipóteses",
    "section": "13.7 Poder do teste",
    "text": "13.7 Poder do teste\nO poder do teste estatístico é a probabilidade de que um teste de hipótese rejeite corretamente a hipótese nula quando uma hipótese alternativa específica é verdadeira. É denotado comumente por \\(1 - \\beta\\) e representa a capacidade de um teste para detectar um efeito, se esse efeito realmente existir. O poder varia de 0 a 1 e, à medida que o poder do teste aumenta, a probabilidade \\(\\beta\\) de cometer um erro tipo II diminui.\n\\[\nPoder \\quad do \\quad teste = P(rejeitar \\quad H_{0}|H_{0} \\quad falsa)\n\\]\n\nNa Figura Figura 13.3, visualiza-se o poder em verde mais escuro. Em um teste de hipótese, o valor \\(\\alpha\\) sempre é estabelecido com antecedência, que geralmente é definido como 0,05, de modo que a taxa de erro do Tipo I é definida antes mesmo de se iniciar o teste. Em seguida, pode-se calcular o valor crítico mínimo necessário para rejeitar \\(H_0\\). É possível traçar uma linha da distribuição da hipótese nula até a distribuição da hipótese alternativa e separar a área sob a curva em duas partes. Se o valor t calculado cair à esquerda da linha tracejada, não se consegue rejeitar \\(H_0\\) quando \\(H_1\\) for verdadeira e é cometido um erro do Tipo II. Se o valor calculado cair à direita, rejeita-se \\(H_0\\) quando \\(H_1\\) é verdadeira e a decisão é correta. Portanto, a área à direita da curva é o poder.\n\n\n\n\n\n\n\n\nFigura 13.3: Nível de significância, probabilidade de erro tipo II, poder e nível de confiança em um teste de hipótese e a região de rejeição da hipótese nula (à direita da linha vertical tracejada).\n\n\n\n\n\nO poder do teste depende de vários fatores, como:\n\nO nível de significância do teste, que é a probabilidade de rejeitar a hipótese nula quando ela é verdadeira (erro tipo I).\nA magnitude do efeito, que é a diferença entre o valor real do parâmetro e o valor considerado na hipótese nula.\nA variabilidade da população, que é medida pelo desvio padrão ou pela variância dos dados.\nO tamanho da amostra, que é o número de observações coletadas para o teste.\n\nEm geral, pode-se dizer:\n\nquanto maior o nível de significância, maior o poder do teste;\n\nquanto maior a magnitude do efeito, maior o poder do teste;\n\nquanto menor a variabilidade da população, maior o poder do teste;\n\nquanto maior o tamanho da amostra, maior o poder do teste.\nExistem diferentes métodos para calcular o poder do teste, dependendo do tipo de teste e da distribuição dos dados. Por exemplo, para um teste de uma média com variância desconhecida, usa-se a distribuição t de Student com \\(n - 1\\) graus de liberdade. Para um teste de duas proporções, usa-se a distribuição normal aproximada. A análise de poder é uma ferramenta útil para planejar um estudo e determinar o tamanho da amostra necessário para obter um poder desejado. Ela também pode ser usada para avaliar a qualidade de um estudo realizado e verificar se o teste foi capaz de detectar um efeito relevante.\n\n\n13.7.1 Exemplo (continuação)\nO teste t retornou um resultado significativo, com valor de t = 1.657 &gt; 1.972, com p = 0.0991. Um resultado significativo não informa sobre a magnitude do efeito. Para isso, lançamos mão do teste d de Cohen que pode ser calculado, usando a função cohensD() do pacote lsr:\n\nd &lt;- lsr::cohensD (data = dados, formula = pesoRN ~ sexo)\nd\n\n[1] 0.2343115\n\n\nNa Seção 14.2.6.1, se entrará em maiores detalhes, por enquanto, será assumido que a magnitude do efeito é pequena.\nDe posse do valor do d de Cohen, é possível calcular, através da função pwr.t.test() do pacote pwr, o poder do teste estatístico. Os argumentos dessa função são:\n\nn \\(\\to\\) número de observações por amostra;\nd \\(\\to\\) magnitude do efeito, d de Cohen;\nsig.level \\(\\to\\) nível de significância (padrão = 0.05);\npower \\(\\to\\) poder do teste;\ntype \\(\\to\\) tipo de teste (one- , two- ou paired-samples);\nalternative \\(\\to\\) hipótese alternativa, deve ser “one-sided” ou “two-sided (padrão).\n\nO parâmetro que se quer calcular deve ser passado como NULL. Assim, o poder do teste estatístico do exemplo é:\n\npoder &lt;- pwr::pwr.t.test(n = 150,\n                         d = d,\n                         sig.level = 0.05, \n                         power = NULL,\n                         type = \"two.sample\",\n                         alternative = \"two.sided\")\npoder\n\n\n     Two-sample t test power calculation \n\n              n = 150\n              d = 0.2343115\n      sig.level = 0.05\n          power = 0.5250278\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nA saída mostra que no lugar do NULL, aparece o poder do teste estatístico. Ou seja, o poder foi de 0.525 , consequentemente, como \\(\\beta = 1 – Poder\\), então, \\(\\beta\\) = 0.475.\nO poder geralmente é definido em 0,80 (ou 0,90). Isto significa que se existirem efeitos verdadeiros a serem encontrados em 100 estudos diferentes com 80% de poder, apenas 80 em 100 testes estatísticos irão realmente detectá-los. Se não for garantido poder suficiente, é possível que nenhum efeito seja detectado, por isso, deve-se calcular o tamanho amostral necessário, antes de iniciar qualquer estudo, para garantir o poder pretendido.\n\n\n\n\n\n\nFletcher, Robert H, Suzanne W Fletcher, e Grant S Fletcher. 2014. «Acaso». Em Epidemiologia Clínica: Elementos Essenciais, Quinta Edição, 108–9. Artmed Editora.\n\n\nGuyatt, Gordon, Roman Jaeschke, Nancy Heddle, et al. 1995. «Basic statistics for clinicians: 1. Hypothesis testing.» CMAJ: Canadian Medical Association Journal 152 (1). Canadian Medical Association: 27.\n\n\nKelen, Gabor D., Charles B. Brown, e James Ashton. 1988. «Statistical reasoning in clinical trials: hypothesis testing». Am J Emerg Med 1 (1). W.B. Saunders Company: 52–61.\n\n\nMenezes, Renée Xavier de, e Marcelo Nascimento Burattini. 2004a. «Testes de Hipótese e intervalos de Confiança». Em Métodos Quantitativos em Medicina, editado por Eduardo Massad, Renée Xavier de Menezes, Paulo Sérgio Panse Silveira, e Neli Regina Siqueira Ortega, 225–41. Barueri, São Paulo: Editora Manole Ltda.\n\n\n———. 2004b. «Testes de Hipótese e intervalos de Confiança». Em Métodos Quantitativos em Medicina, editado por Eduardo Massad, Renée Xavier de Menezes, Paulo Sérgio Panse Silveira, e Neli Regina Siqueira Ortega, 225–41. Barueri, São Paulo: Editora Manole Ltda.",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "13-testeHipoteses.html#footnotes",
    "href": "13-testeHipoteses.html#footnotes",
    "title": "13  Teste de Hipóteses",
    "section": "",
    "text": "Ter em mente que nunca se pode saber com total certeza se existe um efeito na população.↩︎\nEsta foi uma suposição inicial! Fingiu-se que os dados do arquivo dadosMater.xlsx com o filtro para as gestações a termo é a “população”↩︎\nPara ver todas as estatísticas do teste, basta escrever teste$ e apertar a tecla TAB do teclado e surgirá um menu para escolha.↩︎",
    "crumbs": [
      "Parte V - Inferência Estatística",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "14-teste-t.html",
    "href": "14-teste-t.html",
    "title": "14  Comparação entre duas médias",
    "section": "",
    "text": "14.1 Pacotes necessários para este capítulo\npacman::p_load(car, \n               effectsize, \n               flextable,\n               ggpubr, \n               ggsci, \n               knitr,\n               readxl, \n               rstatix, \n               tidyverse)",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "14-teste-t.html#teste-t-para-amostras-independentes",
    "href": "14-teste-t.html#teste-t-para-amostras-independentes",
    "title": "14  Comparação entre duas médias",
    "section": "14.2 Teste t para amostras independentes",
    "text": "14.2 Teste t para amostras independentes\nO teste t de amostras independentes é usado para comparar duas médias de amostras de grupos não relacionados. Isso significa que há pessoas diferentes fornecendo escores para cada grupo. O objetivo desse teste é determinar se as amostras são diferentes uma da outra.\n\n14.2.1 Dados usados neste capítulo\nSuponha que. em uma determinada Universidade, tenham sido coletadas as notas de Bioestatística de uma turma de 40 alunos. Estes dados estão aqui. Salve o mesmo no seu diretório de trabalho para a leitura dos dados.\n\n14.2.1.1 Leitura dos dados\nPara a leitura dos dados, será usada a função read_excel() incluída no pacote readxl, que precisa ser instalado e carregado. Os dados serão recebidos por um objeto que será denominado de dados:\n\ndados &lt;- readxl::read_excel(\"dados/dadosNotas.xlsx\") %&gt;% \n  mutate(sexo = factor(sexo,\n                       levels = c(1, 2),\n                       labels = c(\"Masculino\", \"Feminino\")))\n\nstr(dados)\n\ntibble [40 × 2] (S3: tbl_df/tbl/data.frame)\n $ notas: num [1:40] 63.1 76.3 57.7 66.9 73.1 70.3 63.6 75.7 73.5 83 ...\n $ sexo : Factor w/ 2 levels \"Masculino\",\"Feminino\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nObserva-se que existem 40 alunos, sendo 20 mulheres e 0 homens. A variável notas é uma variável numérica que corresponde a a nota centesimal e sexo é uma variável categórica.\n\n\n14.2.1.2 Exploração e resumo dos dados\nInicialmente, calcular a média e o desvio padrão da variável notas de acordo com sexo, usando a função group_by () e summarise do pacote dplyr\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr:: summarise(n = n(),\n                    media = mean(notas, na.rm = TRUE),\n                    dp = sd(notas, na.rm = TRUE),\n                    mediana = median(notas, na.rm = TRUE),\n                    Q1 = quantile(notas,0.25, na.rm = TRUE),    \n                    Q3 = quantile(notas, 0.75, na.rm = TRUE),\n                    me = 1.96 * dp/sqrt(n)) \nresumo\n\n# A tibble: 2 × 8\n  sexo          n media    dp mediana    Q1    Q3    me\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino    20  59.9  7.34    59.2  54.3  63.5  3.22\n2 Feminino     20  68.4  7.79    68.6  63.2  73.8  3.41\n\n\nA saída informa que a média das notas das mulheres é 59.9, bem acima das notas dos homens, mostrando uma diferença nos escores de -8.5. Parece que o desempenho das mulheres em Bioestatística é melhor do que o dos homens!\nAlém do resumo numérico, é interessante construir um gráfico do tipo boxplot (Figura 14.1), usando o pacote ggplot2 (veja Seção 8.3) para observar a distribuição dos dados:\n\nggplot2::ggplot(data = dados, aes(x = sexo, \n                                  y = notas, \n                                  fill = sexo)) + \n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  scale_fill_manual(values = c(\"cyan\",\"pink2\")) +\n  labs (x = \"Sexo\", \n        y = \"Notas\") + \n  theme_classic(base_size = 13) + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 14.1: Boxplot dos dados\n\n\n\n\n\nOs boxplot sugerem que as notas dos alunos diferem, de acordo o sexo.\n\n\n\n14.2.2 Definição das hipóteses estatísticas\nAs hipóteses comparam as médias dos dois grupos. Para um teste bicaudal, as hipóteses são escritas como:\n\\[\nH_{0}: \\mu_{F} = \\mu_{M}\n\\]\n\\[\nH_{1}: \\mu_{F} \\neq \\mu_{M}\n\\]\n\n\n14.2.3 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0.05. A distribuição t é dependente dos graus de liberdade, dados por:\nNo exemplo,\n\nn1 &lt;- resumo$n[1]\nn2 &lt;- resumo$n[2]\ngl &lt;- n1 + n2 - 2\ngl\n\n[1] 38\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl =38 para uma hipótese alternativa bicaudal é obtido com a função qt (p, df), onde \\(df = gl\\) e \\(p = 1 - \\alpha/2\\)\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round (qt((1-alpha/2), gl), 3)\ntc\n\n[1] 2.024\n\n\nPortanto, se\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\nt_{calculado}| \\ge t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\n\n\n14.2.4 Teste estatístico\nPara determinar se existe uma diferença estatisticamente significativa entre as médias das notas de dois grupos independentes, será usado o teste t para duas amostras independentes, também conhecido como teste t de Student, baseado na distribuição de mesmo nome.\n\n14.2.4.1 Lógica do teste t\nO teste t compara as médias de duas amostras independentes, usando o erro padrão como métrica da diferença entre essas médias. Quanto maior o valor de t , maior a probabilidade de que as amostras pertençam a grupos diferentes, ocorrendo nessas circunstâncias a rejeição da hipótese nula (Pagano e Kimberly 2000).\nCalcula-se o teste t com a seguinte equação:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{EP_{d}}\n\\]\nOnde \\(EP_d\\) é o erro padrão da diferença entre a médias \\(\\bar{x}_1 - \\bar{x}_2\\). Se a hipótese nula for verdadeira, as amostras foram retiradas da mesma população e, portanto, \\(\\mu_1 - \\mu_2 = 0\\). Assim, a equação fica:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{EP_d}\n\\]\nO erro padrão da diferença \\(\\bar{x}_1 - \\bar{x}_2\\) é calculado de maneiras diferentes:\n\nSe a variâncias nos dois grupos forem iguais, usa-se:\n\n\\[\nEP_d = \\sqrt{s_o^2(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\nOnde \\(s_o^2\\) é a variância combinada ou conjugada que é, simplesmente, a média ponderada das variância dos grupos:\n\\[\ns_0^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{(n_1 -1)+ (n_2-1)}\n\\] Quando os grupos têm o mesmo tamanho (\\(n_1 = n_2\\)), \\(s_o^2\\) é simplesmente a média aritmética da variância dos grupos:\n\\[\ns_0^2 = \\frac {s_1^2 + s_2^2}{2}\n\\]\n\\[\nEP_d = \\sqrt{\\frac{2 s_o^2}{n}}\n\\]\n\nSe as variâncias dos dois grupos forem diferentes:\n\n\\[\nEP_d = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\n\\]\nEsta explicação da lógica e dedução da estatística de teste serve para uma melhor compreensão de como o teste funciona, mas para executar um teste t não há necessidade disso, basta saber como encaminhar ao R e como interpretar o resultado fornecido por ele.\n\n\n14.2.4.2 Pressupostos do teste t\nO teste t assume que:\n\nAs amostras são independentes;\nDeve haver distribuição normal. Entretanto, quando as amostras são grandes (teorema do limite central), isso não é muito importante;\nExista homocedasticidade, ou seja, as variâncias dos grupos devem ser iguais.\n\nViolar o pressuposto de número 3 tem importância se os tamanhos dos grupos forem diferentes. Se os grupos tiverem o mesmo tamanho e a amostra for grande, este pressuposto torna-se menos importante, não preocupando muito se essa hipótese foi violada (Zimmerman 2004). O pressuposto tem mais importância em grupos pequenos e desiguais. Existe um teste, denominado teste t de Welch que corrige essa violação. É possível portanto, esquecer esse pressuposto e fazer o teste de Welch sempre.\nAvaliação da normalidade\nUma boa parte dos procedimentos estatísticos são testes paramétricos 1 com base na distribuição normal. Ou seja, se assume que a distribuição dos dados segue o modelo da distribuição normal. Se essa suposição não for atendida, a lógica por trás do teste de hipóteses pode ser violada.\nPode-se verificar a normalidade de maneira visual, observando o comportamento dos dados através de gráficos como o próprio boxplot (Figura 14.1), onde se observa que as medianas dos boxplots se encontra praticamente no centro das caixas. Outra forma, é o gráfico Q-Q (Figura 14.2). O gráfico QQ (ou gráfico quantil-quantil) desenha a correlação entre uma determinada amostra e a distribuição normal. Uma linha de referência de 45 graus também é plotada. Um gráfico Q-Q é um gráfico de dispersão criado plotando dois conjuntos de quantis um contra o outro. Se ambos os conjuntos de quantis vierem da mesma distribuição, observa-se os pontos formando uma linha aproximadamente reta. Se os valores caírem na diagonal do gráfico, a variável é normalmente distribuída. Os desvios da diagonal mostram desvios da normalidade. Para desenhar um gráfico Q-Q pode ser usado a função ggqqplot ()2 do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%.\n\nggqqplot(dados, x = \"notas\", color = \"sexo\") +\n  labs(y = \"Notas\",\n       x = \"Quantis teóricos\")\n\n\n\n\n\n\n\nFigura 14.2: Gráficos Q-Q\n\n\n\n\n\nObservando os gráficos, verifica-se que a variável notas tem uma distribuição visualmente normal aceitável em ambas populações, pois o histograma se ajusta à curva normal e os gráficos Q-Q mostram que os dados seguem aproximadamente a linha diagonal.\nOutra maneira de analisar a normalidade é verificar se a distribuição como um todo se desvia de uma distribuição normal comparável. Para isso, usam-se testes estatísticos de normalidade. Os dois principais são o teste de Shapiro-Wilk e o teste de Kolmogorov-Smirnov (K-S).\nEsses testes comparam os dados da amostra com um conjunto de valores normalmente distribuídos com a mesma média e desvio padrão. Se o teste não for significativo (P &gt; 0,05), informa-se que a distribuição da amostra não é significativamente diferente de uma distribuição normal. Se, no entanto, o teste for significativo (P \\(\\le\\) 0,05), a distribuição em questão será significativamente diferente de uma distribuição normal.\nO método de Shapiro-Wilk é amplamente recomendado para teste de normalidade (Razali, Wah, et al. 2011), (Ghasemi e Zahediasl 2012), (Yap e Sim 2011).\n\nsw &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;%\n  rstatix::shapiro_test(notas)\nsw\n\n# A tibble: 2 × 4\n  sexo      variable statistic     p\n  &lt;fct&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino notas        0.972 0.802\n2 Feminino  notas        0.973 0.812\n\n\nA saída mostra que ambos valores P do teste, 0.802 e 0.812, estão acima de 0,05, corroborando com a não rejeição da normalidade dos dados.\nHomogeneidade da Variância\nNa visualização da Figura 14.1, nos dois grupos de alunos, observa-se que há, entre os limites inferior e superior, uma dispersão das notas em torno da região central. Esta dispersão parece ser semelhante nos grupos. Isto sugere que haja homogeneidade das variâncias.\nPortanto, homogeneidade da variância é o pressuposto de que a dispersão das medidas é aproximadamente igual em diferentes grupos de casos, ou que a dispersão dos valores são aproximadamente iguais em pontos diferentes da variável preditora.\nAlém do aspecto visual, a homogeneidade da variância pode ser testada com o teste de Levene. Neste teste, a \\(H_{0}\\) é todas as variâncias são iguais. No R, a função que calcula o teste é leveneTest() do pacote car (Fox e Weisberg 2019). Os argumentos são:\n\ny \\(\\to\\) variável de resposta para o método padrão ou um objeto lm ou fórmula. Se y for um objeto de modelo linear ou uma fórmula, as variáveis do lado direito do modelo devem ser todas fatores e devem ser completamente cruzadas;\ngroup \\(\\to\\) fator que define os grupos;\ncenter \\(\\to\\) O nome de uma função para calcular o centro de cada grupo; mean fornece o teste de Levene original; o padrão, median, fornece um teste mais robusto;\ndata \\(\\to\\) conjunto de dados para avaliar a formula.\n\n\nlevene &lt;- car::leveneTest(notas~sexo, \n                          center = mean, \n                          data = dados)\nlevene\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  0.4575 0.5029\n      38               \n\n\nA saída do teste de Levene retorna um valor p &gt; 0,05, confirma a impressão visual dos boxplots de que os grupos têm homogeneidade das variâncias, portanto a hipótese nula de igualdade das variâncias não pode ser rejeitada.\nUm outro teste que compara duas variância poderia ser usado. É o teste F que pode ser calculado com a função var.test() do pacote stats, incluído no R base. Seus argumentos pode ser consultados na ajuda do R.\n\nteste.Var &lt;- var.test(notas~sexo, alternative = \"two.sided\" , data = dados)\nteste.Var\n\n\n    F test to compare two variances\n\ndata:  notas by sexo\nF = 0.88701, num df = 19, denom df = 19, p-value = 0.7966\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3510912 2.2409994\nsample estimates:\nratio of variances \n         0.8870148 \n\n\nA saída do teste permite uma conclusão igual ao teste de Levene, pois o valor p = 0.7966.\n\n\n14.2.4.3 Execução do teste t de Student\nOs pressupostos do teste não foram violados, portanto ele pode ser realizado com confiança. Será utilizado a função t_test() do pacote rstatix (Kassambara 2022a) para calcular o teste t para amostras independentes. Ele fornece uma estrutura compatível com operador pipe %&gt;% (pipe-friendly) para executar testes t de uma e duas amostras. Para consultar os argumentos, consulte a Seção 13.5.3 ou a ajuda do RStudio.\n\n teste &lt;- dados %&gt;% rstatix::t_test(formula = notas ~ sexo,\n                                    detailed = TRUE,\n                                    var.equal = TRUE)\n teste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1 group2    n1    n2 statistic       p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    -8.51      59.9      68.4 notas Mascu… Femin…    20    20     -3.56 0.00102\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída retorna a estimativa da diferença média (-8.515), as estimativas das médias dos grupos (arredondadas), a estatística do teste (-3.5586328) o valor P (0.00102), graus de liberdade (38)3 e outras métricas.\nTambém é possível ver os resultados do teste t , usando o objeto teste que os recebeu. Por exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa da diferença entre as médias.\n\nIC95 &lt;- round(c(teste$conf.low, teste$conf.high),3)\nIC95\n\n[1] -13.359  -3.671\n\n\n\n\n\n14.2.5 Conclusão\nComo \\(|t_{calculado}|\\) = -3.559 &gt; \\(|t_{0,05;58}|\\) = 2.024, rejeita-se \\(H_{0}\\). Observa-se que o valor P é muito pequeno (0.00102) e, portanto, a diferença observada nas médias dos dois grupos deve ser assumida como significativa.\nAssim, pode-se admitir que as médias das notas são diferentes, com probabilidade de erro extremamente pequena. A estimativa da diferença média (\\(\\mu_1 - \\mu_2\\)) é fornecida pelo intervalo de confiança de 95% (-13.359, -3.671). Observe que o valor zero não está contido no intervalo e isto confirma a não significância estatística da diferença.\nConcluindo, as notas de Bioestatística das mulheres e as notas dos homens são diferentes, a diferença (\\(\\mu_1 - \\mu_2\\)) encontrada é estatisticamente significativa (t = -3.559, gl = 38, P = 0.00102), com uma confiança de 95%.\nEsta conclusão pode ser visualizada em um gráfico (Figura 14.3) que exibirá a saída do teste t:\n\nConstruir dois boxplots, usando o ggplot2 com cores do New England Journal of Medicine (NEJM), do pacote ggsci . Atribuir a um objeto bp:\n\n\nbp &lt;- ggplot(dados, aes(x=sexo, y=notas)) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1)+\n    geom_boxplot(aes(fill = sexo),\n                 color = \"black\")+\n    scale_color_nejm() +\n    theme_classic(base_size = 13) +\n    theme(legend.position=\"none\")\n\n\nAdicionar ao boxplot novos rótulos e os testes realizados:\n\n\nbp +\n labs(x = \"Sexo\", \n      y = \"Notas\", \n      title = \"Notas de Bioestatística\",\n      subtitle = rstatix::get_test_label(stat.test = teste,\n                                           correction = \"none\",\n                                           detailed = TRUE,\n                                           type = \"expression\",\n                                           p.col = \"p\"))\n\n\n\n\n\n\n\nFigura 14.3: Boxplots comparando os dois grupos\n\n\n\n\n\n\n\n14.2.6 Tamanho do Efeito\nA significância estatística deve ter uma atenção relativa do pesquisador, pois ela apenas mede a probabilidade de rejeitar uma hipótese nula, uma vez que ela seja verdadeira. Ajudam a determinar, em uma pesquisa, a significância dos resultados encontrados em relação à hipótese nula, mas não informam nada em relação a magnitude do efeito. Por exemplo, mostra se determinado tratamento afeta as pessoas, mas não dizem quanto isso as afeta.\nO tamanho do efeito (effect size) é uma medida quantitativa da magnitude do efeito. Quanto maior o tamanho do efeito, mais forte é a relação entre duas variáveis. É possível observar o tamanho do efeito ao comparar dois grupos quaisquer para ver quão substancialmente diferentes eles são.\nNormalmente, em ensaios clínicos tem-se um grupo de tratamento e um grupo de controle. O grupo de tratamento é uma intervenção que se espera efetue um resultado específico. O valor do tamanho do efeito mostrará se a terapia teve um efeito pequeno, médio ou grande. Isso tem mais relevância do que simplesmente informar o tamanho do valor P.\n\n14.2.6.1 d de Cohen\nTambém conhecida como diferença média padronizada, o d de Cohen(Cohen 1988) (Lindenau e Guimaraes 2012) é uma medida adequada e bastante popular para encontrar a magnitude do efeito na comparação entre duas médias.\nPara calcular a diferença média padronizada se verifica a diferença entre as médias dos dois grupos e se divide pelo desvio padrão conjugado:\n\\[\nd = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_{o}}\n\\]\nOnde,\n\\[\ns_o =\\sqrt \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2 - 2}\n\\]\nVoltando ao exemplo das notas dos alunos de Bioestatística, o d de Cohen é calculado, usando a função cohensD() do pacote lsr que usa os seguintes argumentos:\n\nx \\(\\to\\) um vetor numérico de valores de dados, variável preditora;\ny \\(\\to\\) um vetor numérico de valores de dados, variável resposta;\nformula \\(\\to\\) Fórmula na forma variável resposta ~ grupo;\ndata \\(\\to\\) dataframe ou matriz;\nmethod \\(\\to\\) Qual versão da estatística d devemos calcular? Os valores possíveis são pooled(padrão), x.sd, y.sd, corrected, raw, paired e unequal.;\nmu \\(\\to\\) O valor “nulo” contra o qual o tamanho do efeito deve ser medido. Quase sempre é 0 (padrão); raramente especificado.\n\nAssim, o d de Cohen pode ser obtido da seguinte forma:\n\nd &lt;- lsr::cohensD (notas ~ sexo, data = dados)\nd\n\n[1] 1.125339\n\n\nBastante simples! Agora, como interpretar este resultado de d = 1,3 (arredondado)? Sua interpretação não é intuitiva, recomenda-se usar a Tabela 14.1 para interpretar (Cohen 1988).\n\ndf &lt;- data.frame(d = c(\"&lt; 0,2\", \"0,2 &lt; 0.5\", \n                       \"0.5 &lt; 0.8\", \"&gt;= 0,8 \"),\n                 sig = c(\"insignificante\", \"pequeno\", \n                         \"médio\", \"grande\"))\n\nminha_tab &lt;- flextable(df) %&gt;%\n  set_header_labels(\n    d = \"d de Cohen\",\n    sig = \"Interpretação\") %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  width(j = 1:2, width = 1.5) %&gt;%\n  align(align = \"left\", part = \"header\") %&gt;%\n  align(align = \"left\", part = \"body\") %&gt;%\n  bold(part = \"header\") \n\nminha_tab\n\n\n\nTabela 14.1: Tamanho do Efeito\n\n\n\nd de CohenInterpretação&lt; 0,2insignificante0,2 &lt; 0.5pequeno0.5 &lt; 0.8médio&gt;= 0,8 grande\n\n\n\n\n\nAssim, as notas dos alunos diferem significativamente (P &lt; 0,0001) de acordo com a sexo, sendo que as mulheres têm notas mais altas do que os homens e a magnitude dessa diferença é grande (d = 1.13).",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "14-teste-t.html#teste-t-para-grupos-pareados",
    "href": "14-teste-t.html#teste-t-para-grupos-pareados",
    "title": "14  Comparação entre duas médias",
    "section": "14.3 Teste t para grupos pareados",
    "text": "14.3 Teste t para grupos pareados\nUm teste t pareado é usado para estimar se as médias de duas medidas relacionadas são significativamente diferentes uma da outra. Esse teste é usado quando duas variáveis contínuas são relacionadas porque são coletadas do mesmo participante em momentos diferentes (antes e depois), de locais diferentes na mesma pessoa ao mesmo tempo ou de casos e seus controles correspondentes.\n\n14.3.1 Dados usados nesta seção\nO banco de dados é constituído por uma amostra de 15 escolares portadores de asma não controlada. Fizeram avaliação da sua função pulmonar no início do uso de um novo corticoide inalatório. Após 60 dias, repetiram a avaliação da função pulmonar4.\nPara baixar o banco de dados, clique aqui. Faça o downloado para o seu diretório de trabalho.\n\n14.3.1.1 Leitura e transformação dos dados\nLeia o arquivo dadosPar.xlsx a partir do diretório de trabalho, usando a função read_excel() do pacote readxl. Atribuir os dados a um objeto com o nome dados.\n\ndados &lt;- readxl::read_excel(\"dados/dadosPar.xlsx\")\n\nA estrutura dos dados podem ser visualizada, usando a função str():\n\nstr(dados)\n\ntibble [15 × 3] (S3: tbl_df/tbl/data.frame)\n $ id   : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ basal: num [1:15] 1.3 1.47 2.06 1.95 1.47 1.13 1.48 0.94 1.05 0.87 ...\n $ final: num [1:15] 1.53 1.63 2.35 2.7 2.01 1.53 1.66 1.59 1.5 1.61 ...\n\n\nO dataframe dados encontra-se no formato amplo (wide), ou seja, com as colunas basal e final colocadas lado a lado como se fossem duas variáveis distintas, quando, na realidade, constituem-se em apenas uma variável contendo as medidas de VEF1 (Volume Expiratório Forçado no primeiro segundo).\nA função pivot_longer() do pacote tidyr fará a transformação do formato amplo para o longo (long). Este processo não é obrigatório, mas será realizado para fins de treinamento. O novo banco de dados será atribuído ao objeto dadosL. A função pivot_longer() necessita dos seguintes argumentos:\n\ndados \\(\\to\\) dataframe a ser pivotado, tranformado;\ncols \\(\\to\\) colunas a serem transformadas no formato longo;\nnames_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos nomes das colunas de dados;\nvalues_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos valores das células;\n… \\(\\to\\) possui outros argumento. Ver ajuda.\n\n\ndadosL &lt;- dados %&gt;% \n  tidyr::pivot_longer(c(basal, final), \n                      names_to = \"momento\",\n                      values_to = \"medidas\")\nstr(dadosL)\n\ntibble [30 × 3] (S3: tbl_df/tbl/data.frame)\n $ id     : num [1:30] 1 1 2 2 3 3 4 4 5 5 ...\n $ momento: chr [1:30] \"basal\" \"final\" \"basal\" \"final\" ...\n $ medidas: num [1:30] 1.3 1.53 1.47 1.63 2.06 2.35 1.95 2.7 1.47 2.01 ...\n\n\n\n\n14.3.1.2 Medidas Resumidoras\nPara resumir as variáveis, serão usadas as funções group_by() e summarise() do pacote dplyr, aplicadas ao formato longo dadosL:\n\nresumo &lt;- dadosL %&gt;% \n  dplyr::group_by(momento) %&gt;% \n  dplyr::summarise(n = n (),\n                   media = mean(medidas, na.rm = TRUE),\n                   dp = sd (medidas, na.rm = TRUE),\n                   mediana = median (medidas, na.rm = TRUE),\n                   IIQ = IQR (medidas, na.rm =TRUE),\n                   ep = dp/sqrt(n),\n                   me = ep * qt(1 - (0.05/2), n - 1)) \nresumo\n\n# A tibble: 2 × 8\n  momento     n media    dp mediana   IIQ    ep    me\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 basal      15  1.31 0.427    1.26  0.48 0.110 0.236\n2 final      15  1.69 0.471    1.59  0.38 0.122 0.261\n\n\n\n\n14.3.1.3 Visualização dos dados\n\nGráficos\n\nApenas, por uma questão didática, serão apresentadas três maneiras de mostrar os dados visualmente. Podem ser usados qualquer um dos tipos a seguir, pois todos dão, praticamente, a mesma informação.\nGráfico de barra de erro\n\n\n\n\n\n\n\n\nFigura 14.4\n\n\n\n\n\nNesse gráfico (Figura 14.4), a altura da barra representa a média do Volume Forçado em 1 seg (VEF1) nos diferentes momentos (basal e final). O erro corresponde a margem de erro (me) a partir do ponto (média), ou seja, é o intervalo de confiança de 95%. O limite inferior do IC95% foi suprimido.\nBoxplot\n\n\n\n\n\n\n\n\nFigura 14.5\n\n\n\n\n\nA altura da caixa dos boxplots (Figura 14.5)) é o intervalo interquartil (IIQ) e corresponde a 50% dos dados. A linha que corta horizontalmente a caixa é a mediana. Os bigodes da caixa (whiskers) em suas extremidades são os limites inferior e superior dos dados, excluindo os valores atípicos (outliers), representado no boxplot final por um ponto vermelho, acima do limite superior. Os pontos em vermelho (dentro das caixas) representam as médias.\nGráfico de linha\n\n\n\n\n\n\n\n\nFigura 14.6\n\n\n\n\n\nEste gráfico de linha (Figura 14.6) com representação da margem de erro tem a mesma interpretação do gráfico de barra de erro. A escolha do tipo de gráfico depende da ênfase do autor sobre os dados.\n\n\n14.3.1.4 Criação de uma variável que represente a diferença entre as médias\nA diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados):\n\n#|message: false \n#|warning: false\n\ndados$D &lt;- dados$basal - dados$final\n\nhead (dados)\n\n# A tibble: 6 × 4\n     id basal final     D\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.3   1.53 -0.23\n2     2  1.47  1.63 -0.16\n3     3  2.06  2.35 -0.29\n4     4  1.95  2.7  -0.75\n5     5  1.47  2.01 -0.54\n6     6  1.13  1.53 -0.4 \n\n\n\n\n\n\n\n\nAtenção\n\n\n\nO banco de dados, agora, apresenta uma nova variável D, pois o foco do teste t pareado é essa diferença entre as médias, basal e final, a média das diferenças.\n\n\nResumo da variável D\nAo resumo será atribuído ao nome sumario (sem acento):\n\nresumoD &lt;- dados %&gt;% \n  dplyr::summarise(media = mean (D),\n                   dp = sd (D),\n                   mediana = median (D),\n                   IIQ = IQR (D),\n                   min = min (D),\n                   max = max (D))\nresumoD\n\n# A tibble: 1 × 6\n   media    dp mediana   IIQ   min     max\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 -0.377 0.218   -0.38 0.285 -0.75 -0.0400\n\n\nExiste uma diferença de 0.38L entre o VEF1 basal e o final. A pergunta que se faz é: Esta diferença tem significância estatística? Os gráficos sugerem que sim!\n\n\n\n14.3.2 Definição das hipóteses estatísticas\nSerá usado um teste bicaudal. Se a intervenção não produz efeito, então:\n\\[\nH_0: \\mu_D = 0  \n\\] Se a intervenção produz efeito, então:\n\\[\nH_1: \\mu_D \\neq 0\n\\]\n\n\n14.3.3 Regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição t que é dependente dos graus de liberdade. O número de graus de liberdade á igual ao número de observações menos 1, neste caso são o número de pares menos 1.\n\nn &lt;- length(dados$D)\ngl &lt;- n - 1\ngl\n\n[1] 14\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl = 14 para uma hipótese alternativa bicaudal:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\nround(qt(p, 14), 3)\n\n[1] 2.145\n\n\nPortanto, se\n\\[\n\\mid t_{calculado}\\mid &lt; \\mid t_{crítico}\\mid -&gt; não \\quad rejeitar \\quad H_{0} \\\\ \\mid t_{calculado}\\mid &gt; \\mid t_{crítico}\\mid -&gt; rejeitar \\quad H_{0}\n\\]\n\n\n14.3.4 Teste estatístico\n\n14.3.4.1 Lógica do teste\nA estatística do teste t dependente é a mesma do teste t independente r dada por:\n\\[\nT = \\frac{\\bar{D} - \\mu_{D}}{EP_{D}}\n\\]\nComo na equação do teste t para amostras independentes, sob a hipótese nula igual a zero, \\(\\mu_{D} = 0\\), assim, a equação fica:\n\\[\nT = \\frac{\\bar{D}}{EP_{D}}\n\\]\nA estimativa do erro padrão das diferenças é dada por:\n\\[\nEP_{D}=\\frac{s_{D}}{\\sqrt{n}}\n\\]\nO desvio padrão das diferenças, \\(s_{D}\\) , é dado por:\n\\[\ns_{D}=\\sqrt\\frac{\\Sigma(D_{i} - \\bar{D})^2}{n - 1}\n\\]\nOnde \\(D_{i}\\) são as diferença individuais (\\(x_1 - y_1, x_2 - y_2, ..., x_n - y_n\\)).\nDa mesma maneira que no teste t para grupos independentes, essa demonstração serve para uma melhor compreensão de como o teste funciona, mas para executar este teste t não há necessidade disso, basta saber como encaminhar ao R, como será visto adiante.\n\n\n14.3.4.2 Pressupostos do teste\nO teste t pareado assume que os seguintes pressupostos devem ser atendidos:\n\nOs dados devem ser dependentes;\nA variável desfecho deve estar em uma escala contínua;\nAs diferenças entre os pares devem ter distribuição normal.\n\nAo usar um teste t pareado, a variação entre os pares de medidas é a estatística mais importante e a variação entre os participantes, como no teste t de duas amostras independentes, é de pouco interesse, não havendo necessidade de se verificar se as variâncias dos grupos são iguais.\nPara testar o pressuposto de normalidade das diferenças, usa-se a variável criada da diferença entre os pares, D. Verifica-se a normalidade dessa variável com o teste Shapiro-Wilk, usando a função shapiro_test() do pacote rstatix, já usada no teste t de amostras independentes.\n\nshapiro &lt;- dados %&gt;% \n   rstatix::shapiro_test(D)\n shapiro\n\n# A tibble: 1 × 3\n  variable statistic     p\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 D            0.942 0.410\n\n\nO teste de Shapiro-Wilk retorna um valor P &gt; 0,05, mostrando que a variável D que não se pode rejeitar a hipóteses nula de sua normalidade.\nAlém disso, um gráfico Q-Q (Figura 14.7) pode ser usado para avaliar a normalidade, com a função ggqqplot() do pacote ggpubr (Kassambara 2022b) que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%\n\n\n\n\n\n\n\n\nFigura 14.7: Gráfico QQ para avaliar a normalidade\n\n\n\n\n\nOs resultados do teste de Shapiro-Wilk e ográfico QQ, mostram que a \\(H_{0}\\) de normalidade da variável D não é rejeitada, apesar de haver uma pequena assimetria à esquerda que não impede o prosseguimento da análise.\n\n\n14.3.4.3 Execução do teste estatístico\nO cálculo do teste t pareado pode usar a mesma função do teste t para amostras independentes, t_test(), do pacote rstatix, mudando o argumento paired =FALSE(padrão) por paired =TRUE. Assim:\n\nteste_par &lt;- dadosL %&gt;% \n  rstatix:: t_test(formula = medidas ~ momento,\n                   paired = TRUE,\n                   detailed = TRUE) \nteste_par\n\n# A tibble: 1 × 13\n  estimate .y.     group1 group2    n1    n2 statistic         p    df conf.low\n*    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.377 medidas basal  final     15    15     -6.70 0.0000102    14   -0.497\n# ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObserve que foi usado o conjunto de dados de formato longo (dadosL) para usar a fórmula (x ~ grupo). Da mesma maneira do que o teste t para amostras independentes, é possível ver os resultados do teste t , usando o objeto teste_par que os recebeu.\nPor exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa de diferença (D) entre as médias\n\nIC95 &lt;- round(c(teste_par$conf.low, teste_par$conf.high),3)\nIC95\n\n[1] -0.497 -0.256\n\n\n\n\n\n14.3.5 Conclusão\nConclui-se que o VEF1 dos escolares asmáticos se modificou significativamente entre o início e após 60 dias do uso de um novo medicamento com uma confiança de 95%. A diferença (\\(\\mu_{basal} - \\mu_{final}\\)) encontrada é estatisticamente significativa (t = -6.6969, gl = 14, P = 1.02^{-5}), com uma confiança de 95%.\nObserve que o intervalo de confiança de 95% da diferença de -0.38 está todo abaixo de zero (-0.497, -0.256), confirmando a significância.\n\n\n14.3.6 Tamanho do Efeito\nO tamanho do efeito pode ser determinado, também, com o teste d de Cohen, usando a função cohensD() do pacote lsr:\n\nd_par &lt;- lsr::cohensD (dados$basal, dados$final)\nd_par\n\n[1] 0.8379499\n\n\nDessa forma, o uso do novo corticoide inalatório modificou significativamente o VEF1 dos escolares asmáticos com o uso de um novo corticoide inalatório (P = 1.02^{-5}), mostrando um aumento deste e que a magnitude dessa diferença é grande (d = 0.84).\nOs resultados podem ser apresentados usando um gráfico de linha (Figura 14.8)), aproveitando o resultado da função t_test().\n\nresumo %&gt;% \n    ggplot2::ggplot(aes(x=momento, y=media, group=1)) +\n    geom_line(linetype ='dashed') +\n    geom_errorbar(aes(ymin=media - me, \n                      ymax=media + me), \n                  width=0.1,\n                  size = 1,\n                  col = c(\"cyan4\",\"cyan3\")) +\n    geom_point(size = 2) +\n   labs(title=\"Avaliação do Uso de Corticosteroide Inalatório\",\n       subtitle = rstatix::get_test_label(stat.test = teste_par,\n                                          correction = \"none\",\n                                          detailed = TRUE,\n                                          type = \"expression\"),\n       x=\"Momento\", \n       y = \"Volume Expiratório Forçado em 1 seg (L)\",\n       caption = \"d Cohen = 0,84\")+\n   theme_bw() + \n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 14.8: Gráfico de linha mostrando a diferença na resposta ao corticoide inalatório\n\n\n\n\n\n\n\n\n\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral sciences. 2nd Edition. Routledge.\n\n\nFox, John, e Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nGhasemi, Asghar, e Saleh Zahediasl. 2012. «Normality tests for statistical analysis: a guide for non-statisticians». International journal of endocrinology and metabolism 10 (2). Brieflands: 486.\n\n\nKassambara, Alboukadel. 2022a. rstatix: Pipe-Friendly Framework for Basic Statistical Tests. https://CRAN.R-project.org/package=rstatix.\n\n\n———. 2022b. «ggpubr:’ggplot2’ based publication ready plots [R package ggpubr version 0.5.0]». The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN). https://cloud.r-project.org/web/packages/ggpubr/index.html.\n\n\nLindenau, Juliana D, e Luciano Santos Pinto Guimaraes. 2012. «Calculating the Effect Size in SPSS». Revista HCPA 32 (3): 363–81. https://seer.ufrgs.br/hcpa.\n\n\nPagano, Marcello, e Gauvreau Kimberly. 2000. «Comparison of Two Means». Em Principles of Biostatistics, Second Edition, 262–72. CRC Press.\n\n\nRazali, Nornadiah Mohd, Yap Bee Wah, et al. 2011. «Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests». Journal of statistical modeling and analytics 2 (1): 21–33.\n\n\nYap, Bee Wah, e Chiaw Hock Sim. 2011. «Comparisons of various types of normality tests». Journal of Statistical Computation and Simulation 81 (12). Taylor & Francis: 2141–55.\n\n\nZimmerman, Donald W. 2004. «A note on preliminary tests of equality of variances». Br J Math Stat Psychol 57 (1). Wiley Online Library: 173–81.",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "14-teste-t.html#footnotes",
    "href": "14-teste-t.html#footnotes",
    "title": "14  Comparação entre duas médias",
    "section": "",
    "text": "Teste paramétricos são testes estatísticos que se baseiam nos padrões da distribuição populacional da variável em estudo, por exemplo, a distribuição normal é descrita por dois parâmetros – média e desvio padrão – que são suficientes para se conhecer as probabilidades. Os testes que não requerem a especificação da forma de distribuição da população, ou seja, têm distribuição livre, são denominados de não paramétricos.↩︎\nVeja também a Seção 10.3.1.2.↩︎\nSe as variâncias forem diferentes (var.equal = FALSE), o teste calcula os graus de liberdade pela fórmula de Welch, bem mais complicada.↩︎\nO autor entende e recomenda que esta metodologia “ante e depois” deve ser evitada, pois traz consigo importante vieses como regressão à média, efeito placebo ou nocebo, viés temporal, viés de mensuração, etc. Ver Seção 3.5 . O uso aqui tem objetivo didático de mostrar a lógica estatística.↩︎",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "15-anova.html",
    "href": "15-anova.html",
    "title": "15  Análise de Variância",
    "section": "",
    "text": "15.1 Pacotes necessários para este capítulo\npacman::p_load(car, dplyr, effectsize, flextable,\n               ggplot2, ggpubr, ggsci, RColorBrewer,\n               readxl, rstatix)",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "15-anova.html#anova-de-um-fator",
    "href": "15-anova.html#anova-de-um-fator",
    "title": "15  Análise de Variância",
    "section": "15.2 ANOVA de um fator",
    "text": "15.2 ANOVA de um fator\n\n15.2.1 Por que realizar uma ANOVA?\nA análise de variância (ANOVA) é um método estatístico para comparar as médias de três ou mais grupos, testando se existem diferenças significativas entre elas. Para analisar três ou mais médias, uma tendência intuitiva seria fazer comparações por pares, usando um teste t de amostras independentes. Com quatro grupos, por exemplo, é possível compará-los realizando seis testes, grupo 1 versus grupo 2, grupo 1 versus grupo 3, grupo 1 versus grupo 4, grupo 2 versus grupo 3, grupo 2 versus grupo 4 e grupo 3 versus grupo 4. Com 5 grupos o número de testes necessários seria igual a 10. Generalizando, a expressão combinatória, usada para calcular o número de combinações de n elementos distintos de um conjunto de k grupos, sem se importar com a ordem é igual a:\n\\[\n\\frac {k!}{n!(k-n)!}\n\\]\nOu, usando uma função do R , choose(k, n), facilmente se chega ao resultado. Portanto, para quatro grupos:\n\nchoose(4, 2)\n\n[1] 6\n\n\nFoi visto na Seção 13.5.2.1 que ao realizar um teste de hipótese podem ocorrer erros. Em geral, tolera-se aceitar um taxa de falsos positivos (erro tipo I) de até 5% (\\(\\alpha\\) = 0,05). Consequentemente, a probabilidade de um erro do tipo I não ocorrer para cada teste t é de 0,95 (isto é, 1 – 0,05). Como os três testes são independentes, a probabilidade de um erro do tipo I não ocorrer nos seis testes é de \\((0,95)^6 = 0,735\\). Dessa maneira, a probabilidade de ocorrer pelo menos um erro do tipo I nos seis testes t de duas amostras é de 1 – 0,735 ou 0,265 (26,5%), o que é mais alto do que o nível de significância definido de 0,05 (Field, Miles, e Field 2012a). Ou seja, realizar múltiplos testes t, infla o erro tipo I.\nPor essa razão, a ANOVA de um fator é usada para verificar as diferenças entre vários várias médias dentro de um fator, reduzindo assim o número de comparações de pares e a probabilidade de ocorrer um erro tipo I.\n\n\n15.2.2 Lógica do Modelo da ANOVA\nO procedimento de ANOVA é utilizado para testar a hipótese nula de que as médias de três 1 ou mais populações são as mesmas contra hipótese alternativa de que nem todas as médias são iguais.\nNa Seção 14.2.4.2, foram comparadas duas variâncias, usando um teste, denominado de teste F. Este teste, é uma razão entre duas variâncias e recebeu este nome em homenagem a Sir Ronald Aylmer Fisher (veja a Seção 1.2). A variância é uma medida de dispersão que mensura como os dados estão espalhados em torno da média (veja Seção 6.5.3 ) . Quanto maior o seu valor, maior a dispersão.\nConsidere a Figura 15.1, onde está representada a distribuição de uma variável X em três grupos independentes. Pode-se, claramente, distinguir observações provenientes dessas distribuições, pois a sobreposição delas é pequena. Cada uma dela se dispersa pouco em torno da média.\n\n\n\n\n\n\n\n\nFigura 15.1: Três distribuições diferentes\n\n\n\n\n\nAgora, observe o Figura 15.2, onde a distribuição da variável X é mostrada, mantendo as mesmas médias, mas com variâncias maiores. Isto torna claro que se o objetivo é distinguir observações provenientes desses grupos não basta avaliar suas médias, há necessidade de comparar a variação entre os grupos com a variação dentro de cada grupo (Menezes 2004).\n\n\n\n\n\n\n\n\nFigura 15.2: Distribuições com mesmas médias da figura anterior, mas variâncias maiores\n\n\n\n\n\nSe a variação entre os grupos for grande quando comparada à variação dentro de cada grupo, aumenta a probabilidade de reconhecer a proveniência das observações (Figura 15.1). Entretanto, se a variação entre os grupos for pequena comparada à variação dentro do grupo, torna difícil a distinção de observações provenientes dos grupos (Figura 15.2).\nPortanto, a ANOVA de uma via se baseia na comparação da variância entre grupos com a variância dentro dos grupos, utilizando as Somas dos Quadrados (SQ) e os graus de liberdade (gl) para calcular os Quadrados Médios (QM). A razão entre a o Quadrado Médio entre os grupos (QME) e a Quadrado Médio dentro dos grupos (QMD) resulta na estatística F, que é comparada com um valor crítico da distribuição F para determinar a significância estatística da diferença entre as médias dos grupos.\n\\[\nF = \\frac{\\text{variância entre os grupos}}{\\text{variância dentro dos grupos}}\n\\]\nQuando o valor de F fica próximo de 1, significa que as variâncias são muito próximas; quando F é significativamente maior do que 1, é possível distinguir os indivíduos de diferentes grupos. Ou seja, se o objetivo for mostrar que as médias são diferentes, será bom que a variância dentro dos grupos seja baixa. Pode-se pensar na variância dentro do grupo como o ruído que pode obscurecer a diferença entre os sons (as médias). No gráfico da Figura 15.1, o valor de F seria grande; no da Figura 15.2, seria pequeno.\nComo saber se o valor de F é grande o suficiente? Um único valor F é difícil de interpretar sozinho. Há necessidade de colocá-lo em um contexto maior antes que seja possível interpretá-lo. Para fazer isso, usa-se a distribuição F para calcular as probabilidades.\n\n\n15.2.3 Distribuição F\nA distribuição F, também conhecida como distribuição de Fisher-Snedecor, é uma distribuição de probabilidade contínua fundamental na estatística inferencial. Sua principal aplicação é para comparar variâncias de duas ou mais populações normais.\nA razão entre a variabilidade entre os grupos e a variabilidade dentro do grupo segue uma distribuição F quando a hipótese nula é verdadeira. Quando se realiza uma ANOVA com um fator obtém-se um valor F. No entanto, se forem extraídas várias amostras aleatórias do mesmo tamanho da mesma população e fosse repetida a mesma análise, o resultado seriam muitos valores F diferentes, constituindo uma distribuição amostral, denominada de distribuição F.\nA distribuição F assumindo que a hipótese nula é verdadeira, é possível colocar o resultado de qualquer valor F e determinar quão consistente ele é com a hipótese nula e calcular a probabilidade. A probabilidade que se quer calcular é a probabilidade de observar uma estatística F que é pelo menos tão alta quanto o valor que o estudo obteve. Essa probabilidade permite determinar quão comum ou raro é o valor F, sob a suposição de que a hipótese nula é verdadeira. Se a probabilidade for pequena o suficiente, pode-se concluir que dados são inconsistentes com a hipótese nula. Como já foi mostrado em outros momentos, essa probabilidade é o valor p (Seção 13.6).\nA forma da curva de distribuição F varia de acordo com dois graus de iberdade - um para o numerador (variância entre) e outro para o denominador (variância dentro). Cada combinação de graus de liberdade fornece uma curva de distribuição F diferente. As unidades de uma distribuição F são denotadas por F, que possui características importantes:\n\nDistribuição contínua: Como as distribuições normal, t e qui-quadrado (veja Seção 19.2), a distribuição F é uma distribuição contínua;\nValores positivos: A variável F só pode assumir valores maiores ou iguais a zero; A distribuição é limitada à esquerda por zero e não tem limite superior, estendendo-se indefinidamente à direita, o que reflete o fato de que as variâncias são sempre não negativas.\nAssimetria positiva: A distribuição F é assimetrica à direita, ou seja, tem uma cauda longa para o lado positivo, mas a assimetria diminui à medida que o número de graus de liberdade aumenta, conforme observado na Figura 15.3.\nParâmetros: A distribuição F é definida por seus graus de liberdade, o do numerador (gl1) e o do denominador (gl2)2.\n\n\n\n\n\n\n\n\n\nFigura 15.3: Distribuições F\n\n\n\n\n\n\n15.2.3.1 Distribuição F e os graus de liberdade\nNa Figura 15.3, observa-se que à medida que os graus de liberdade aumentam, a distribuição F realmente começa a se parecer com a distribuição normal, especialmente em termos de simetria e concentração em torno da média. Entretanto, há pormenores importantes sobre como isso acontece com o grau de liberdade do numerador (gl1) e com o grau de liberdade do denominador (gl2).\n\n\n\n\nTabela 15.1: Importância dos graus de liberdade\n\n\n\nGraus de LiberdadePapel na Distribuição FEfeito ao aumentargl1 (numerador)Relacionado ao número de grupos ou variáveisA curva fica menos assimétrica, mas ainda com cauda à direitagl2 (denominador)Relacionado ao tamanho da amostraA curva se aproxima mais rapidamente da normal\n\n\n\n\n\nPor que que isso acontece ?\nBom, é um pouco mais complicado!\nVamos lá, procurando tornar o mais simples possível3, acima (Seção 15.2.2) foi mostrado que a razão F na distribuição F é definida como a razão entre duas variâncias estimadas e variância (Seção 6.5.3) é a razão da soma dos quadrados (SQ) dividido pelos graus de liberdade, logo\n\\[\nF = \\frac{(SQ_1 / gl_1)}{(SQ_2 / gl_2)}\n\\]\nonde (SQ1) e (SQ2) são variáveis independentes com distribuições qui-quadrado (veja Seção 19.2) com graus de liberdade (gl1) e (gl2), respectivamente.\nComo a distribuição qui-quadrado se aproxima da normal quando os graus de liberdade aumentam, a razão entre elas (a distribuiçãoa F) também começa a se comportar como uma normal — especialmente quando gl2 é grande (Tabela 15.1) (Ferreira e Helms 2011).\nEsse comportamento é útil na prática porque permite usar testes que assumem normalidade quando os graus de liberdade são grandes, mesmo quando a distribuição original é mais complexa.\n\n\n15.2.3.2 Funções do R para trabalhar com a distribuição F\nNo R, existem quatro funções principaisque são ferramentas essenciais para trabalhar com a distribuição F. Elas seguem um padrão comum a muitas distribuições de probabilidade, e cada uma tem uma finalidade específica.\n\n15.2.3.2.1 Função de Densidade de Probabilidade (FDP)\nA função df() calcula a densidade de probabilidade de um valor específico em uma distribuição F. A densidade de probabilidade não é a probabilidade em si, mas sim a altura da curva da distribuição F em um determinado ponto.\nÉ usada principalmente para visualizar a forma da distribuição F ou para cálculos mais avançados que exigem a densidade em um ponto. Ela ajuda a entender a “concentração” de probabilidade em diferentes valores. Usa os seguintes argumentos:\n\nx : representa o valor na escala F, que é o valor que se obtém ao calcular a estatística de um teste F (Fobservado) em uma análise, como um teste ANOVA, por exemplo;\n\ndf1: são os graus de liberdade do numerador;\n\ndf2; são os graus de liberdade do denominador.\n\n\n# Exemplo: FPD de 2, com df1 = 4 e df2 = 50\n\ndf(x = 2, df1 = 4, df2 = 50)\n\n[1] 0.1512717\n\n\n\n\n15.2.3.2.2 Função de Distribuição Cumulativa (CDF)\nA função pf() calcula a probabilidade acumulada de uma distribuição F. Ela retorna a probabilidade de que uma variável aleatória F seja menor ou igual a um valor q (quantil).\nÉ a função mais usada para obter o valor p de um teste F. Dado um valor Fobservado (estatística de teste), você usa pf() para encontrar a probabilidade de obter um valor F igual ou mais extremo, o que é fundamental para tomar decisões sobre rejeitar ou não a hipótese nula. Usa os seguintes argumentos:\n\nq : O valor quantil para o qual se quer calcular a probabilidade acumulada;\ndf1: são os graus de liberdade do numerador;\ndf2; são os graus de liberdade do denominador;\nlower.tail = TRUE: (Padrão) Calcula a probabilidade da cauda inferior (\\(P(F \\le q)\\)). Usar FALSE para a cauda superior (\\(P(F \\gt q)\\)), que é o que geralmente se busca em testes de hipótese.\n\n\n# Exemplo: Probabilidade de F ser menor que 2, com df = 4 e df2 = 50\npf(q = 2, df1 = 4, df2 = 50)\n\n[1] 0.8911717\n\n# Exemplo: Probabilidade da cauda superior de F ser maior que 2 (para valor p)\npf(q = 2, df1 = 4, df2 = 50, lower.tail = FALSE)\n\n[1] 0.1088283\n\n\n\n\n15.2.3.2.3 Função Quantil\nA função qf() é o inverso de pf(). Ela recebe uma probabilidade p e retorna o valor quantil correspondente, ou seja, o valor q que tem uma probabilidade acumulada p.\nÉ usada para encontrar o valor crítico de uma distribuição F para um certo nível de significância (\\(\\alpha\\)). Esse valor crítico é a linha de corte que se compara com sua estatística de teste Fobservada para decidir sobre o resultado do teste. Usa os seguintes argumentos:\n\np : A probabilidade acumulada (geralmente, \\(1 − \\alpha\\) para testes de cauda superior);\ndf1: são os graus de liberdade do numerador;\ndf2; são os graus de liberdade do denominador;\nlower.tail = TRUE: (Padrão) Encontra o valor q tal que (\\(P(F \\le q) = p\\)).\n\n\n# Exemplo: Encontrar o valor crítico de F para um nível de significância de 5% (0.05) com df1 = 4 e df2 = 50.\nqf(p = 0.95, df1 = 4, df2 = 50) \n\n[1] 2.557179\n\n# ou \nqf(0.05, 4, 50, lower.tail = FALSE)\n\n[1] 2.557179\n\n\n\n\n15.2.3.2.4 Geração de Números Aleatórios\nA função rf() gera números aleatórios que seguem a distribuição F.\nÉ útil para simulações, modelagem estatística, testes de Monte Carlo4 ou para entender a forma da distribuição F gerando amostras aleatórias. Seus argumentos são:\n- n : O número de observações aleatórias que se deseja gerar.\n- df1: são os graus de liberdade do numerador;\n\ndf2; são os graus de liberdade do denominador.\n\n\n# Exemplo: Gerar 50 números aleatórios de uma distribuição F com df1 = 4 e df2 = 50\namostra_aleatoria &lt;- rf(n = 50, df1 = 4, df2 = 50)\namostra_aleatoria\n\n [1] 1.03254605 0.07423810 0.38808436 0.85862053 0.64626086 1.53283297\n [7] 1.52155218 0.51280536 0.46332826 1.14995416 0.45050318 0.53938024\n[13] 2.34822222 0.88096204 0.46777255 1.13408439 2.01287403 0.64195882\n[19] 0.99945591 1.37074671 0.43063735 1.06996363 0.41537716 0.64731567\n[25] 0.21281995 0.36433908 1.18968574 1.61093237 0.44171598 0.74843921\n[31] 0.99799396 0.50854663 0.19467111 0.81514029 1.79863264 2.16599254\n[37] 1.70116380 0.62189196 0.69087339 1.71874186 0.57677554 0.78933654\n[43] 0.46100641 2.52358645 0.09330181 1.82562446 2.37003004 0.17142583\n[49] 1.30149593 2.09201591\n\n\n\n\n\n\n\n\nExercício 1\n\n\n\nEssas funções são úteis para resolver problemas de probabilidade envolvendo a distribuição F. Por exemplo, qual é a probabilidade de uma variável aleatória F com 4 e 50 graus de liberdade no numerador e no denominador, respectivamente, ser menor que 1? Qual a função deve ser usada?\n\n\nResposta:\nPode-se usar a função pf() :\n\nq &lt;- 1\npf(q, df1=4, df2=50)\n\n[1] 0.5835786\n\n\nOu seja, ao se observar a curva da Figura 15.3 de cor verde (gl1 = 4 e gl2 = 50), a probabilidade abaixo de x = 1 é igual a 58,4%, arrendondando.\n\n\n\n\n\n\nExercício 2\n\n\n\nPara saber altura (densidade de probabilidade) da curva de cor verde quando x = 1, basta olhar na Figura 15.3, ou seja, ao redor de 0.50. Entretanto, é difícil saber o valor exato. O que fazer para obter este valor?\n\n\nResposta:\nCalcular a densidade de probabilidade com a função df()\n\nx &lt;- 1\ndf(x, df1=4, df2=50)\n\n[1] 0.5207772\n\n\n\n\n\n\n\n\nExercício 3\n\n\n\nQual o valor do nível crítico de F que deixa 50% da área da curva à esquerda, supondo-se os mesmos graus de liberdade anteriores?\n\n\nResposta:\nCalcular a com a função qf()\n\np &lt;- 0.50 \nFcrit &lt;- qf(p, df1 = 4, df2 = 50)\nFcrit\n\n[1] 0.8506612\n\n\nPara representar, graficamente, esse resultado, foi construido o gráfico da Figura 15.4 com a função ggolot2(). Verifica-se que a área sob a curva abaixo de 0,85 é igual a 50%.\n\n\n\n\n\n\n\n\nFigura 15.4: Área da curva da distribuição F (4,50) abaixo de x = 0,85 é igual a 50%\n\n\n\n\n\n\n\n\n\n\n\nExercício 4\n\n\n\nGerar 10.000 valores aleatórios de uma distribuição F (20, 100) e após plotar um histograma com curva da distribuição F sobreposta.\n\n\nResposta:\n\n# Parâmetros da distribuição F\ndf1 &lt;- 20\ndf2 &lt;- 100\n\n# Gerar 100.000 valores aleatórios da distribuição F\nset.seed(123)  # para reprodutibilidade\nvalores &lt;- rf(10000, df1, df2)\n\n\n# Criar histograma com densidade\ndf_dados &lt;- data.frame(F_valores = valores)\n\n# Curva teórica da distribuição F\nx_teorico &lt;- seq(0, max(valores), length.out = 500)\ny_teorico &lt;- df(x_teorico, df1, df2)\ndf_teorico &lt;- data.frame(x = x_teorico, y = y_teorico)\n\n# Gráfico com histograma e curva teórica\nggplot(df_dados, aes(x = F_valores)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = \"lightblue\", color = \"black\", alpha = 0.6) +\n  geom_line(data = df_teorico, aes(x = x, y = y), color = \"red\", size = 1.2) +\n  labs(\n    title = \"Histograma de valores simulados da distribuição F(20, 100)\",\n    subtitle = \"Com curva teórica sobreposta\",\n    x = \"Valor de F\",\n    y = \"Densidade\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\nFigura 15.5: Histograma com curva sobreposta de uma distribuição F (20,100)\n\n\n\n\n\nObservando o gráfico da Figura 15.5, verifica-se que a curva se assemelha muito com a curva normal (Seção 9.7).\n\n\n\n\n15.2.4 Cenário para a realização da ANOVA de um fator\nA análise de variância (ANOVA) de um fator, também conhecida como ANOVA de uma via, é uma extensão do teste t independente para comparar duas médias em uma situação em que há mais de dois grupos. Dito de outra forma, o teste t para uso com duas amostras independentes é um caso especial da análise de variância de uma via. A ANOVA de um fator compara o efeito de uma variável preditora (variável independente, fator) sobre uma variável contínua (desfecho). Por exemplo, verificar se a intensidade do tabagismo na gestação afeta o peso dos recém-nascidos Figura 15.6.\n\n\n\n\n\n\nContexto\n\n\n\nDiversos estudos (Schuh 2008) documentaram a associação entre tabagismo materno na gestação e redução do peso dos recém-nascidos. Em um estudo (Madi et al. 2010) foi registrado informações maternas e neonatais 1368 partos na maternidade do Hospital Geral de Caxias do Sul (HGCS), 2008.\n\n\n\n15.2.4.1 Dados do exemplo\nPara testar a hipótese de que a intensidade do tabagismo materno afeta o peso do recém-nascido, foram selecionadas as variáveis quantFumo (número de cigarros por dia) e pesoRN (peso ao nascer, em gramas) do banco de dados dadosMater.xlsx (Seção 5.6). A variável quantFumo foi categorizada em três níveis, formando a variável tabagismo, por meio das funções mutate() e case_when():\n\nNão fumantes: gestantes que relataram nunca ter fumado;\n\nFumantes leves: gestantes que fumavam até 10 cigarros por dia;\n\nFumantes intensas: gestantes que fumavam mais de 10 cigarros por dia.\n\nEm seguida, foram filtrados os recém-nascidos a termo5 (entre 37 e 41 semanas completas de gestação), e o resultado foi atribuído ao objeto dados. Como os grupos apresentavam tamanhos amostrais bastante desbalanceados6, foi realizada uma subamostragem aleatória7 de 120 gestantes não fumantes, de modo a equilibrar os três grupos para a análise. O conjunto final de dados balanceados foi atribuído ao objeto dados_anova.\n\ndados &lt;- readxl::read_excel(\"dados/dadosmater.xlsx\") %&gt;% \n  dplyr::select(quantFumo, pesoRN, sexo, ig) %&gt;%   \n  dplyr::mutate(tabagismo = case_when(\n                  quantFumo == 0 ~ \"Não\",\n                  quantFumo &gt;0 & quantFumo &lt;= 10 ~ \"Leve\",\n                  quantFumo &gt; 10  ~ \"Intenso\"),\n                tabagismo = factor(tabagismo,\n                                   levels = c(\"Não\", \"Leve\", \"Intenso\")),\n    sexo = factor(sexo,\n                  levels = c(1, 2),\n                  labels = c(\"Masc\", \"Fem\"))) %&gt;% \n  dplyr::filter(ig&gt;=37 & ig &lt; 42)\n\nstr(dados)\n\ntibble [1,085 × 5] (S3: tbl_df/tbl/data.frame)\n $ quantFumo: num [1:1085] 0 0 20 0 20 20 0 0 0 0 ...\n $ pesoRN   : num [1:1085] 3285 3100 3100 2800 3270 ...\n $ sexo     : Factor w/ 2 levels \"Masc\",\"Fem\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ig       : num [1:1085] 37 37 37 38 39 39 39 39 39 39 ...\n $ tabagismo: Factor w/ 3 levels \"Não\",\"Leve\",\"Intenso\": 1 1 3 1 3 3 1 1 1 1 ...\n\ntable(dados$tabagismo)\n\n\n    Não    Leve Intenso \n    853     121     111 \n\n\nCorrigindo em parte o desbalanceamento:\n\n# Subamostragem dos não fumantes\nset.seed(123)  # para reprodutibilidade\n\nnao_fumantes &lt;- dados %&gt;%\n  filter(tabagismo == \"Não\") %&gt;%\n  slice_sample(n=120)\n\n# Manter os outros grupos como estão\nleves &lt;- dados %&gt;% filter(tabagismo == \"Leve\")\nintensos &lt;- dados %&gt;% filter(tabagismo == \"Intenso\")\n\n# Unir os três grupos balanceados\ndados_anova &lt;- bind_rows(nao_fumantes, leves, intensos)\n\n# Manter apenas as variáveis necessárias\ndados_anova &lt;- dados_anova %&gt;% \n  dplyr::select(sexo, tabagismo, pesoRN)\n\nstr(dados_anova)\n\ntibble [352 × 3] (S3: tbl_df/tbl/data.frame)\n $ sexo     : Factor w/ 2 levels \"Masc\",\"Fem\": 1 1 1 2 1 2 1 1 1 1 ...\n $ tabagismo: Factor w/ 3 levels \"Não\",\"Leve\",\"Intenso\": 1 1 1 1 1 1 1 1 1 1 ...\n $ pesoRN   : num [1:352] 3110 2580 2965 3430 3060 ...\n\ntable(dados_anova$tabagismo)\n\n\n    Não    Leve Intenso \n    120     121     111 \n\n\n\n\n\n\n\n\nCUIDADO! Perda do Poder estatístico\n\n\n\nAo reduzir o grupo de não fumantes de 853 para 120, perde-se informação. E isso pode diminuir a precisão da estimativa da média desse grupo. Aumenta o risco de erro tipo II (veja Seção 13.5.2.1).\nEntretanto, como o objetivo é comparar grupos de forma justa, esse sacrifício é aceitável — especialmente em análises exploratórias ou didáticas.\n\n\n\n\n15.2.4.2 Exploração e resumo dos dados\nAs medidas resumidoras serão obtidas, usando as funções group_by () e summarise () do pacote dplyr.\n\nalpha = 0.05\nresumo &lt;- dados_anova %&gt;%\n  dplyr::group_by(tabagismo) %&gt;%\n  dplyr::summarise(n = n(),\n                   media = mean(pesoRN, na.rm = TRUE),\n                   dp = sd (pesoRN, na.rm = TRUE),\n                   ep = dp/sqrt(n),\n                   me = qt ((1-alpha/2), n-1)*ep,\n                   IC_Inf = media - me,\n                   IC_sup = media + me)\nresumo\n\n# A tibble: 3 × 8\n  tabagismo     n media    dp    ep    me IC_Inf IC_sup\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Não         120 3235.  461.  42.1  83.4  3152.  3319.\n2 Leve        121 3121.  444.  40.4  80.0  3041.  3201.\n3 Intenso     111 3045.  533.  50.6 100.   2944.  3145.\n\n\n\nmedia_geral &lt;- mean(dados_anova$pesoRN)\nround(media_geral, 0)\n\n[1] 3136\n\n\n\n\n15.2.4.3 Visualização gráfica dos dados\nOs gráficos de dispersão (Figura 15.6) são uma maneira interessante de visualizar os dados, principalmente com o pacote ggplot28 :\n\n\n\n\n\n\n\n\nFigura 15.6: Boxplots do impacto do tabagismo materno no peso ao nascer\n\n\n\n\n\nA Figura 15.6 exibe uma variabilidade pequena entre as médias dos grupos, comparada a média geral e uma grande variabilidade dentro dos grupos. Pensando na estatística F que avalia se a variância entre os grupos é substancialmente maior que a variância dentro dos grupos, pode-se esperar que o efeito do tabagismo sobre os pesos dos recém-nascidos não é tão acentuado.\n\n\n\n15.2.5 Definição das hipóteses estatísticas\nPara testar a igualdade entre as médias, \\(H_{0}: \\mu_{1} = \\mu_{2} =  \\mu_{3}\\), supondo homocedasticidade, isto é, que as variâncias sejam iguais.\nA hipótese alternativa, \\(H_1\\), diz que, pelo menos, uma das médias é diferente das demais. Ela não é unilateral ou bilateral, é multifacetada porque permite qualquer relação que não seja todas as médias iguais. Por exemplo, a \\(H_1\\) inclui o caso em que \\(μ_1=μ_2\\), mas \\(μ_3\\) tem um valor diferente.\n\n\n15.2.6 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), geralmente escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição F. O número de graus de liberdade total \\((n – 1)\\) é dividido em dois componentes:\n\nGrau de liberdade do numerador (ENTRE) é dado por \\(gl_{E} = k - 1\\), onde k é o número de grupos.\nGrau de liberdade do denominador (DENTRO ou residual) é dado por \\(gl_{D} = n - k\\), onde, \\(n = \\sum n_{i}\\).\n\nNo exemplo, para um \\(\\alpha = 0,05\\), tem-se:\n\nalpha &lt;- 0.05\nk &lt;-  length(resumo$media)\nn &lt;- nrow(dados_anova)\nglE &lt;-  k - 1\nglE\n\n[1] 2\n\nglD &lt;- n - k\nglD\n\n[1] 349\n\n\nCom esses dados, usando a a função qf()calcula-se o valor crítico de F (Figura 15.7) que é igual:\n\nFcrit &lt;- qf(1 - alpha, glE, glD)\nround(Fcrit, 2)\n\n[1] 3.02\n\n\nPortanto, se\n\\[\n\\begin{array}{l}\n|F_{calculado}| &lt; |F_{crítico}| \\Rightarrow \\text{não se rejeita } H_0 \\\\\n|F_{calculado}| \\ge |F_{crítico}| \\Rightarrow \\text{rejeita-se } H_0\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\nFigura 15.7: Curva da Distribuição F 3,196 = 2,65\n\n\n\n\n\n\n\n15.2.7 Teste Estatístico\nA estatística de teste é obtida calculando duas estimativas da variância populacional, \\(\\sigma^2\\): a variância entre os grupos (\\(s_{E}^2\\)) e a variância dentro dos grupos (\\(s_{D}^2\\)).\nA variância entre os grupos também é chamada de quadrado médio entre os grupos (\\(QM_{E}\\)) e é igual a soma dos quadrados entre (\\(SQ_{E}\\)) ou do fator dividida pelos graus de liberdade entre:\n\\[\nQM_{E} = \\frac{SQ_{E}}{gl_{E}}\n\\]\nA variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual (\\(QM_{D}\\)) e é igual a soma dos quadrados dentro dividida pelos graus de liberdade dentro:\n\\[\nQM_{D} = \\frac {SQ_{D}}{gl_{D}}\n\\]\nA variância entre os grupos, \\(QM_{E}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação entre as médias das amostras extraídas de diferentes populações. Para o exemplo das três categorias de tabagismo durante a gestação, o \\(QM_{E}\\) será baseado nos valores das médias dos pesos dos recém-nascidos nos três grupos diferentes. Se as médias de todas as populações em consideração forem iguais, as médias das respectivas amostras ainda serão diferentes, mas a variação entre elas deverá ser pequena e, consequentemente, espera-se que o valor do \\(QM_{E}\\) seja pequeno. No entanto, se as médias das populações consideradas não são todas iguais, espera-se que a variação entre as médias das respectivas amostras seja grande e, consequentemente, o valor de \\(QM_{E}\\) seja grande.\nA variância dentro das amostras, \\(QM_{D}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação dos dados de diferentes amostras. Para o exemplo das tr~es categorias de tabagismo durante a gestação, o \\(QM_{D}\\) será baseado nas médias individuais dos pesos dos recém-nascidos incluídos nas três amostras retiradas de três populações. O conceito de \\(QM_{D}\\) é semelhante ao conceito de desvio padrão conjugado ou agrupado, \\(s_{o}\\), para duas amostras.\nA estatística de teste é, como já mencionado, a razão das variâncias entre e dentro do grupo. Dessa maneira,\n\\[\nF = \\frac {s_{E}^2}{s_{D}^2} = \\frac {\\frac {SQ_{E}}{gl_{E}}}{\\frac {SQ_{D}}{gl_{D}}} = \\frac {QM_{E}}{QM_{D}}\n\\]\n\n15.2.7.1 Ajuste do modelo\nNo R, para ajustar um modelo ANOVA clássica com um fator categórico, pode-se usar a função aov() do R base9. É uma função simples e direta para modelos balanceados. Ela espera a chamada notação de fórmula, portanto, os dados são incluídos separando as duas variáveis de interesse separadas por \\(\\sim\\) (til) e os dados (dados_anova), onde as variáveis especificadas na fórmula, são encontradas.\n\nmodelo.aov &lt;- aov(pesoRN ~ tabagismo, dados_anova)  \nsummary(modelo.aov)\n\n             Df   Sum Sq Mean Sq F value Pr(&gt;F)  \ntabagismo     2  2136213 1068107   4.647 0.0102 *\nResiduals   349 80209539  229827                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA saída, liberada pela função summary(), é bem reduzida, relatando as informações específicas da Tabela da ANOVA, a estatística F junto com o valor p e os graus de liberdade, soma dos quadrados (Sum Sq) e quadrados médios (Mean Sq), que com frequência se necessita para o relatório do modelo.\nA variância entre os grupos também é chamada de quadrado médio entre os grupos e é igual à soma dos quadrados entre ou do fator dividida pelos graus de liberdade entre. A variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual e é igual à soma dos quadrados dentro dividida pelos graus de liberdade dentro.\nA ANOVA detectou um efeito significativo do fator, que neste caso é o tabagismo, o valor \\(F_{calculado} = 4,647 &gt; F_{crítico} = 3.02\\) e o valor p = 0,0102 (&lt; 0,05).\nPode-se simplesmente relatar isso e encerrar, mas é provável que se queira saber quais grupos diferem uns dos outros. Lembre-se de que não se pode apenas inferir isso a partir de uma visão dos dados, existem testes estatísticos para ajudar a entender as diferenças dos grupos, que serão posteriormente realizados, apos a avaliação dos pressupostos do modelo.\n\n\n15.2.7.2 Avaliação dos pressupostos do teste\nAo realizar um teste de ANOVA de um fator deve-se assumir que:\n\nExista normalidade dos resíduos (Altman 1991);\nExista homogeneidade de variâncias: os grupos devem ter variâncias semelhantes (homocedasticidade);\nAmostras aleatórias e independentes;\nTodos os grupos devem ter tamanho amostral adequado. Grupos com menos de 10 participantes são problemáticos por reduzirem a precisão da média. Na prática, deve-se evitar menos de 30 participantes. A relação entre os grupos não deve ser maior do que 1:4 Peat e Barton (2014);\nNão devem existir valores atípicos (outliers);\nA mensuração dos dados deve ser em nível intervalar ou de razão.\n\n\n15.2.7.2.1 Avaliação da normalidade\nVerifica-se a premissa de normalidade dos resíduos, usando o teste de Shapiro-Wilk e desenhando um gráfico de probabilidade normal (gráficos Q-Q).\nOs resíduos são prodizidos pelo modelo.aov e podem ser diretamente acessados, através dele, criando as variáveis resíduos e ajustados , no dataframe dados_anova, para uso na avaliação:\n\ndados_anova &lt;- dados_anova %&gt;%\n  mutate(residuos = residuals(modelo.aov),\n         ajustados = fitted(modelo.aov))\nstr(dados_anova)\n\ntibble [352 × 5] (S3: tbl_df/tbl/data.frame)\n $ sexo     : Factor w/ 2 levels \"Masc\",\"Fem\": 1 1 1 2 1 2 1 1 1 1 ...\n $ tabagismo: Factor w/ 3 levels \"Não\",\"Leve\",\"Intenso\": 1 1 1 1 1 1 1 1 1 1 ...\n $ pesoRN   : num [1:352] 3110 2580 2965 3430 3060 ...\n $ residuos : Named num [1:352] -125 -655 -270 195 -175 ...\n  ..- attr(*, \"names\")= chr [1:352] \"1\" \"2\" \"3\" \"4\" ...\n $ ajustados: Named num [1:352] 3235 3235 3235 3235 3235 ...\n  ..- attr(*, \"names\")= chr [1:352] \"1\" \"2\" \"3\" \"4\" ...\n\n\nO teste de Shapiro-Wilk, para avaliar a normalidade dos resíduos, é feito com a função shapiro_test() do pacote rstatix, usando um teste geral dos resíduos, pois dados têm tamanhos semelhantes e se quer validar a suposição global do modelo:\n\ndados_anova %&gt;%\n  shapiro_test(residuos)\n\n# A tibble: 1 × 3\n  variable statistic      p\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 residuos     0.993 0.0820\n\n\nPara o gráfico Q-Q (Figura 15.8), pode ser usado a função ggqqplot () do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao \\(IC_{95\\%}\\).\n\nggpubr::ggqqplot(\n  data = dados_anova,\n  x = \"residuos\",\n  color = \"steelblue\",\n  title = \"QQ Plot dos Resíduos\",\n  ylab = \"Quantis amostrais\",\n  xlab = \"Quantis amostrais\",\n  conf.int = TRUE)\n\n\n\n\n\n\n\nFigura 15.8: Gráficos Q-Q\n\n\n\n\n\nUma outra maneira de observar a distribuição dos resíduos é através de um histograma ( Figura 15.9):\n\nggplot(dados_anova, aes(x = residuos)) +\n  geom_histogram(bins = 15, \n                 fill = \"lightblue\", \n                 color = \"black\") +\n  labs(title = \"Histograma dos Resíduos\", \n       x = \"Resíduos\", y = \"Frequência\") +\n  theme_classic(base_size = 13)\n\n\n\n\n\n\n\nFigura 15.9: Histograma dos resíduos\n\n\n\n\n\nA saída do teste de Shapiro-Wilk um valor p = 0,082, ou seja, maior que 0.05 e, portanto, não há evidência de violação da normalidade. Os gráficos Q-Q e o histograma reforçam está conclusão, e pode-se assumir que os resíduos têm uma distribuição praticamente normal.\n\n\n15.2.7.2.2 Verificação da presença de outliers\nPode-se aqui, além de usar boxplots, usar a função a função identify_outliers() do pacote rstatix (Kassambara 2022) e, quando os grupos forem desbalanceados, usar a função by_group() do pacote dplyr para avaliar por nível de fator:\n\ndados_anova %&gt;% \n  rstatix::identify_outliers(residuos)\n\n# A tibble: 11 × 7\n   sexo  tabagismo pesoRN residuos ajustados is.outlier is.extreme\n   &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n 1 Masc  Não         2051   -1184.     3235. TRUE       FALSE     \n 2 Masc  Leve        4350    1229.     3121. TRUE       FALSE     \n 3 Fem   Leve        1715   -1406.     3121. TRUE       FALSE     \n 4 Fem   Leve        4620    1499.     3121. TRUE       FALSE     \n 5 Masc  Intenso     1440   -1605.     3045. TRUE       FALSE     \n 6 Masc  Intenso     1795   -1250.     3045. TRUE       FALSE     \n 7 Masc  Intenso     4410    1365.     3045. TRUE       FALSE     \n 8 Fem   Intenso     1895   -1150.     3045. TRUE       FALSE     \n 9 Fem   Intenso     4390    1345.     3045. TRUE       FALSE     \n10 Fem   Intenso     4225    1180.     3045. TRUE       FALSE     \n11 Fem   Intenso     1785   -1260.     3045. TRUE       FALSE     \n\n\nExistem 11 observações marcadas como is.outlier = TRUE, mas nenhuma como is.extreme = TRUE, o que indica que esses pontos estão fora dos limites interquartis (IIQ), mas não ultrapassam os limites extremos (&gt; 3×IIQ). São outliers moderados, não extremos — ainda assim, merecem atenção, verificando se esses pontos têm influência desproporcional no modelo .\nPara isso, pode-se usar medidas de influência (Armitage, Berry, e Matthews 2002), que são usadas na análise de regressão, pois a ANOVA pode ser encarada como um caso especial de regressão linear com variáveis categóricas. E como foi observado a presença de vários outliers eles devem ser avaliados., pois podem influenciar os coeficientes. Isto é particularmente importante quando se trabalha com poucos grupos ou grupos desbalanceados e também quando se tem mais de um fator e existe interação entre os fatores.\n\nDistância de Cook: avalia o impacto de cada ponto na estimativa dos coeficientes (Cook e Weisberg 1986). É calculada pela função cooks.distance() que exige um objeto da classe lm (linear model) e, portanto, o modelo deve ser refeito. Este modelo necessita, como o aov(), de uma fórmula (pesoRN~tabagismo) e dos dados:\n\nmodelo_lm &lt;- lm(pesoRN~tabagismo,data = dados_anova) \n\nplot(cooks.distance(modelo_lm), \n     type = \"h\", \n     main = \"\")\nabline(h = 1, col = \"red\", lty = 2) \n\n\n\n\n\n\n\nFigura 15.10: Distância de Cook\n\n\n\n\n\nValores acima de 1 (ou muito maiores que os demais) indicam alta influência. Como não existem valores acima de 110, a influência é pequena.\nAlavancagem (Leverage)\nAs estatísticas de alavancagem (ou valores hat) medem a influência do valor observado da variável desfecho sobre os valores previstos. Os valores de alavancagem podem situar-se entre 0 (o caso não tem qualquer influência) e 1 (o caso tem influência total ). Se nenhum caso exercer influência indevida sobre o modelo, seria de esperar que todos os valores de alavancagem se situassem próximos do valor médio. Alguns autores (Hoaglin e Welsch 1978) recomendam investigar casos com valores superiores ao dobro da média como ponto de corte para identificar casos com influência indevida.\n\nvalores_hat &lt;- hatvalues(modelo_lm)\n\nsummary(valores_hat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008264 0.008264 0.008333 0.008523 0.009009 0.009009 \n\nvalores_altos &lt;- valores_hat[valores_hat &gt; 2 * summary(valores_hat)[4]]\nvalores_altos\n\nnamed numeric(0)\n\n\nDFBETAS (Difference in BETA)\nÉ uma medida de diagnóstico importante na análise de resíduos em um modelo linear. Ele serve para quantificar o quanto a remoção de uma única observação (um ponto de dados) influencia a estimativa de cada coeficiente no modelo (Belsley, Kuh, e Welsch 1980.)\n\n\ndfb &lt;- dfbetas(modelo_lm)\nhead(dfb)\n\n  (Intercept) tabagismoLeve tabagismoIntenso\n1 -0.02401343    0.01701525       0.01664599\n2 -0.12597546    0.08926272       0.08732557\n3 -0.05183764    0.03673072       0.03593360\n4  0.03735572   -0.02646923      -0.02589480\n5 -0.03360471    0.02381137       0.02329462\n6  0.10465239   -0.07415379      -0.07254453\n\n\nUm valor de DFBETAS grande em valor absoluto (positivo ou negativo) para uma observação específica e um coeficiente indica que essa observação é altamente influente na estimativa daquele coeficiente em particular.\nUm DFBETAS positivo significa que a remoção da observação fez com que a estimativa do coeficiente diminuísse. Um DFBETAS negativo significa que a remoção da observação fez com que a estimativa do coeficiente aumentasse.\nValores maiores que 1 ou menores que –1 (em amostras pequenas) indicam influência relevante. A função summary() permite ver os extremos por coeficiente:\n\nsummary(dfb)\n\n  (Intercept)         tabagismoLeve        tabagismoIntenso    \n Min.   :-2.291e-01   Min.   :-1.919e-01   Min.   :-2.345e-01  \n 1st Qu.: 0.000e+00   1st Qu.:-1.906e-02   1st Qu.:-2.254e-02  \n Median : 0.000e+00   Median : 0.000e+00   Median : 0.000e+00  \n Mean   : 5.667e-06   Mean   : 1.944e-06   Mean   :-5.770e-06  \n 3rd Qu.: 0.000e+00   3rd Qu.: 1.845e-02   3rd Qu.: 2.021e-02  \n Max.   : 2.086e-01   Max.   : 2.048e-01   Max.   : 1.987e-01  \n\n\nOs valores estão todos abaixo de 0.25, o que indica que nenhuma observação está alterando significativamente os coeficientes. O modelo está estável e não sofre influência desproporcional de nenhuma observação individual. Isso reforça a confiabilidade dos efeitos estimados para os níveis de tabagismo.\n\n\n15.2.7.2.3 Avaliação da homogeneidade das variâncias\nEm seguida, testa-se a suposição de que as variâncias são iguais, usando o Teste de Levene através da função leveneTest () do pacote car.\nQuando há interação entre os fatores, cada combinação de níveis dos fatores forma um grupo distinto. O teste de Levene deve ser aplicado entre esses grupos combinados.\n\ncar::leveneTest(pesoRN~tabagismo, \n                center = mean, \n                data = dados_anova)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value Pr(&gt;F)\ngroup   2  1.7955 0.1676\n      349               \n\n\nO teste de Levene exibe como resultado um valor p &gt; 0,05, mostrando que não é possível rejeitar a \\(H_0\\) de igualdade das variâncias.\nO teste de Levene calcula a distância de cada observação ao centro do grupo (média ou mediana), e testa se essas distâncias têm variâncias semelhantes entre os grupos. Quando os dados são aproximadamente normais usa-se center = mean. É mais sensível, mas menos robusto. Quando há outliers ou dados assimétricos, recomenda-se center = median, porque é mais robusto11.\nObserve que , neste exemplo, tanto faz usar um ou outro método, pois os dados não violam o a pressuposição de normalidade:\n\ncar::leveneTest (pesoRN~tabagismo, \n                 center = median, \n                 data = dados_anova)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   2  1.7984 0.1671\n      349               \n\n\nDa mesma maneira que no teste t, os pressupostos têm mais importância em grupos pequenos e desiguais. Para o exemplo em análise, os pressupostos foram verificados e pode-se assumir que os grupos são independentes e as médias têm distribuição normal e existe homocedasticidade, além disso, os grupos têm o tamanhos semelhantes. Portanto, a análise pode ser continuada.\n\n\n15.2.7.2.4 Conclusão\nA análise de variância de uma via foi aplicada para investigar o efeito do tabagismo sobre o escore da variável dependente. Os resultados indicaram diferenças estatisticamente significativas entre os níveis do fator, evidenciando que o tabagismo está associado a variações no escore analisado. A suposição de normalidade dos resíduos foi avaliada por meio do teste de Shapiro-Wilk, cujo resultado (p = 0,082) não indicou violação da normalidade. A presença de outliers foi identificada globalmente, com 11 observações classificadas como moderadas, distribuídas entre os grupos de tabagismo. Nenhuma observação foi considerada extrema, o que reduz o risco de distorção sistemática nos resultados. A análise de influência, realizada por meio dos DFBETAS e do gráfico de influência, demonstrou que nenhuma observação individual exerceu impacto relevante sobre os coeficientes do modelo. Os valores máximos de DFBETAS permaneceram abaixo de 0,25, indicando estabilidade nas estimativas, conforme os critérios propostos por Field (Field, Miles, e Field 2012b). Dessa forma, o modelo ajustado pode ser considerado estatisticamente adequado, com efeitos confiáveis e interpretáveis entre os níveis do fator tabagismo. As suposições da ANOVA foram atendidas, e os resultados obtidos são válidos para fins de inferência.\n\n\n\n15.2.7.3 O que fazer se os pressupostos são violados?\nSe a homogeneidade da variância é o problema, um teste possível de ser implementado no R é o F de Welch, aplicando a funçãowelch.test(), incluída no pacote onewaytests (Dag, Dolgun, e Konar 2018). Existem também testes não paramétricos, como o Teste de Kruskal-Wallis, que será visto mais adiante (Seção 20.6).\n\n\n\n15.2.8 Testes post-hoc\nOs testes de comparações múltiplas constituem-se em uma análise após a realização da ANOVA. Se houve uma diferença, indicada pela ANOVA, os testes de comparações múltiplas ou também conhecidos como teste post hoc, ajudam a quantificar as diferenças entre os grupos para determinar quais grupos diferem significativamente uns dos outros.\nAqui será usado o HSD de Tukey, que é conservador. HSD vem da expressão em inglês - Honest Significant Difference Damasio (2021). Este teste requer um objeto aov no qual executa seu procedimento, que chamaremos de pwc12. O procedimento de Tukey HSD executará uma comparação de pares de todas as combinações possíveis dos grupos e testará esses pares para diferenças significativas entre suas médias, tudo enquanto ajusta o valor p a um limite superior de significância para compensar o fato de que muitos testes estatísticos estão sendo realizados e a probabilidade de um falso positivo aumenta com o aumento do número de testes. A função a ser usada é a tukey_hsd(), do pacote rstatix.\n\npwc &lt;- rstatix::tukey_hsd (modelo.aov)\npwc\n\n# A tibble: 3 × 9\n  term      group1 group2  null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tabagismo Não    Leve             0   -114.     -259.      31.6 0.158  \n2 tabagismo Não    Intenso          0   -191.     -339.     -42.1 0.00761\n3 tabagismo Leve   Intenso          0    -77.0    -225.      71.3 0.441  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nCom base nos valores p &lt; 0,05 tem-se três combinações de grupos: Não-Leve, Não-Intenso e Leve-Intenso Isto mostra que uma diferença significativa (p = 0,0076) entre mães não fumantes e as fumates intensas. Os demais grupos não apresentaram uma difirença significativa.\nPode-se visualizar isso na Figura 15.11 obtida , usando os resultados da função tukey_hsd() Esta função gera o teste de Tukey com as diferença entre os pares e os intervalos de confiança que permitem a construção do gráfico, com o código abaixo:\n\n# Criar uma coluna com os pares comparados\npwc$comparacao &lt;- paste(pwc$group1, \"vs\", pwc$group2)\n\n# Reordenar os pares para o eixo Y\npwc$comparacao &lt;- factor(pwc$comparacao, levels = rev(pwc$comparacao))\n\n# Gráfico horizontal\nggplot(pwc, aes(x = estimate, y = comparacao, color = p.adj.signif)) +\n  geom_point(size = 3) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2, size=1.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray40\") +\n  labs(\n    title = \"Teste de Tukey: Diferença entre grupos de tabagismo\",\n    x = \"Diferença estimada no peso ao nascer (g)\",\n    y = \"Comparações entre grupos\",\n    color = \"Significância\"\n  ) +\n  theme_bw(base_size = 13)\n\n\n\n\n\n\n\nFigura 15.11: Gráficos do Teste de Tukey\n\n\n\n\n\n\n\n15.2.9 Tamanho do efeito\nUma das medidas de tamanho de efeito mais comumente relatadas para a ANOVA é o eta ao quadrado (\\(\\eta^2\\)), que é um índice da força da associação entre um fator e uma variável dependente. Eta ao quadrado é a proporção da variação total atribuível ao fator. É calculado como a razão da variância do fator para a variância total e os valores variam de 0 a 1. Pode ser calculado o eta qo quadrado parcial ou generalizado. Aquio eta ao quadrado parcial é suficiente, enquanto que para desenhos mais complexos, como ANOVA de medidas repetidas, está mais indicado o generalizado, pois além de incluir o efeito entre sujeito, inclui outros efeitos\nEsta medida pode ser obtida com o pacote effectsize (Ben-Shachar, Lüdecke, e Makowski 2020), usando a função eta_squared(), usando o modelo modelo.aov.\n\neffectsize::eta_squared (modelo.aov, partial = TRUE)\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntabagismo | 0.03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nApesar de ser controverso, pode-se seguir a orientação da Tabela 15.2), para a interpretação (Watson 2021):\n\n\n\n\nTabela 15.2: Interpretação do Tamanho do Efeito\n\n\n\nResultadoTamanho do Efeito0.01pequeno0,06médio0,14grande\n\n\n\n\n\n\n\n\n\n\n\nPara curiosos\n\n\n\nO tamanho do efeito (eta ao quadrado) pode ser obtido de uma forma bem simples, sob o ponto de vista matemático. Ele é obtido pela fórmula, onde SQ = soma dos quadrados:\n\\[\n\\eta^{2}=\\frac{SQ_{efeito}}{SQ_{total}}\n\\]\nAssim, obtendo a soma dos quadrados a partir do modelo.aov utilizando a função model_parameters()13 do pacote parameters:\n\nanova &lt;- parameters::model_parameters(modelo.aov)\n\nSS_efeito &lt;- anova$Sum_Squares[1]\nSS_total &lt;- anova$Sum_Squares[1]+anova$Sum_Squares[2]\n\neta_quadrado &lt;- SS_efeito/SS_total\nround(eta_quadrado,2)\n\n[1] 0.03\n\n\nDesta forma , a variância do efeito, no caso tabagismo, corresponde a a 0,03 (3%) da variância total.\n\n\n\n\n15.2.10 Conclusão\nO peso dos recém-nascidos foi estatisticamente diferente entre os diferentes grupos, F(2, 349) = 4.65, P = 0.0102, \\(\\eta^2\\) = 0,03.\nAs análises post-hoc de Tukey revelaram que o peso dos recém-nascidos a termo no grupo das gestantes não fumantes apresentou uma diferença estatisticamente significativa do grupo de tabagismo intenso (-191g, IC95%: -339 a -42.1 g; P = 0,0076). Nos demais grupos não houve diferença significativa.\n\n\n15.2.11 Apresentação dos resultados\nSerão apresentados boxplots (Figura 15.12)), com ggboxplot(), do pacote ggpubr, utilizando, para cores do pacote ggsci., paleta do periódico New England of Medicine . Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix.\n\n# 1. Construir um gráfico de linha com ggline do ggpubr\ngline &lt;- ggpubr::ggline(dados_anova,\n                         x = \"tabagismo\",\n                         y = \"pesoRN\",\n                         color = \"cyan4\",\n                         linetype = \"dashed\",\n                        linewidth = 0.7,\n                         add = \"mean_ci\",\n                         point.size = 2,\n                         legend = \"none\",\n                         ggtheme = theme_classic(),\n                         xlab = \"Tabagismo Materno\" ,\n                         ylab = \"Peso do Recém-nascido (g)\") +\n  theme (text = element_text (size = 13),\n         axis.text.x = element_text(size = 11))\n\n# 2. Realizar a ANOVA com rstatix para verificar o efeito geral\nteste_anova &lt;- rstatix::anova_test(dados_anova, \n                                   pesoRN ~ tabagismo) %&gt;%\n  rstatix::add_significance()\n\n# 2. Realizar o Teste de Tukey (comparações múltiplas par a par)\npwc &lt;- dados_anova %&gt;%\n  rstatix::tukey_hsd(pesoRN ~ tabagismo) %&gt;%\n  rstatix::add_significance()\n\n# 3. Adicionar as posições x e y para as comparações post-hoc\npwc &lt;- pwc %&gt;% \n  rstatix::add_xy_position(x = \"tabagismo\")\n\npwc &lt;- pwc %&gt;%\n  dplyr::mutate(y.position = c(3330, 3350, 3370))\n\n# 4. Plotar o gráfico com o resultado das comparações post-hoc\ngline + \n  stat_pvalue_manual(pwc, \n                     tip.length = 0,\n                     hide.ns = FALSE) + \n  # Incluir a estatística da ANOVA no subtítulo\n  labs(subtitle = get_test_label(stat.test = teste_anova, \n                                 detailed = TRUE),\n       caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 15.12: Efeito do tabagismo na gestação sobre o peso do recém-nascido.([**]: p entre 0,001 e 0,01).",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "15-anova.html#anova-de-dois-fatores",
    "href": "15-anova.html#anova-de-dois-fatores",
    "title": "15  Análise de Variância",
    "section": "15.3 ANOVA de dois fatores",
    "text": "15.3 ANOVA de dois fatores\nA ANOVA de dois fatores é uma extensão da ANOVA de um fator. Neste tipo de ANOVA, ao invés de observar o efeito de um fator sobre a variável desfecho contínua, é analisado simultaneamente o efeito de duas variáveis de agrupamento. Outros sinônimos para a ANOVA de dois fatores são: ANOVA fatorial ou ANOVA de duas vias. Quando se tem dois ou mais fatores, além de observar o efeito desses fatores sobre a variável desfecho, há necessidade de verificar se eles não interagem entre si. Portanto, é um objetivo importante da ANOVA fatorial avaliar se há um efeito de interação estatisticamente significativo entre os fatores.\n\n15.3.1 Dados para o exemplo\nO conjunto de dados, usado no exemplp, será o mesmo trabalhado na ANOVA de um fator (Seção 15.2.4.1) acresentando mais um fator (sexo) na análise.\n\ndados_anova &lt;- dados_anova %&gt;% \n  dplyr::select(pesoRN, sexo, tabagismo)\n\nstr(dados_anova)\n\ntibble [352 × 3] (S3: tbl_df/tbl/data.frame)\n $ pesoRN   : num [1:352] 3110 2580 2965 3430 3060 ...\n $ sexo     : Factor w/ 2 levels \"Masc\",\"Fem\": 1 1 1 2 1 2 1 1 1 1 ...\n $ tabagismo: Factor w/ 3 levels \"Não\",\"Leve\",\"Intenso\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nEstes dados permitem a sua aplicação em uma ANOVA de dois fatores, pois têm:\n\nUma Variável Dependente (numérica/contínua): pesoRN (Peso do Recém-Nascido, em gramas).\nDuas Variáveis Independentes (categóricas/fatores): sexo (com 2 níveis: Masc, Fem) e tabagismo (com 3 níveis: Não, Leve, Intenso).\n\nA partir da ANOVA de dois fatores será possível avaliar:\n\nO Efeito Principal do sexo no pesoRN.\nO Efeito Principal do tabagismo no pesoRN.\nO Efeito de Interação entre sexo e tabagismo no pesoRN.\n\n\ntable(dados_anova$sexo, dados_anova$tabagismo)\n\n      \n       Não Leve Intenso\n  Masc  72   63      59\n  Fem   48   58      52\n\n\nAtravés da função table(), observa-se que os dados têm uma estrutura um pouco mais complexa daquela da ANOVA de um fator, pois, à medida que aumentam os fatores, aumentam o número de células na estrutura do modelo. Existe uma preocupação a mais a ser analisada no ajuste do modelo. Esses dados estão claramente desbalanceados, tanto entre os níveis de tabagismo quanto entre os sexos. Tem-se um desenho 2 x 3 com cada célula contendo um número diferente de indivíduos. Isso pode afetar diretamente a forma como os efeitos são estimados em uma ANOVA de duas vias. Entretanto, as contagens variam de 48 a 72. Essa variação (máx/mín ≈ 1,5:1) é muito mais gerenciável. Segundo Peat (Barton e Peat 2014), um desequilíbrio no tamanho das células de mais de 1:4, no modelo, seria uma preocupação. Da mesma forma, um número mínimo por célula de 10, sendo desejável pelo menos 30, pois um número pequeno leva a uma perda de poder estatístico. Na prática, na área da saúde, igual número em cada uma das caselas é muito raro (Bland 2015).\n\n15.3.1.1 Descrição dos dados\nNa saída da função str(), verifica-se que as variáveis de interesse: tabagismo está como fator com 3 níveis colocados de acordo com a intensidade de tabagismo materno em uma ordem lógica (Não, Leve e Intenso); a variável sexo estão como fator em dois níveis: Masculino e Feminino e não tem uma ordem lógica. A variável peso do recém-nascido (pesoRN) é uma variável num (numérica).\nA sumarização dos dados será feita com as funções group_by() e summarise() do pacote dplyr para a variável pesoRN por grupos, sexo e tabagismo.\n\nalpha &lt;- 0.05\nresumo &lt;- dados_anova %&gt;% \n  dplyr::group_by(sexo, tabagismo) %&gt;% \n  dplyr::summarise(n = n(),\n                   media = mean(pesoRN, na.rm=TRUE),\n                   dp = sd(pesoRN, na.rm=TRUE),\n                   ep = dp/sqrt(n),\n                   me = qt((1 - alpha/2),n-1)*ep,\n                   linf = media - me,\n                   lsup = media + me,\n                   .groups = \"drop\")\nresumo\n\n# A tibble: 6 × 9\n  sexo  tabagismo     n media    dp    ep    me  linf  lsup\n  &lt;fct&gt; &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masc  Não          72 3265.  456.  53.7  107. 3158. 3372.\n2 Masc  Leve         63 3198.  406.  51.1  102. 3096. 3300.\n3 Masc  Intenso      59 3124.  520.  67.7  136. 2989. 3260.\n4 Fem   Não          48 3191.  470.  67.9  137. 3054. 3327.\n5 Fem   Leve         58 3038.  473.  62.0  124. 2914. 3162.\n6 Fem   Intenso      52 2954.  537.  74.5  150. 2805. 3104.\n\n\nUsou-se .groups =, para controlar o argumento final.\n\n= \"drops\" , Remove todos os agrupamentos. O resultado fica como uma tabela comum;\n= \"drop_last\" remove apenas o último agrupamento, mantendo o outro;\n= \"keep\" mantém todos os agrupamentos (padrão).\n\n\n\n15.3.1.2 Visualização dos dados\nUm gráfico de linha (Figura 15.13) com barras de erro é uma boa maneira de visualizar o comportamento das variáveis. Mostra o impacto dos níveis de tabagismo (não fumante, fumante leve e fumante pesado) sobre peso dos recém-nascidos de acordo com o sexo.\n\nggpubr::ggline(data = dados_anova, \n               x = \"tabagismo\", \n               y = \"pesoRN\", \n               color = \"sexo\",\n               linewidth =  0.9,\n               linetype = \"dashed\",\n               legend.title = \"\",\n               position = position_dodge(width = 0.2),\n               ylab = \"Peso médio do RN (g)\",\n               xlab = \"Nível de Tabagismo\",\n               add = \"mean_ci\",\n               palette = \"Dark2\")\n\n\n\n\n\n\n\nFigura 15.13: Efeito do tabagismo materno no peso do recém-nascido de acordo com o sexo.\n\n\n\n\n\nTanto os dados numéricos como o gráfico de linhas exibem uma diminuição do peso dos recém-nascidos à medida que os níveis de tabagismo aumentam, bem como parece haver uma diferença entre os sexos.\n\n\n\n\n\n\nQuestão\n\n\n\nOs pesos dos recém-nascidos dependem do seu sexo e dos níveis de tabagismo materno?\n\n\n\n\n\n15.3.2 Hipóteses estatísticas\nComo mencionado acima, uma ANOVA de duas vias é usada para avaliar simultaneamente o efeito de duas variáveis categóricas em uma variável quantitativa contínua. Ela é chamada de ANOVA de duas vias porque compara grupos formados por duas variáveis categóricas independentes.\nA questão acima suscita várias hipóteses. Em particular, o interesse está em:\n\nmedir e testar a relação entre a tabagismo e o peso do recém-nascido,\nmedir e testar a relação entre sexo do recém-nascido e o seu peso, e\npossivelmente verificar se a relação entre tabagismo e peso do recém-nascido é diferente para meninos e maninas (o que é equivalente a verificar se a relação entre sexo e oeso do recém-nascido depende do tabagismo)\n\nAs duas primeiras relações são chamadas de efeitos principais, enquanto o item 3 é conhecido como efeito de interação.\nOs efeitos principais testam se pelo menos um grupo é diferente de outro (durante o controle da outra variável independente). Por outro lado, o efeito de interação tem como objetivo testar se a relação entre duas variáveis difere dependendo do nível de uma terceira variável. Em outras palavras, se a variação entre a resposta e a primeira variável categórica não depender das modalidades da segunda variável categórica, então não há interação entre as duas variáveis. Se, ao contrário, houver uma modificação dessa variação, seja por um aumento no efeito da primeira variável, seja por uma diminuição, então há uma interação.\nVoltando ao exemplo, a ANOVA de Duas Vias testa três hipóteses nulas (\\(H_{0}\\))::\nEfeito principal do sexo no peso do recém-nascido:\n\\(H_{0}\\): o peso médio do recém-nascido é igual entre meninas e meninos.\n\\(H_{1}\\): o peso médio do recém-nascido é diferente entre meninas e meninos.\nEfeito principal do tabagismo no peso do recém-nascido:\n\\(H_{0}\\): o peso médio dos recém-nascidos é igual entre as categorias de tabagismo.\n\\(H_{1}\\): o peso médio dos recém-nascidos é diferente entre as categorias de tabagismo\nInteração entre sexo e álcool:\n\\(H_{0}\\): não há interação entre sexo e tabagismo, o que significa que o efeito do tabagismo no peso do recém-nascido é o mesmo para meninas e meninos. \\(H_{1}\\): há interação entre sexo e peso do recém-nascido, o que significa que o efeito do tabagismo no peso do recém-nascido é diferente para meninas e meninos.\n\n\n15.3.3 Teste estatístico\nDe um modo geral, o sofware estatístico calcula a Soma dos Quadrados(SQ) para cada uma das fontes de variação (sexo, tabagismo, interação e resíduos) e a partir delas, calcula os Quadrados Médios (QM)14 e dividindo cada um pelos seus respectivos graus de liberdade, obtém-se a estatística F para cada efeito. O valor de F é comparado a uma distribuição F (veja a Seção 15.2.3) para obter o valor p associado a cada hipótese, de acordo com o o valor de \\(\\alpha\\) escolhido previamente (em geral, 0,05) .\n\\[\nF = \\frac{\\text{variância entre}}{\\text{variância dentro}} = \\frac{\\frac{SQ_{E}}{gl_{E}}}{\\frac{SQ_{R}}{gl_{R}}} =\\frac{QM_{E}}{QM_{D}}\n\\] O resultado do teste é tipicamente apresentado em uma tabela como esta (Tabela 15.3), denominada de Tabela da ANOVA:\n\n\n\n\nTabela 15.3: Tabela da ANOVA\n\n\n\nFonte de VariaçãoSoma dos QuadradosGraus de LiberdadeQuadrado MédioEstatística FValor pSexo(A)SQA1QMAFApATabagismo(B)SQB2QMBFBpBInteração (A*B)SQA*B2QMA*BFA*BpA*BResíduo/ErroSQErro346QErroTotalSQTotal351\n\n\n\n\n\nOs graus de liberdade são obtidos da seguinte maneira:\n\nGraus de liberdade do fator A (sexo) = nº de níveis de A - 1 = 2 - 1 = 1;\n\nGraude liberdade do fator B (tabagismo) = nº de níveis de B - 1 = 3 - 1 = 2;\n\nGraus de liberdade do Interação (sexo:tabagismo) = (nº de níveis de A - 1) x (nº de níveis de B - 1) = 1 x 2 = 2;\n\nGraus de liberdade do Erro = Total de observações - nº de grupos formados pelos níveis dos fatores = 352 - (2 x 3) = 346;\n\nGraus de liberdsade total = 1 + 2 + 2 + 346 = 351.\n\n\n15.3.3.1 Ajustando um modelo com interação\nNa ANOVA de de uma via foi testado as diferenças entre a médias quando há uma única variável independente. Uma das vantagens da ANOVA é poder observar os efeitos de mais de uma variável independente e como essas variáveis interagem. Para criar um modelo de ANOVA de dois fatores, pode-se usar a função aov() , usada na ANOVA de uma via. O princípio é o mesmo, apenas acrescenta-se o segundo fator, usando o sinal de + e um termo de interação com um asterisco * ou dois pontos : . A inclusão de um efeito de interação em uma ANOVA de duas vias não é obrigatória. Entretanto, para evitar conclusões errôneas, recomenda-se verificar primeiro se a interação é significativa ou não e, dependendo dos resultados, incluí-la ou não. Se a interação não for significativa, é seguro removê-la do modelo final. Por outro lado, se a interação for significativa, ela deverá ser incluída no modelo final que será usado para interpretar os resultados. Portanto, deve-se começar com um modelo que inclui os dois efeitos principais (ou seja, sexo e tabagismo) e a interação:\n\nmod.aov.int &lt;- aov(formula = pesoRN ~ tabagismo * sexo, \n               data = dados_anova) \n\nsummary(mod.aov.int)\n\n                Df   Sum Sq Mean Sq F value  Pr(&gt;F)   \ntabagismo        2  2136213 1068107   4.709 0.00960 **\nsexo             1  1570809 1570809   6.925 0.00888 **\ntabagismo:sexo   2   160346   80173   0.353 0.70250   \nResiduals      346 78478383  226816                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n15.3.3.2 Interpretação do modelo com interação\nSemelhante a uma ANOVA de uma via, o princípio de uma ANOVA de duas vias baseia-se na dispersão total dos dados e em sua decomposição em quatro componentes:\n\nParcela atribuível ao primeiro fator\nParcela atribuível ao segundo fator\nParcela atribuível à interação dos dois fatores\nParte não explicada ou residual.\n\nA soma dos quadrados (coluna Sum Sq ) mostra esses quatro componentes. A ANOVA de duas vias consiste em usar um teste estatístico para determinar se cada componente de variação (atribuível aos dois fatores estudados e à interação deles) é significativamente maior do que o componente residual. Se esse for o caso, concluí-se que o efeito considerado (tabagismo, sexo ou a interação) é significativo.\nVê-se que a variável tabagismo assim como a variável sexo mostraram-se significativas. Os valore p são exibidos na última coluna do resultado acima ( Pr(&gt;F )). A partir desses valores, conclui-se que, no nível de \\(\\alpha\\)= 0,05 :\n\nControlando para o tabagismo, o peso dos recém nascidos são significativamente diferentes entre os dois sexos (p = 0,01),\nControlando para o sexo, o peso dos recém-nascidos é significativamente diferente (p &lt; 0,01) para pelo menos uma categoria de tabagismo, e\nA interação entre sexo e tabagismo (exibida na linha tabagismo:sexo no resultado) mostrou-se não significativa (p = 0,703). Isso quer dizer que o efeito do tabagismo não depende depende do sexo .\n\n\n\n\n\n\n\nCuidado\n\n\n\nAtente para o fato que a função aov() pressupõe um projeto balanceado, o que significa tamanhos de amostra semelhantes dentro dos níveis das variáveis de agrupamento independentes. Para verificar se os dados estão balanceados, proceda como mostrado na Seção 15.3.1. Quando os resultados mostram o mesmo número de indivíduos em todas as células, não importa qual o tipo de ANOVA a ser usado, os resultados serão iguais. A aov() usa as somas de quadrados do tipo I.\nPara delineamentos não balanceados, ou seja, números desiguais de indivíduos em cada subgrupo, os métodos recomendados são:\n\nA ANOVA do tipo II, quando não há interação significativa, que pode ser feita, no R, com Anova(modelo, type = “II”) ou Anova(modelo, type = 2) , em que modelo é o nome do modelo salvo, e\nA ANOVA do tipo III, quando há uma interação significativa, que pode ser feita, no R, com Anova(modelo, type = “III”) ou Anova(modelo, type = 3). Fundamentalmente, a diferença entre um método e outro é como o R calcula a soma dos quadrados ao calcular a ANOVA. Quando os dados são balanceados, os três tipos dão o mesmo resultado 15.\n\n\n\nPode-se dar seguimento, ajustando o modelo sem a interação.\n\n\n15.3.3.3 Modelo sem interação\nA análise anterior mostrou que a interação não é significativa, deve-se re-analisar os efeitos principais usando a ANOVA Tipo II (que é o padrão quando não há interação).\n\nmod.aov &lt;- aov(formula = pesoRN ~ tabagismo + sexo, \n               data = dados_anova) \n\ncar::Anova(mod.aov, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: pesoRN\n            Sum Sq  Df F value   Pr(&gt;F)   \ntabagismo  1912597   2  4.2319 0.015279 * \nsexo       1570809   1  6.9513 0.008751 **\nResiduals 78638730 348                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nEfeito do Tabagismo (p = 0.0153): conclui-se que o tabagismo tem um efeito estatisticamente significativo no peso, ajustado para o sexo. Isso significa que a diferença de peso entre os níveis (Não, Leve, Intenso) é real, independentemente de ser menino ou menina.\nEfeito do Sexo ( p = 0.0088): conclui-se que sexo tem um efeito estatisticamente significativo no peso, ajustado para o tabagismo. Isso significa que a diferença de peso entre meninoss e meninas é real, independentemente do nível de tabagismo.\n\n\n\n\n15.3.4 Avaliação dos pressupostos\n\n15.3.4.1 Criação da variável residuos e valores ajustados\nComo os pressupostos são avaliados nos resíduos, eles, junto com os valores ajustados, serão obtidos a partir do modelo de ajuste em uso (mod.aov)\n\ndados_anova &lt;- dados_anova %&gt;%\n  mutate(residuos = residuals(mod.aov),\n         ajustados = fitted(mod.aov))\n\nstr(dados_anova)\n\ntibble [352 × 5] (S3: tbl_df/tbl/data.frame)\n $ pesoRN   : num [1:352] 3110 2580 2965 3430 3060 ...\n $ sexo     : Factor w/ 2 levels \"Masc\",\"Fem\": 1 1 1 2 1 2 1 1 1 1 ...\n $ tabagismo: Factor w/ 3 levels \"Não\",\"Leve\",\"Intenso\": 1 1 1 1 1 1 1 1 1 1 ...\n $ residuos : Named num [1:352] -179 -709 -324 276 -229 ...\n  ..- attr(*, \"names\")= chr [1:352] \"1\" \"2\" \"3\" \"4\" ...\n $ ajustados: Named num [1:352] 3289 3289 3289 3154 3289 ...\n  ..- attr(*, \"names\")= chr [1:352] \"1\" \"2\" \"3\" \"4\" ...\n\n\n\n\n15.3.4.2 Normalidade dos resíduos\nOs resíduos (erros) do modelo devem seguir uma distribuição normal. Isto pode ser verificado, como visto na ANOVA de uma via, com o teste Shapiro-Wilk, através da função shapiro_test() do pacote rstatix.\n\ndados_anova %&gt;%\n  shapiro_test(residuos)\n\n# A tibble: 1 × 3\n  variable statistic      p\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 residuos     0.990 0.0164\n\n\nO teste retornou um valor p = 0,0164 (p &lt; 0,05). Isto indica que uma evidência estatisticamente significativa de que os respiduos não seguem uma distribuição normal, considerando um nível de signifcância de 5%. Entretanto, como um n &gt; 30 por grupo a ANOVA costuma ser robusta a pequenas violações da normalidade.\n\n15.3.4.2.1 QQplot\nComplementando a valiação da normalidade, será construído um QQ plot com a função ggqqplot() do pacote ggpubr:\n\nggpubr::ggqqplot(data = dados_anova,\n                 x = \"residuos\",\n                 conf.int = TRUE,\n                 shape = 19,\n                 xlab = \"Quantis teóricos\",\n                 ylab = \"Resíduos\",\n                 color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 15.14: QQ Plot dos resíduos\n\n\n\n\n\nApesar do valor p de 0,0164 no teste de Shapiro-Wilk indicar desvio da normalidade, o gráfico QQ (Figura 15.14) mostra que os resíduos seguem uma linha razoavelmente reta, com pequenas curvaturas apenas nas extremidades (caudas).\n\n\n15.3.4.2.2 Histograma\nUm histograma aumenta a segurança na avaliação da normalidade, principalmente quando o teste de Shapiro-Wilk rejeita a hipótese nula da normalidade:\n\nggpubr::gghistogram(data = dados_anova, \n                    x = \"residuos\",\n                    fill = \"lightblue\",\n                    bins = 10,\n                    color = \"black\",\n                    ylab = \"Frequência\",\n                    xlab = \"Resíduos\")\n\n\n\n\n\n\n\nFigura 15.15: Histograma dos resíduos\n\n\n\n\n\nA Figura 15.15 do histograma dos resíduos, mostra uma distribuição aproximadamente simétrica, centrada em torno de 0, leves caudas mais pesadas, mas sem grandes distorções e sem assimetrias graves nem multimodalidade.\nA conclusão geral da avaliação da normalidade aponta um leve desvio de normalidade dos resíduos (valor p de 0,0164 no Shapiro-Wilk), mas com base no QQ plot razoavelmente linear e no histograma com forma quase normal e no tamanho amostral (&gt; 100 por grupo), pode-se seguir com a ANOVA com segurança. A ANOVA é bastante robusta a essa leve violação.\n\n\n\n15.3.4.3 Homogeneidade da variância\nPara verificar a homoscedasticidade (igualdade de variâncias) na ANOVA de duas vias, o Teste de Levene é uma excelente escolha. A forma de aplicar o teste usa a função leveneTest() do pacote car e depende se a interação entre os fatores for incluída ou não.\nQuando há interação entre os fatores, cada combinação de níveis dos fatores:\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   5  0.5088 0.7696\n      346               \n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   5  0.5088 0.7696\n      346               \n\n\nNo exemplo que está sendo testado não houve interação, então, o teste de Levene pode ser aplicado separadamente para cada fator principal:\n\nleveneTest(pesoRN ~ tabagismo, data = dados_anova)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   2  1.7984 0.1671\n      349               \n\nleveneTest(pesoRN ~ sexo, data = dados_anova)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  2.1217 0.1461\n      350               \n\n\n\n15.3.4.3.1 Gráfico diagnóstico\nO gráfico de resíduos vs. valores ajustados ajuda a avaliar a homoscedasticidade dos resíduos e ausência de padrão sistemático nos resíduos:\n\nggplot(dados_anova, \n       aes(x = ajustados, \n           y = residuos)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(title = \"Resíduos vs Ajustados\") +\n  theme_classic(base_size = 13)\n\n\n\n\n\n\n\nFigura 15.16: Gráfico diagnóstico de homoscedasticidade\n\n\n\n\n\nA Figura 15.16 que a distribuição vertical dos resíduos parece razoavelmente constante em torno de zero, para todos os valores ajustados, não havendo uma heteroscedasticidade visível. Também não um padrão em forma de funil, sinal de que o modelo está bem ajustado e os resíduos se comportam aleatoriamente.\nSão visíveis alguns outliers (resíduos bem distantes de zero), mas nada extremo ou preocupante considerando o tamanho da amostra.\nPortanto, apoiado no Teste de Levene e no gráfico da Figura 15.16, reforça a homogeneidade das variâncias.\n\n\n\n15.3.4.4 Outliers\nO gráfico diagnóstico Figura 15.16 apontou alguns outliers que serão identificados, usando a função identify_outliers() do pacote rstatix:\n\ndados_anova %&gt;% \n  dplyr::group_by(sexo, tabagismo) %&gt;% \n  rstatix::identify_outliers(residuos)\n\n# A tibble: 11 × 7\n   sexo  tabagismo pesoRN residuos ajustados is.outlier is.extreme\n   &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n 1 Masc  Não         2051   -1238.     3289. TRUE       FALSE     \n 2 Masc  Não         4305    1016.     3289. TRUE       FALSE     \n 3 Masc  Não         4315    1026.     3289. TRUE       FALSE     \n 4 Masc  Leve        4350    1164.     3186. TRUE       FALSE     \n 5 Masc  Intenso     1440   -1668.     3108. TRUE       FALSE     \n 6 Masc  Intenso     1980   -1128.     3108. TRUE       FALSE     \n 7 Masc  Intenso     1795   -1313.     3108. TRUE       FALSE     \n 8 Masc  Intenso     4410    1302.     3108. TRUE       FALSE     \n 9 Fem   Leve        1715   -1336.     3051. TRUE       FALSE     \n10 Fem   Leve        4620    1569.     3051. TRUE       FALSE     \n11 Fem   Intenso     4390    1417.     2973. TRUE       FALSE     \n\n\nAlguns outliers (11) foram identificados com base nos resíduos padronizados, mas não foram classificados como extremos (acima de \\(3 \\times IIQ\\)). Eles estão distribuídos entre diferentes combinações de fatores, o que sugere que não há viés sistemático. Considerando o tamanho amostral e a robustez da ANOVA, optou-se por mantê-los na análise.\n\n\n\n\n\n\nModelo de relatório\n\n\n\nVerificação dos pressupostos da ANOVA Fatorial\nAntes da interpretação dos resultados obtidos por meio da Análise de Variância (ANOVA), é fundamental avaliar se os pressupostos do modelo foram atendidos. A validade das inferências estatísticas depende diretamente do cumprimento dessas condições.\n\nNormalidade dos resíduos\nA suposição de normalidade dos resíduos foi inicialmente avaliada por meio do teste de Shapiro-Wilk, cujo resultado indicou uma leve violação desse pressuposto (p = 0,02). Apesar disso, a inspeção visual do gráfico quantil-quantil (QQ plot) e do histograma dos resíduos revelou uma distribuição aproximadamente simétrica, com pequeno desvio nas caudas, sem indícios de distorções severas. Considerando o elevado tamanho amostral por grupo (superior a 100 observações), a ANOVA demonstra robustez suficiente para lidar com pequenas violações da normalidade.\nHomogeneidade das variâncias (homocedasticidade)\nA igualdade das variâncias entre os grupos foi testada por meio do teste de Levene, utilizando a mediana como medida de centralidade. O resultado (p = 0,7696) não indicou diferenças estatisticamente significativas entre as variâncias, corroborando o pressuposto de homocedasticidade. Essa evidência foi reforçada pelo gráfico de resíduos versus valores ajustados, no qual a dispersão dos resíduos se apresentou constante, sem tendência ou padrão sistemático.\nIndependência dos resíduos\nA independência dos resíduos é assumida com base no delineamento experimental adotado, o qual não apresenta indícios de dependência entre as observações.\nIdentificação de valores atípicos (outliers)\nAlguns resíduos foram identificados como outliers, porém nenhum foi classificado como extremo. Esses casos encontram-se distribuídos entre diferentes grupos e não apresentam padrão que indique viés sistemático. Dado o tamanho da amostra e o impacto estatístico limitado desses pontos, optou-se por mantê-los na análise.\n\nConclusão\nA análise conjunta dos testes estatísticos e das evidências gráficas indica que os pressupostos da ANOVA foram suficientemente atendidos. Dessa forma, os resultados inferenciais obtidos podem ser interpretados com segurança e validade estatística.\n\n\n\n\n\n15.3.5 Testes post hoc\n\n15.3.5.1 Teste de Tukey\nQuando uma análise de variância (ANOVA) detecta diferenças estatisticamente significativas entre médias de grupos, o próximo passo é identificar quais pares de médias diferem entre si. Para isso, utiliza-se um teste post hoc, ou seja, um procedimento complementar que controla o erro tipo I em múltiplas comparações.\nEntre os testes post hoc disponíveis, o teste de Tukey HSD (Honest Significant Difference) é um dos mais utilizados pela sua robustez e facilidade de interpretação.\n\npwc &lt;- rstatix::tukey_hsd (mod.aov)\n\n\n15.3.5.1.1 Preparar os dados para visualização\n\npwc &lt;- pwc %&gt;%\n  mutate(\n    comparacao = paste(group1, \"vs\", group2),\n    signif_color = ifelse(p.adj.signif %in% c(\"**\", \"***\", \"****\"), \n                          p.adj.signif, \"ns\"))\n\n\n\n15.3.5.1.2 Gráfico com pontos (diferença média) e barras de erro(IC)\n\nggplot(pwc, aes(x = estimate, \n                y = reorder(comparacao, estimate))) +\n  geom_vline(xintercept = 0, \n             linetype = \"dashed\", \n             color = \"gray40\") +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.3) +\n  geom_point(aes(color = signif_color), size = 3) +\n  scale_color_manual(values = c(\"****\" = \"gold\", \"***\" = \"darkorange\", \"**\" = \"greenyellow\", \"ns\" = \"gray60\"),name = \"Significância\") +\n  labs(\n    title = \"Diferenças entre Grupos - Tukey HSD\",\n    x = \"Diferença Média\",\n    y = \"Comparações\") +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\nFigura 15.17: Teste de Tukey\n\n\n\n\n\n\n\n15.3.5.1.3 Análise dos Testes Post-hoc (Tukey HSD)\nComparações entre níveis de tabagismo:\n\nNão fumante vs. Leve:\nNão há diferença estatisticamente significativa no peso do recém-nascido entre os grupos “Não Fumante” e “Fumante Leve”. A diferença estimada de perda (sinal negativo) ou ganho (sinal positivo) de peso foi de -113,75 g (IC95%: - 257,90; 30,41), p = 0,153.\nNão fumante vs. Intenso:\nExiste uma diferença estatisticamente significativa no peso dos recém-nascidos entre os grupos “Não Fumante” e “Fumante Intenso”. A diferença estimada de perda de peso foi de -190,72 g (IC95%: -338,07; -43,37), p = 0,007. Os recém-nascidos de mães fumantes intensas apresentaram peso significativamente menor do que os de mães não fumantes.\nLeve vs. Intenso:\nNão há diferença estatisticamente significativa no peso dos recém-nascidos entre os grupos “Fumante Leve” e “Fumante Pesado”. A diferença estimada de perda (sinal negativo) ou ganho (sinal positivo) de peso foi de -76,97 g (IC95%: - 224,03; 70,08), p = 0,435.\nConclusão Principal: O principal impacto no peso não vem da transição de “Não Fumante” para “Fumante Leve”, mas sim da comparação entre os extremos, mostrando que a exposição intensa ao tabagismo causa uma redução de peso estatisticamente relevante.\nComparação entre sexos:\nMasculino vs. Feminino:\nA comparação entre os sexos revelou diferença estatisticamente significativa (p = 0,0089). Recém-nascidos do sexo masculino apresentaram, em média, peso 133,97 g superior ao dos do sexo feminino (IC95%: 43,37; 338,07).\n\n\n\n\n\n15.3.6 Tamanho do Efeito\nO eta quadrado (η²) e omega quadrado para cada fator, que indicam quanto da variância total é explicada por cada um:\n\neffectsize::eta_squared (mod.aov, partial = TRUE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\ntabagismo |           0.03 | [0.00, 1.00]\nsexo      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::omega_squared (mod.aov, partial = TRUE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 (partial) |       95% CI\n-------------------------------------------\ntabagismo |             0.02 | [0.00, 1.00]\nsexo      |             0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nValores acima de 0.01 são pequenos, 0.06 médios e acima de 0.14 grandes (segundo Cohen).\n\n\n15.3.7 Relatando os resultados\n\n15.3.7.1 Construção de um gráfico linha com teste e valor p\n\n# Comparações por pares\npwc &lt;- dados_anova %&gt;%\n  dplyr::group_by(tabagismo) %&gt;%\n  rstatix::tukey_hsd(formula = pesoRN ~ sexo)\n\n# Calcular e adicionar posições x e y\npwc &lt;- pwc %&gt;%\n  add_xy_position(fun = \"mean_ci\", \n                  x = \"tabagismo\", \n                  dodge = 0.8)\n\n# Cálculo da anova co o rstatix\nanova &lt;-  anova_test(mod.aov)\n\n# Construção do gráfico\ngl &lt;- ggpubr::ggline(dados_anova, \n                     x = \"tabagismo\", y = \"pesoRN\", \n                     add = \"mean_ci\",\n                     color = \"sexo\",\n                     linetype = \"dashed\", \n                     linewidth =  0.9,\n                     palette = \"nejm\",\n                     position = position_dodge(0.2)) +\n  theme(legend.key.size = unit(0.3, 'cm')) +\n  theme(legend.position = \"right\")+ \n  stat_pvalue_manual(pwc,\n                     label = \"p.adj.signif\", \n                     tip.length = 0.005,\n                     y.position = 3500) +\n  labs (x = \"Tabagismo\",\n        y = \"Peso do Recém-Nascido\",\n        subtitle = rstatix::get_test_label (anova, \n                                            detailed = TRUE),\n        caption = rstatix::get_pwc_label(pwc))\ngl\n\n\n\n\n\n\n\nFigura 15.18: Gráfico de linha\n\n\n\n\n\nOs resultados indicam que tanto o sexo do recém-nascido quanto o nível de tabagismo materno influenciam significativamente o peso ao nascer. Observou-se:\n\nUm efeito negativo do tabagismo intenso sobre o peso neonatal, sugerindo impacto negativo crescente conforme o nível de exposição.\nEfeito de Tabagismo (Tabela 15.4):\n\n\n\n\n\nTabela 15.4: Efeito do tabagismo sobre o peso neonatal\n\n\n\nComparaçãoDiferençaIC95%Valor pSigNão vs. Leve-113.75[-257,90; 30,41]0.153nsNão vs. Intenso-190.72[-338,07; -43,37]0.007**Leve vs. Intenso-76.97[-224,03; 70,08]0.435ns\n\n\n\n\n\n\nUm peso maior entre recém-nascidos do sexo masculino, corroborado pelos dados descritivos e pelos resultados inferenciais.\nEfeito de Sexo (Tabela 15.5):\n\n\n\n\n\nTabela 15.5: Efeito do sexo sobre o peso neonatal\n\n\n\nComparaçãoDiferençaIC95%Valor pSigMasc vs. Fem133.97[33,78; 234,16]0.008**\n\n\n\n\n\nTodos os pressupostos da ANOVA foram adequadamente atendidos, permitindo confiança nos resultados obtidos.\n\n\n\n\n\n\nAltman, Douglas G. 1991. «One way analysis of variance». Em Practical Statistics for Medical Research, 206–9. London: Chapman & Hall/CRC.\n\n\nArmitage, Peter, Geoffrey Berry, e John Nigel Scott Matthews. 2002. «Checking the Model». Em Statistical Methods in Medical Research, Fourth Edition, 356–75. Blackwell Science.\n\n\nBarton, Belinda, e Jennifer Peat. 2014. «Analysis of variance». Em Medical Statistics: A Guide to SPSS, Data Analysis and Critical Appraisal, Second Edition, 112–16. John Wiley & Sons Ltd.\n\n\nBelsley, David A., Edwin Kuh, e Roy E. Welsch. 1980. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley Series em Probability e Statistics. New York, NY: John Wiley & Sons.\n\n\nBen-Shachar, Mattan S, Daniel Lüdecke, e Dominique Makowski. 2020. «effectsize: Estimation of effect size indices and standardized parameters». Journal of Open Source Software 5 (56): 2815.\n\n\nBland, Martin. 2015. Em An Introduction to mrdical Statistics, Fourth Edition, 145–47. Oxford University Press.\n\n\nCook, R. D., e S. Weisberg. 1986. Residuals and Influence in Regression. Monographs on statistics e applied probability. Chapman; Hall. https://books.google.com.br/books?id=aMDpswEACAAJ.\n\n\nDag, Osman, Anil Dolgun, e Naime Meric Konar. 2018. «Onewaytests: An R Package for One-Way Tests in Independent Groups Designs.» R Journal 10 (1): 175–99.\n\n\nDamasio, Bruno. 2021. «Teste post hoc: O que é e qual utilizar?» Blog Psicometria Online. https://www.blog.psicometriaonline.com.br/o-que-e-um-teste-post-hoc/.\n\n\nFerreira, Daniel F, e Billy P Helms. 2011. «Aproximação normal da distribuição F». Rev. Bras. Biom. 29 (2): 222–28.\n\n\nField, Andy, Jeremy Miles, e Zoë Field. 2012a. «Comparing several means: ANOVA (GML 1)». Em Discovering Statistics Using R, 399–400. Sage Publications, Ltd.\n\n\n———. 2012b. «Influential cases». Em Discovering statistics using R, 269–71. Sage Publications, Ltd.\n\n\nHoaglin, David C., e Roy E. Welsch. 1978. «The Hat Matrix in Regression and ANOVA». The American Statistician 32: 17–22. https://api.semanticscholar.org/CorpusID:120982185.\n\n\nKassambara, Alboukadel. 2022. rstatix: Pipe-Friendly Framework for Basic Statistical Tests. https://CRAN.R-project.org/package=rstatix.\n\n\nMadi, José Mauro, Ricardo da Silva de Souza, Breno Fauth de Araujo, Petrônio Fagundes Oliveira Filho, et al. 2010. «Prevalence of toxoplasmosis, HIV, syphilis and rubella in a population of puerperal women using Whatman 903 filter paper». The Brazilian Journal of Infectious Diseases 14 (1). Elsevier: 24–29.\n\n\nMenezes, Renée Xavier de. 2004. «Análise de Variância». Em Métodos Quantitativos em Medicina, editado por Eduardo Massad, Renée Xavier de Menezes, Paulo Sérgio Panse Silveira, e Neli Regina Siqueira Ortega, 297–300. Barueri, São Paulo: Editora Manole Ltda.\n\n\nPeat, Jennifer, e Belinda Barton. 2014. «Continuous variables: analysis of variance». Em Medical statistics : a guide to SPSS, data analysis, and critical appraisal, 114. New York, NY: John Wiley & Sons.\n\n\nSchuh, Claudia Maria. 2008. «Efeitos da exposição ao fumo durante a gestação nas medidas antropométricas dos recém-nascidos». Mathesis, Universidade Federal do Rio Grande do Sul.\n\n\nWatson, Peter. 2021. «Rules of thumb on magnitudes of effect sizes». MRC Cognition and Brain Sciences Unit. Cambridge University. https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize.",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "15-anova.html#footnotes",
    "href": "15-anova.html#footnotes",
    "title": "15  Análise de Variância",
    "section": "",
    "text": "Pode ser usada também para comparar a média de duas populações e o resultado será o mesmo de um teste t para amostras independentes.↩︎\nQuando se referir a abreviação df de degree of freedom em algumas funções, optou-se por manter df ,sem a tradução para gl (graus de liberdade), como em geral se usa no texto.↩︎\nNa realidade, bem simplista!↩︎\nTestes de Monte Carlo, também conhecidos como Simulações de Monte Carlo, são uma classe de métodos computacionais que dependem de amostragem aleatória repetida para obter resultados numéricos. O nome “Monte Carlo” vem do famoso cassino em Mônaco, pois o método se baseia na aleatoriedade de um jogo de roleta.↩︎\nFiltrar recém-nascidos a termo (≥37 e &lt;42 semanas) foi uma escolha para reduzir variabilidade.↩︎\nO desbalanceamento pode introduzir vieses que podem afetar a validade das conclusões, pois grupos pequenos podem não ser representativos; os grupos maiores podem ter uma média mais estável, enquanto que os menores podem ter flutuações aleatórias ou outliers.↩︎\nA subamostragem foi realizada com controle de aleatoriedade (set.seed) para garantir reprodutibilidade, e os grupos foram praticamente igualados , permitindo aplicação da ANOVA de uma via com maior robustez e comparabilidade entre os níveis de exposição ao tabagismo.↩︎\nVolte à Seção 8.3 para mais informações sobre o como fazer gráficos no ggplot2.↩︎\nO teste estatístico também pode ser realizado com a função anova_test() do pacote rstatix que será usado na construção da Figura 15.12.↩︎\nPor isso, não apareceu a linha vermelha horizontal tracejada.↩︎\nUm método é mais robusto quando ele continua funcionando bem mesmo que algumas suposições sejam violadas, como presença de outliers ou dados não perfeitamente normais.↩︎\nFoi escolhido o nome pwc com o objtivo de lembrar que uma comparação de pares (Pairwise Comparison).↩︎\nPara saber quais modelos são suportados pela função, digite no Console do RStudio: parameters::supported_models()↩︎\nQue são as variâncias.↩︎\nPara os interessados, pode-se obter maiores informações sobre os diferentes tipos de ANOVA em https://www.r-bloggers.com/2011/03/anova-%E2%80%93-type-iiiiii-ss-explained/↩︎",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html",
    "href": "16-anovaRep.html",
    "title": "16  Anova de Medidas Repetidas",
    "section": "",
    "text": "16.1 Pacotes usados neste capítulo\npacman::p_load(afex, \n               dplyr, \n               effectsize,\n               emmeans,\n               flextable,  \n               ggplot2, \n               ggpubr, \n               ggsci, \n               knitr, \n               lmtest,\n               nlme,\n               readxl, \n               rstatix, \n               tidyr)\nApós o ajuste do modelo, modelo_lme, os valores residuais bem como os ajustados, serão incorporados ao dataframe dadosL_uni:",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html#introdução",
    "href": "16-anovaRep.html#introdução",
    "title": "16  Anova de Medidas Repetidas",
    "section": "16.2 Introdução",
    "text": "16.2 Introdução\nA ANOVA de medidas repetidas (Repeated Measures ANOVA) é uma técnica estatística usada quando o mesmo grupo de participantes é medido várias vezes, seja ao longo do tempo ou sob diferentes condições. Ela é ideal para dados longitudinais ou experimentos intra-sujeitos, onde cada sujeito serve como seu próprio controle. São exemplos típicos:\n• Medir a qualidade de vida de crianças asmáticas em vários momentos para avaliar o impacto de um programa de atenção ao asmático;\n• Testar tempos de reação em um teste em três tempos em um mesmo grupo;\n• Testar a autoestima de um mesmo grupo após a introdução de um tipo especial de dieta.\nA Tabela 16.1 mostra algumas diferenças importantes entre a ANOVA e a ANOVA de Medidas Repetidas:\n\n\n\n\nTabela 16.1: Comparação entre ANOVA e ANOVA de medidas repetidas\n\n\n\nCaracterísticaANOVA (entre sujeitos)ANOVA de Medidas Repetidas (intra-sujeitos)Tipo de AmostraGrupos independentesMesmos sujeitos em diferentes condiçõesControle da variabilidade individualNão controlaControla (cada sujeito é seu próprio controle)Sensibilidade estatísticaMenorMaior (reduz o erro residual)Suposição de esfericidadeNão se aplicaNecessária (testada com o teste de Mauchly)\n\n\n\n\n\nA ANOVA de medidas repetidas é mais poderosa estatisticamente porque reduz a variabilidade entre sujeitos, focando nas mudanças dentro de cada indivíduo. A ANOVA de medidas repetidas pode ser entendida como uma expansão da ANOVA (Anunciação 2021).",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html#anova-de-medidas-repetidas-unifatorial",
    "href": "16-anovaRep.html#anova-de-medidas-repetidas-unifatorial",
    "title": "16  Anova de Medidas Repetidas",
    "section": "16.3 ANOVA de medidas repetidas unifatorial",
    "text": "16.3 ANOVA de medidas repetidas unifatorial\nA ANOVA de medidas repetidas de um fator é um teste estatístico que compara as médias de três ou mais grupos relacionados, onde os mesmos participantes são medidos repetidamente em diferentes momentos ou condições para avaliar a mudança em uma variável dependente. É uma extensão do teste t pareado para mais de dois grupos e, por avaliar o mesmo grupo ao longo do tempo, controla diferenças individuais.\n\n16.3.1 Dados usados no exemplo\n\n\n\n\n\n\nCenário do exemplo\n\n\n\nUm estudo, realizado em um ambulatório de Doenças Respiratórias Pediátricas, avaliou a função pulmonar de um grupo de crianças por um período de 3 meses (0, 45 dias, 90 dias) usando um novo corticoide inalatório. Todas as crianças tinham idade entre 7 e 15 anos e a medida avaliadora foi PFE (Pico de Fluxo Expiratório) ou Peak Flow Meter (PFM), taxa máxima de ar expelida pelos pulmões durante uma expiração forçada), indicador importante da função pulmonar, especialmente no monitoramento de asma.\n\n\nOs dados podem ser obtidos aqui. Baixe no seu diretório de trabalho e carregue com a função read_excel() do pacote readxl. Na ANOVA de medidas repetidas de um fator, será usado apenas o grupo, denominado de caso, que usou o medicamento testado.\n\ndados_uni &lt;- readxl::read_excel(\"dados/dadosAsma.xlsx\") %&gt;% \n  filter(grupo == \"caso\")\n\nstr(dados_uni)\n\ntibble [41 × 5] (S3: tbl_df/tbl/data.frame)\n $ id   : num [1:41] 1 2 3 4 5 6 7 8 9 10 ...\n $ grupo: chr [1:41] \"caso\" \"caso\" \"caso\" \"caso\" ...\n $ t0   : num [1:41] 170 100 160 180 250 180 170 180 240 200 ...\n $ t1   : num [1:41] 180 140 180 180 300 200 180 200 320 200 ...\n $ t2   : num [1:41] 220 100 220 250 240 280 220 250 350 250 ...\n\n\n\n16.3.1.1 Exploração e transformação dos dados\nObservando a saída da função str(), verifica-se os dados se encontram no formato amplo (a variável tempo está colocada em três colunas, t0, t1, t2) e a ANOVA de medidas repetidas necessita de que os dados estejam no formato longo. Isso significa que, ao invés de ter cada medida repetida em uma coluna separada, haverá uma coluna para o identificador (id) do indivíduo, uma coluna para o tempo/condição (fator de medidas repetidas), e uma coluna para o valor da variável dependente (VD = escores).\nPara fazer esta transformação será realizada pela função pivot_longer()1do pacote tidyr (Wickham e Girlich 2022). Nesta função, no argumento data, coloca-se o nome do conjunto de dados; em cols, há necessidade informar as colunas do formato amplo que serão reunidas. No argumento names_to, nomear a coluna que conterá as colunas unificadas e em values_to, especificar o nome da variável no formato longo que conterá os valores. A variável id e a nova variável tempo devem ser convertida para fatores e o novo conjunto de dados será atribuído a um objeto nomeado dadosL_uni:\n\ndadosL_uni &lt;- dados_uni %&gt;%\n  tidyr::pivot_longer(cols = c(t0, t1, t2),\n                      names_to = \"tempo\",\n                      values_to = \"escores\") %&gt;% \n  convert_as_factor(id, tempo)\n\nstr(dadosL_uni)\n\ntibble [123 × 4] (S3: tbl_df/tbl/data.frame)\n $ id     : Factor w/ 41 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ grupo  : chr [1:123] \"caso\" \"caso\" \"caso\" \"caso\" ...\n $ tempo  : Factor w/ 3 levels \"t0\",\"t1\",\"t2\": 1 2 3 1 2 3 1 2 3 1 ...\n $ escores: num [1:123] 170 180 220 100 140 100 160 180 220 180 ...\n\n\n\n\n16.3.1.2 Sumarização dos dados\nSerão calculadas algumas estatísticas resumidas dos escores espirométricos por grupos (tempo): média e desvio padrão, usando a funções group_by() e summarise() do pacote dplyr:\n\nresumo &lt;- dadosL_uni %&gt;%\n  dplyr::group_by(tempo) %&gt;% \n  dplyr::summarise(n = n(),\n                   média = mean(escores, na.rm =TRUE),\n                   dp = sd(escores, na.rm = TRUE))\nresumo\n\n# A tibble: 3 × 4\n  tempo     n média    dp\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 t0       41  174.  42.8\n2 t1       41  205.  53.2\n3 t2       41  229.  56.1\n\n\nO resumo mostra que a média aumenta do momento inícial (t0) para o final (t2).\n\n\n16.3.1.3 Visualização dos dados\nA visualização pode ser obtida com um conjunto de boxplots (Figura 16.1), usando o pacote ggpubr com a função ggboxplot(). Este gráfico permite visualizar a variação dos escores com o tempo e a variabilidade em cada um dos momentos.\n\nggpubr::ggboxplot (dadosL_uni,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1, \n                   x = \"tempo\", \n                   y = \"escores\", \n                   color = \"black\",\n                   fill = \"lightblue\",\n                   ylab = \"Pico de Fluxo Expiratório (l/min)\",\n                   xlab = \"Tempo\",\n                   outliers = TRUE,\n                   ggtheme = theme_classic(),\n                   legend = \"none\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 16.1: Boxplots mostrando o impacto de uma medicação através do tempo\n\n\n\n\n\nOutra maneira de obter praticamente as mesmas informações visuais é através de um gráfico de linha (Figura 16.2) que mostra bem o comportamento dos escores com o tempo e em cada momento:\n\nggpubr::ggline(dadosL_uni, \n               x = \"tempo\",\n               y = \"escores\",\n               color = \"gray60\",\n               add.params = list(color = \"steelblue\"),\n               linewidth = 1,\n               linetype = \"dashed\",\n               add = c(\"mean_ci\"),\n               point.size = 1.5,\n               point.color = \"red\",\n               ggtheme = theme_classic()) +\n  ylab(\"Pico de Fluxo Expiratório (l/min)\") +\n  xlab(\"Tempo\")\n\n\n\n\n\n\n\nFigura 16.2: Boxplots mostrando o impacto de uma medicação através do tempo\n\n\n\n\n\nAmbas as figuras exibem uma modificação dos escores espirométricos (melhora nitidamente!) à medida que o tempo passa, utilizando a medicação.\n\n\n\n16.3.2 Criação do modelo de ajuste\nPara criar o modelo da ANOVA de medidas repetidas pode-se usar a função aov_car() pacote afex(Singmann et al. 2025) que fornece uma abordagem de alta robustez estatística e libera o teste de Mauchly com as correções de esfericidade.\n\n\n\n\n\n\nEstrutura do modelo aov_car\n\n\n\naov_car(valor ~ fator + Error(sujeito/fator), data = dados)\nOnde,\n\ndata: dataframe no fornato longo\nfórmula: que deve incluir\n\nvalor = variável dependente;\nfator = variável independente (fator de medidas repetidas);\nsujeito = variável que identifique o indivíduo, para um modelo de medidas repetidas de um fator - Error(id/tempo)\n\nO termo Error(id/tempo) é a chave para a análise de variância de medidas repetidas. É para o R entender que se usará um delineamento de medidas repetidas. Ele não é uma função, mas sim um símbolo de estrutura dentro da fórmula do aov() . Lê-se: “Erro aninhado em id dentro do tempo”. Este termo instrui ao R para decompor a variabilidade total do modelo em partes específicas, isolando a variância que é realmente do efeito do tratamento (tempo) que é apenas devida às diferenças individuais (id).\n\n\n\nPortanto, no exemplo em estudo, a sintaxe para o ajuste do modelo é:\n\nmodelo_aov &lt;- aov_car(escores ~ tempo + Error(id/tempo), data = dadosL_uni)\n\nEsse comando cria um objeto modelo.aov 2 que contém os resultados da ANOVA de medidas repetidas (tabela da ANOVA do tipo III) e outras informações, como o teste de Mauchly.\nPara ver a partição da variância e o teste F, usa-se a função summary():\n\nsummary(modelo_aov)\n\nWarning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps &gt; 1\ntreated as 1\n\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    \n(Intercept) 5052075      1   250720     40 806.011 &lt; 2.2e-16 ***\ntempo         61417      2    61853     80  39.718 1.047e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic p-value\ntempo        0.97503  0.6107\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(&gt;F[GG])    \ntempo 0.97564  1.879e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        HF eps   Pr(&gt;F[HF])\ntempo 1.025029 1.047466e-12\n\n\nA saída do modelo mostra a Tabela da ANOVA de Medidas repetidas de um fator (tempo) e resume os resultados do teste de esfericidade de Mauchly e as correções de Greenhouse-Geisser (GG) e Huynh-Feldt (HF).\nO teste de Mauchly avalia a suposição de esfericidade, que é o pressuposto de que as variâncias das diferenças entre todos os pares de níveis de repetição (os três momentos de medição) são iguais. Como o valor p = 0,6107 não se rejeita a H0 e se conclui que a esfericidade é válida e podemos confiar nos resultados da ANOVA univariada tipo III sem necessidade de correção. As correções de Greenhouse-Geisser e Huynh-Feldt confirmam a forte significância do efeito de tempo , entretanto, embora apresentadas, não são estritamente necessárias neste caso (veja mais sobre esfericidade na Seção 16.4.0.3).\nA tabela principal da ANOVA univariada Tipo III, assumindo esfericidade, testa a hipótese nula de que não há diferença entre as médias das medições do PFE nos três momentos de tempo.\n\nO intercepto doi significativo (p\\(&lt; 2,2 \\times 10^-16\\)), indicando que média geral é significativamente diferente de zero. Este resultado não é importante para o efeito do tratamento.\nO efeito do tempo foi significativo (F = 39,72; p \\(= 1,047 \\times 10^-12\\)), o que significa rejeitar a H0 de igualdade das médias através do tempo. O F é igual QMtempo/QM~residuo ou erro~.\nO Quadrado Médio(QM) é obtido dividindo a Soma dos Quadrados(SQ) pelos respectivos graus de liberdade (gl) e F pela razão entre os quadrados médios(variâncias):\n\n\\[QM_{tempo} = \\frac{SQ_{tempo}}{gl_{tempo}}\\]\n\\[QM_{erro} = \\frac{SQ_{erro}}{gl_{erro}}\\]\n\\[F = \\frac{QM_{tempo}}{QM_{erro}}\\]\nA Tabela da ANOVA pode ser resumida coma a Tabela 16.2.\n\n\n\n\nTabela 16.2: Tabela da ANOVA de Medidas repetidas tipo III, assumindo a esfericidade\n\n\n\nFonte de VariaçãoSoma dos QuadradosglQuadrado MédioFValor ptempo61.417230.708,50039,718&lt; 0.0001resíduo61.85380773,162\n\n\n\n\n\nExiste uma diferença estatisticamente significativa nas medições de Peak Flow Meter das crianças asmáticas ao longo dos três momentos distintos (0, 45 e 90 dias). Isto apenas indica que pelo menos dois momentos são diferentes, haverá necessidade de realizar testes post hoc.\n\n\n16.3.3 Avaliação dos pressupostos\nA ANOVA de Medidas Repetidas Unifatorial possui três pressupostos importantes e dois deles são avaliados diretamente a partir dos resíduos do modelo:\n\nNormalidade dos Resíduos\n\nAusência de valores atípicos\n\nEsfericidade (pressuposto exclusivo para medidas repetidas e é avaliado nas diferenças entre as medidas, não diretamente nos resíduos do modelo, mas é fundamental).\n\n\n16.3.3.1 Obtenção dos resíduos e valores ajustados\nPara a avaliação dos pressupostos há necessidade de obter os resíduos do modelo. Os resíduos correspondem a diferença entre os valores observados e os valores ajustados (fitted values) pelo modelo. Esses valores podem ser extraídos com a função residuals() e a função fitted() a partir do modelo. Para a avaliação dos pressupostos sera ajustado um modelo com a função lme() do pacote nlme, pois ele fornece resíduos menos dispersos, mais realistas e considera a estrutura de correlação entre medidas (Matsubara e Araujo 2019), (Pinheiro e Bates 2000).",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html#estrutura-do-modelo-lme",
    "href": "16-anovaRep.html#estrutura-do-modelo-lme",
    "title": "16  Anova de Medidas Repetidas",
    "section": "16.4 Estrutura do modelo lme",
    "text": "16.4 Estrutura do modelo lme\nO modelo_lme deve incluir o seguinte:\nmodelo_lme &lt;- lme(\n  resposta ~ efeitos fixos,\n  random = ~ estrutura_aleatoria | unidade_agrgadora,\n  correlation = estrutura_correlacao,\n  data = dataframe)\n\nParte fixa, representada pela fórmula: No exemplo, escores ~ grupo * tempo. Esta fórmula modela os escores como função dos efeitos principais de grupo e tempo, mais a interação entre eles. Isso responde como os escores variam entre grupos, ao longo do tempo, e se essa variação depende do grupo. Na ANOVA de medidas repetidas unifatorial, o fator grupo não existe porque há apenas um grupo. Nesse caso, a fórmula fica simplificada para apenas escores ~ tempo.\nParte aleatória: ramdom = ~1| id. Aqui se afirma que cada id (sujeito) tem um intercepto próprio, ou seja cada indivíduo começa de um ponto diferente. Isso captura a variabilidade individual entre sujeitos.\nEstrutura de correlação composta simétrica: correlation = corCompSymm(form = ~1 | id). Isso significa que todas as observações dentro de um mesmo sujeito (id) são igualmente correlacionadas entre si, independentemente do tempo. É uma forma simples de modelar a dependência entre medidas repetidas. Este argumento não é obrigatório, mas pode melhorar o ajuste e a interpretação do modelo. Deve ser considerado se:\n\nOs tempos são equidistantes (ex; dias, semanas);\nHá suspeita que há correlação entre as medidas dentro de cada sujeito.\nO modelo sem ele apresenta resíduos autocorrelacionados ou ajuste ruim.\n\nFonte dos dados: data  = dataframe . Dados sobre o qual o modelo será ajustado. Devem estar no formato longo (cada linha é uma observação de um sujeito em um tempo).",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html#anova-de-medidas-repetidas-bifatorial",
    "href": "16-anovaRep.html#anova-de-medidas-repetidas-bifatorial",
    "title": "16  Anova de Medidas Repetidas",
    "section": "16.5 ANOVA de medidas repetidas bifatorial",
    "text": "16.5 ANOVA de medidas repetidas bifatorial\nA ANOVA de medidas repetidas de dois fatores é um teste estatístico usado para comparar as médias de uma variável dependente entre diferentes grupos (divididos por dois fatores) onde as mesmas amostras são medidas múltiplas vezes, seja ao longo do tempo ou sob diferentes condições. Ela avalia se há diferenças significativas entre os níveis de cada fator (os dois fatores) e, principalmente, se existe uma interação entre os dois fatores.\nO estudo é organizado em torno de dois fatores (variáveis independentes). Um pode ser um fator entre sujeitos (ex: grupo de tratamento vs. grupo de controle) e outro pode ser um fator de medidas repetidas (intra-sujeitos). O principal objetivo é entender se a combinação dos dois fatores tem um efeito na variável dependente. Se uma interação for significativa, ela deve ser a prioridade na análise, pois os efeitos dos fatores isolados podem não ser tão relevantes.\n\n16.5.1 Dados usados no exemplo\nOs dados que servirão de exemplo são os mesmos da Seção 16.3.1 sem usar o filtro para os 41 casos que correspondem as crianças de 5 a 15 anos que receberam um novo corticoide inalatório. Os controle são 41 crianças da mesma idade que receberam o tratamento habitual (Figura 16.6).\n\n\n\n\n\n\n\n\nFigura 16.6: Ensaio Clínico\n\n\n\n\n\n\ndados_bi &lt;- readxl::read_excel(\"dados/dadosAsma.xlsx\") \nstr(dados_bi)\n\ntibble [82 × 5] (S3: tbl_df/tbl/data.frame)\n $ id   : num [1:82] 1 2 3 4 5 6 7 8 9 10 ...\n $ grupo: chr [1:82] \"caso\" \"caso\" \"caso\" \"caso\" ...\n $ t0   : num [1:82] 170 100 160 180 250 180 170 180 240 200 ...\n $ t1   : num [1:82] 180 140 180 180 300 200 180 200 320 200 ...\n $ t2   : num [1:82] 220 100 220 250 240 280 220 250 350 250 ...\n\n\nAssim, como realizado com a ANOVA de medidas repetidas de um fator, os dados serão transformados para o formato longo e atribuídos a um objeto denominado dadosL_bi:\n\ndadosL_bi &lt;- dados_bi %&gt;%\n  tidyr::pivot_longer(cols = c(t0, t1, t2),\n                      names_to = \"tempo\",\n                      values_to = \"escores\") %&gt;% \n  convert_as_factor(id, tempo)\n\ndadosL_bi %&gt;% sample_n_by(grupo, tempo, size = 1)\n\n# A tibble: 6 × 4\n  id    grupo    tempo escores\n  &lt;fct&gt; &lt;chr&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1 31    caso     t0        150\n2 41    caso     t1        230\n3 33    caso     t2        200\n4 50    controle t0        150\n5 57    controle t1        180\n6 50    controle t2        210\n\n\n\n16.5.1.1 Sumarização dos dados\nOs dados serão sumarizados por grupo e tempo e, em seguida, serão calculadas algumas estatísticas resumidas da variável de escore: média e sd (desvio padrão). Isso será realizado através das funções group_by() e get_summary_stats() do pacote rstatix.\n\ndadosL_bi %&gt;%\n  rstatix::group_by(grupo, tempo) %&gt;% \n  rstatix::get_summary_stats(escores, type = \"mean_sd\")\n\n# A tibble: 6 × 6\n  grupo    tempo variable     n  mean    sd\n  &lt;chr&gt;    &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 caso     t0    escores     41  174.  42.8\n2 caso     t1    escores     41  205.  53.2\n3 caso     t2    escores     41  229.  56.1\n4 controle t0    escores     41  184.  43.6\n5 controle t1    escores     41  192.  37.5\n6 controle t2    escores     41  212.  40.9\n\n\n\n\n16.5.1.2 Visualização dos dados\nPode-se observar os dados de diversas maneias. Para ANOVA medidas repetidas de dois fatores, o gráfico de linha com barra de erro é bastante útil, pois além de permitir a visualização dos dados através do tempo, pode-se ver a existência ou não de interação entre os fatores. Para criar a (Figura 16.7) serão usadas as funções ggline() do pacote ggpubr.\n\nggpubr::ggline(dadosL_bi, \n               x = \"tempo\",\n               y = \"escores\",\n               color = \"grupo\",\n               palette = \"d3\",\n               linewidth = 0.7,\n               linetype = \"dashed\",\n               add = \"mean_ci\",\n               error.plot = \"errorbar\",\n               position = position_dodge(width = 0.3),\n               point.size = 1.5,\n               ggtheme = theme_bw()) +\n  ylab(\"Pico de Fluxo Expiratório (l/min)\") +\n  xlab(\"Tempo\")\n\n\n\n\n\n\n\nFigura 16.7: Efeito do novo corticoide inalatório nos escores de Pico de Fluxo Expiratório ao longo do tempo nos grupos casos e controles\n\n\n\n\n\nA observação da Figura 16.7 mostra que ambos os grupos melhoraram a função respiratória com o tempo. O grupo caso teve uma melhora mais acentuada, o que pode indicar que a intervenção foi eficaz. O grupo controle manteve uma evolução positiva estável. A análise estatística (como ANOVA de medidas repetidas) poderia confirmar se essas diferenças são significativas ou apenas tendências visuais.\n\n\n\n16.5.2 Criação do modelo com dois fatores\nEste modelo permite testar se há diferença no tempo, independentemente do grupo; testar se há diferença entre os grupos, independentemente do tempo e verificar se há interação entre tempo e grupo, ou seja, se a mudança ao longo do tempo difere entre os grupos (por ex., se o tratamento teve efeito). Usando o formato longo, pode-se fazer o ajuste do modelo com a função aov_car() do pacote afex. Essa função também é apropriada para delineamentos com mais de um fatores: um fator intra-sujeitos (tempo); um fator entre sujeitos (grupo) e, principalmente, se houver interação, desbalanceamento e se deseja realizar uma ANOVA tipo III. Além disso, a aov_car() já calcula automaticamente o teste de esfericidade de Mauchly e as correções (Greenhouse-Geisser e Huynh-Feldt).\nVeja a sintaxe da função em Seção 16.3.2.\n\nmodelo_aov2 &lt;- aov_car(escores ~ grupo * tempo + Error(id/tempo), \n                  data = dadosL_bi, \n                  factorize = FALSE)\n\nContrasts set to contr.sum for the following variables: grupo\n\nsummary(modelo_aov2)\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(&gt;F)    \n(Intercept) 9763350      1   430255     80 1815.3608 &lt; 2.2e-16 ***\ngrupo          2923      1   430255     80    0.5435  0.463132    \ntempo         69803      2    81254    160   68.7251 &lt; 2.2e-16 ***\ngrupo:tempo    8479      2    81254    160    8.3482  0.000356 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n            Test statistic p-value\ntempo              0.95941 0.19459\ngrupo:tempo        0.95941 0.19459\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n             GG eps Pr(&gt;F[GG])    \ntempo       0.96099  &lt; 2.2e-16 ***\ngrupo:tempo 0.96099  0.0004365 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(&gt;F[HF])\ntempo       0.984147 5.875378e-22\ngrupo:tempo 0.984147 3.867194e-04\n\n\nEmbora os grupos não tenham diferenças nos escores , independente do temo (p = 0,463), a forma como os escores mudam ao longo do tempo é diferente (p &lt; 0,0001). Além disso, a evolução dos escores ao longo do tempo depende do grupo. Ou seja, existe uma interação significativa entre grupo e tempo (p = 000356).\nA esfericidade é assumida, pois o Teste de Mauchly, tanto para o tempo como para o grupo:tempo têm valores p &gt; 0,05. Portanto, mesmo que as correções de Greenhouse-Geisser e Huynh-Feldt tenham sido fornecidas, elas são desnecessárias. Isso significa, na prática, que o grupo caso teve um aumento mais acentuado nos escores ao longo do tempo do que o grupo controle. Sugere que a intervenção associada ao grupo caso teve um efeito positivo crescente. Serão necessárias comparações post hoc para ver onde estão as diferenças entre tempo e grupo.\n\n\n16.5.3 Avaliação dos pressupostos do modelo\n\n16.5.3.1 Esfericidade\nO Teste de Mauchly, obtido junto com o modelo, mostrou que tanto o tempo como a interação grupo:tempo têm valores p &gt; 0,05. Não houve violação da esfericidade. Não há necessidade das correções de GG e HF.\n\n\n16.5.3.2 Obtenção dos resíduos e valores ajustados\nDa mesma forma como realizado para a ANOVA de medidas repetidas unifatorial, para avaliar os pressupostos, há necessidade de obter os resíduos do modelo.\nA função aov_car() ajusta a ANOVA com estrutura para medidas repetidas, mas não expõe resíduos diretamente e, portanto, eles necessitam ser extraídos a partir de outro modelo de ajuste como o modelo linear de efeitos mistos (Linear mixed-effects model) através da função lme() do pacote nlme. (veja Seção 16.3.3.1).\n\nmodelo_lme &lt;- lme(\n  escores ~ grupo * tempo,\n  random = ~1 | id,\n  correlation = corCompSymm(form = ~1 | id),\n  data = dadosL_bi)\n\nA partir desse modelo, pode-se extrair os resíduos e valores ajustados e incorporá-los aos dadosL_bi, facilitando a análise do modelo.\n\ndadosL_bi$residuos &lt;- residuals(modelo_lme)\ndadosL_bi$ajustados &lt;- fitted(modelo_lme)\n\nstr(dadosL_bi)\n\ntibble [246 × 6] (S3: tbl_df/tbl/data.frame)\n $ id       : Factor w/ 82 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 2 3 3 3 4 ...\n $ grupo    : chr [1:246] \"caso\" \"caso\" \"caso\" \"caso\" ...\n $ tempo    : Factor w/ 3 levels \"t0\",\"t1\",\"t2\": 1 2 3 1 2 3 1 2 3 1 ...\n $ escores  : num [1:246] 170 180 220 100 140 100 160 180 220 180 ...\n $ residuos : Named num [1:246] 7.08 -13.11 2.45 6.51 16.31 ...\n  ..- attr(*, \"names\")= chr [1:246] \"1\" \"1\" \"1\" \"2\" ...\n  ..- attr(*, \"label\")= chr \"Residuals\"\n $ ajustados: Named num [1:246] 162.9 193.1 217.6 93.5 123.7 ...\n  ..- attr(*, \"names\")= chr [1:246] \"1\" \"1\" \"1\" \"2\" ...\n  ..- attr(*, \"label\")= chr \"Fitted values\"\n\n\n\n\n16.5.3.3 Normalidade dos resíduos\nA normalidade dos resíduos é realizada dentro de cada tempo e grupo por ser mais rigorosa. Para realizar essa ação se fará uso da função shapiro_test() do pacote rstatix e de gráficos QQ (Figura 16.8) com o ggplot2.\n\ndadosL_bi %&gt;%\n  group_by(grupo, tempo) %&gt;%\n  shapiro_test(residuos)\n\n# A tibble: 6 × 5\n  grupo    tempo variable statistic        p\n  &lt;chr&gt;    &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 caso     t0    residuos     0.892 0.000961\n2 caso     t1    residuos     0.967 0.274   \n3 caso     t2    residuos     0.980 0.674   \n4 controle t0    residuos     0.973 0.441   \n5 controle t1    residuos     0.968 0.292   \n6 controle t2    residuos     0.928 0.0126  \n\n\nGráfico QQ\n\nggplot(dadosL_bi, \n       aes(sample = residuos,\n           color = tempo)) + \n  stat_qq() + \n  stat_qq_line() + \n  facet_grid(grupo ~ tempo) + \n  labs( title = \"QQ Plot dos Resíduos por Grupo e Tempo\", \n        x = \"Quantis Teóricos\", \n        y = \"Quantis dos Resíduos\" ) + \n  theme_bw()\n\n\n\n\n\n\n\nFigura 16.8: QQ plot dos resíduos por grupo e tempo\n\n\n\n\n\nOs resultados indicam que, embora a maioria dos subgrupos apresente distribuição aproximadamente normal dos resíduos (p &gt; 0,05), foram observadas violações pontuais nos tempos t0 do grupo “caso” (p = 0,00096) e t2 do grupo “controle” (p = 0,0126). Os Q-Q plots confirmaram essas distorções com leve curvatura nas caudas. Apesar dessas exceções, os resíduos apresentaram comportamento adequado na maioria das condições avaliadas. Considerando o tamanho amostral (n = 41, por grupo) e a robustez dos modelos mistos frente a desvios moderados da normalidade, entende-se que os pressupostos foram suficientemente atendidos para a validade da análise.\n\n\n16.5.3.4 Pesquisa de outliers nos residuos\nA presença de outliers serão analisados, usando função identify_outliers() do pacote rstatix e através de boxplots.\n\ndadosL_bi %&gt;%\n  dplyr::group_by(grupo, tempo) %&gt;%\n  rstatix::identify_outliers(residuos)\n\n# A tibble: 8 × 8\n  grupo    tempo id    escores residuos ajustados is.outlier is.extreme\n  &lt;chr&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 caso     t0    13        150    -55.2      205. TRUE       FALSE     \n2 caso     t0    20        120    -33.9      154. TRUE       FALSE     \n3 caso     t0    26        110    -55.9      166. TRUE       FALSE     \n4 caso     t0    30        200    -40.8      241. TRUE       FALSE     \n5 controle t2    47        300     24.1      276. TRUE       FALSE     \n6 controle t2    48        300     24.1      276. TRUE       FALSE     \n7 controle t2    77        280     40.3      240. TRUE       FALSE     \n8 controle t2    78        300     48.2      252. TRUE       TRUE      \n\n\nBoxplots\n\nggpubr::ggboxplot (dadosL_bi,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1, \n                   x = \"tempo\", \n                   y = \"residuos\", \n                   color = \"black\",\n                   fill = \"grupo\",\n                   ylab = \"Pico de Fluxo Expiratório (l/min)\",\n                   xlab = \"Tempo\",\n                   outliers = TRUE,\n                   ggtheme = theme_classic(),\n                   legend = \"top\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 16.9: Boxplots mostrando o efeito de um corticoide inalatório\n\n\n\n\n\nA análise dos resíduos do modelo ajustado por efeitos mistos (lme) identificou 8 observações como outliers (Figura 16.9), sendo uma classificada como outlier extremo (id = 78). Essas observações não comprometem os pressupostos da homocedasticidade e normalidade dos resíduos. Assim, não foi indicada a exclusão de dados, mantendo-se a integridade da amostra para fins analíticos.\n\n16.5.3.4.1 Análise da influência dos outliers\nPara análise de influência em modelos de efeitos mistos (lme), usa-se abordagens adaptadas, já que funções como cooks.distance() não são aplicáveis diretamente e necessitam de um modelo equivalente mais simples:\n\n# Ajuste de modelo linear simples para o teste (sem efeitos aleatórios)\nmodelo_lm &lt;- lm(escores ~ grupo * tempo, data = dadosL_bi)\n\n# Distância de Cook\ncooksD &lt;- cooks.distance(modelo_lm)\n\n# Alavancagem\nleverage &lt;- hatvalues(modelo_lm)\n\nAdicionar a distância de Cook e os valores da leverage (alavancagem) nos dadosL_bi:\n\ndadosL_bi$cooksD &lt;- cooksD\n\ndadosL_bi$leverage &lt;- leverage\n\nCriar uma variável com observações influentes:\n\ndadosL_bi$is.influential &lt;- dadosL_bi$cooksD &gt; (4 / nrow(dadosL_bi))\n\nVisualizar em um gráfico , construído com o ggplot2:\n\nggplot(dadosL_bi, \n       aes(x = seq_along(cooksD), y = cooksD)) +\n  geom_bar(stat = \"identity\") +\n  geom_hline(yintercept = 4 / nrow(dadosL_bi), \n             linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Cook's Distance\", \n       x = \"Observação\", \n       y = \"Distância de Cook\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigura 16.10: Boxplots mostrando o efeito de um corticoide inalatório\n\n\n\n\n\nO gráfico da distância de Cook (Figura 16.10) mostra que a maioria das observações tem baixa influência (valores próximos de zero). Algumas observações ultrapassam a linha de corte (em vermelho), definida como 4/n (n = número de observações). No exemplo, o ponto de corte é \\(4/246 \\approx 0,016\\), isto indica possível influência desproporcional no modelo. Entretanto, não há muitos pontos extremos, o que é um bom sinal. Após esse passo, deve-se identificar esses valores influentes e avaliá-los:\n\ndadosL_bi %&gt;% filter(is.influential) %&gt;% \n  select(id, grupo, tempo, escores, cooksD, leverage)\n\n# A tibble: 11 × 6\n   id    grupo tempo escores cooksD leverage\n   &lt;fct&gt; &lt;chr&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 2     caso  t2        100 0.0334   0.0244\n 2 5     caso  t1        300 0.0182   0.0244\n 3 9     caso  t1        320 0.0267   0.0244\n 4 9     caso  t2        350 0.0293   0.0244\n 5 16    caso  t1        300 0.0182   0.0244\n 6 21    caso  t1        110 0.0179   0.0244\n 7 30    caso  t1        308 0.0214   0.0244\n 8 30    caso  t2        320 0.0166   0.0244\n 9 35    caso  t2        320 0.0166   0.0244\n10 40    caso  t0        270 0.0183   0.0244\n11 40    caso  t1        300 0.0182   0.0244\n\n\nForam identificados 11 observações influentes segundo a Distância de Cook, todas no grupo caso e com escores extremos (muito baixos ou muito altos). Os valores de Cook’s D estão acima do limiar (~0.016), mas ainda não são alarmantes (valores &gt; 1 seriam muito preocupantes). Os leverage values (~0.0244) também são baixos, indicando que essas observações não têm posição incomum nos preditores, mas o valor da variável dependente (escore) é que impacta mais. Como esses valores não se constituem em um erro de digitação ou inconsistência nos escores de PEF, os valores serão mantidos. Essa abordagem não será demonstrada, pois já foi realizada pelo autor e as conclusões não mudam.\n\n\n\n\n\n\nScript completo para para comparar os modelos.\n\n\n\n\n# 1. Remover observações influentes\ndadosL_sem_inf &lt;- dadosL_bi %&gt;% \n  filter(!is.influential)\n\n# 2. Ajustar modelo com todos os dados\nmodelo_completo &lt;- aov_car(\n  escores ~ grupo * tempo + Error(id/tempo), \n  data = dadosL_bi,\n  factorize = TRUE)\n\n# 3. Ajustar modelo sem observações influentes\nmodelo_sem_inf &lt;- aov_car(\n  escores ~ grupo * tempo + Error(id/tempo), \n  data = dadosL_sem_inf,\n  factorize = TRUE)\n\n# 4. Comparar os resultados\nsummary(modelo_completo)\nsummary(modelo_sem_inf)\n\n\n\n\n\n\n16.5.3.5 Homoscedasticidade\nA homoscedasticidade será avaliada por um gráfico de dispersão dos resíduos:\n\nggplot(dadosL_bi, aes(x = ajustados, y = residuos)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Resíduos vs. Valores Ajustados\",\n    x = \"Valores Ajustados\",\n    y = \"Resíduos\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\nFigura 16.11: Gráfico de dispersão resíduos vs valores ajustados\n\n\n\n\n\nObservando a Figura 16.11 , verifica-se uma boa dispersão dos pontos ao redor da linha zero,  sem padrão claro (como funil ou curva) e variação aproximadamente constante ao longo dos ajustados.\nPortanto, não há evidências visuais fortes de violação da homocedasticidade. O pressuposto de variâncias constantes entre as combinações de grupo e tempo parece razoavelmente atendido.\nO gráfico já atende o requisito para avaliar a homoscedasticidade, mas também é possível aplicar o teste de Breusch-Pagan, que precisa de um modelo ajustado com lm() (veja Seção 16.5.3.4.1). Como o modelo foi ajustado com a lme(), há necessidade de ajustar um modelo equivalente sem efeitos aleatórios, apenas para o teste:\n\nbptest(modelo_lm)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo_lm\nBP = 12.822, df = 5, p-value = 0.02511\n\n\nO valor p &lt; 0.05 indica que há evidência de heteroscedasticidade — ou seja, a variância dos resíduos não é constante entre os grupos ou tempos. Isso viola o pressuposto de homoscedasticidade, que é importante para testes F na ANOVA clássica.\nComo o modelo principal é uma ANOVA de medidas repetidas via aov_car() ou lme(), esses modelos já são mais robustos à heteroscedasticidade do que o lm() usado no teste.\nApesar da significância estatística, o efeito observado foi pequeno, e o uso de um modelo misto (lme), que acomoda estruturas de variância flexíveis, reduz a influência dessa violação sobre os resultados inferenciais.\n\n\n16.5.3.6 Resumo da avaliação dos pressupostos\nA adequação dos pressupostos foi avaliada para o modelo de ANOVA de medidas repetidas de dois fatores ajustado com a função aov_car() do pacote afex, considerando as variáveis grupo (caso, controle) e tempo (t0, t1, t2), com medidas repetidas por sujeito (id).\n1. Normalidade dos resíduos:\nA normalidade foi verificada por meio do teste de Shapiro-Wilk para os resíduos intra-sujeitos em cada combinação de grupo e tempo. Alguns desvios foram observados, com valores p &lt; 0.05 para casos em t0 e controles em t2, sugerindo violação da normalidade em partes dos dados. Ainda assim, os resíduos em geral apresentaram distribuição aproximadamente simétrica nos gráficos Q-Q , sendo a violação considerada moderada. Como a ANOVA é relativamente robusta a desvios da normalidade, especialmente em amostras maiores, o modelo foi mantido.\n2. Outliers:\nForam detectadas 8 observações outliers com base nos resíduos do modelo ajustado com lme(). Uma dessas foi classificada como extremo, e todas associadas a valores de escores distantes da média esperada. Adicionalmente, a análise de influência (Cook’s Distance) identificou 11 observações influentes, principalmente no grupo caso, indicando possível impacto em estimativas do modelo.\nA comparação entre os modelos com e sem essas observações revelou que a exclusão dos influentes não alterou as conclusões principais: o efeito de tempo permaneceu altamente significativo (p &lt; 0.001), o efeito de grupo não foi significativo (p &gt; 0.46), e a interação grupo × tempo continuou significativa (p &lt; 0.001), embora com leve redução no F. Isso sugere que o modelo é estável e robusto, mesmo na presença de influências pontuais.\n3. Esfericidade:\nA esfericidade, pressuposto fundamental em ANOVA com medidas repetidas, foi avaliada com o teste de Mauchly, que não indicou violação para tempo nem para a interação grupo × tempo (p &gt; 0.19). As correções de Greenhouse-Geisser e Huynh-Feldt foram consistentes, mantendo a significância dos efeitos, o que reforça a confiabilidade da análise.\n4. Homoscedasticidade:\nA homogeneidade de variâncias foi avaliada graficamente e formalmente via teste de Breusch-Pagan em modelo lm paralelo, retornando p = 0.025, o que indica heterocedasticidade leve. Essa violação reforça a importância de validar os resultados com modelos mistos (lme), que lidam melhor com esse tipo de estrutura de erro.\nConclusão:\nApesar de desvios pontuais nos pressupostos (normalidade, homoscedasticidade), o modelo demonstrou consistência estatística e robustez analítica. A remoção de observações influentes não alterou os resultados principais, fortalecendo a confiança nas conclusões. A abordagem adotada, que incluiu modelos alternativos e verificação cuidadosa de resíduos, garante a validade da inferência realizada com a ANOVA de medidas repetidas.\n\n\n\n16.5.4 Testes post hoc\nConsiderando que a interação grupo:tempo foi significativa (ver Seção 16.5.2), se justifica explorar as diferenças dentro e entre grupos ao longo do tempo.\nEmbora de tanto o modelo construído com aov_car(), do pacote afex, quanto o modelo com lme(), do pacote nlme, possam ser utilizados para ajustar modelos de ANOVA de medidas repetidas, a opção para os pós testes, será será pelo uso do lme(), assim como realizado com os resíduos. Existem alguns motivos para isso:\n\nNos pós-testes, será usado o pacote emmeans e ele funciona melhor com o lme(), pois reconhece muito bem o modelo misto e permite pós-testes com estrutura correta (por exemplo, comparações dentro de sujeitos);\nFacilidade para extrair resíduos, valores ajustados, influências etc. A aov_car() encapsula o modelo de forma menos transparente (resultado mais voltado ao relatório da ANOVA – tipo III, teste de Mauchly, GG/HF, etc.). Com lme(), se tem acesso direto ao modelo e seus componentes.\nMais controle da estrutura de correlação (caso se precise modelar estruturas mais complexas de medidas repetidas).\n\n\n16.5.4.1 Comparações entre níveis de tempo dentro de cada grupo\nPara entender como os escores mudam ao longo do tempo em casos e controles separadamente, recomenda-se:\n\nemmeans_tempo_dentro_grupo &lt;- emmeans(modelo_lme, ~ tempo | grupo)\ncontrastes_tempo_dentro &lt;- contrast(emmeans_tempo_dentro_grupo, \n                                    method = \"pairwise\", \n                                    adjust = \"tukey\")\nsummary(contrastes_tempo_dentro)\n\ngrupo = caso:\n contrast estimate   SE  df t.ratio p.value\n t0 - t1     -30.2 4.98 160  -6.067  &lt;.0001\n t0 - t2     -54.6 4.98 160 -10.977  &lt;.0001\n t1 - t2     -24.4 4.98 160  -4.910  &lt;.0001\n\ngrupo = controle:\n contrast estimate   SE  df t.ratio p.value\n t0 - t1      -7.8 4.98 160  -1.568  0.2625\n t0 - t2     -27.8 4.98 160  -5.586  &lt;.0001\n t1 - t2     -20.0 4.98 160  -4.018  0.0003\n\nDegrees-of-freedom method: containment \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nA saída mostra que, no grupo caso, houve diferenças significativas entre todos os tempos. Isso sugere que o PFE aumentou progressivamente ao longo do tempo no grupo que recebeu o novo corticoide inalatório. No grupo controle , o padrão indica que a melhora foi mais tardia (apenas entre t1 e t~2), possivelmente refletindo um efeito menor ou mais lento do medicamento.\n\n\n16.5.4.2 Comparações entre os grupos dentro de cada tempo\nPara entender se há diferença entre casos e controles em t0, t1, t2:\n\nemmeans_grupo_dentro_tempo &lt;- emmeans(modelo_lme, ~ grupo | tempo)\ncontrastes_grupo_dentro &lt;- contrast(emmeans_grupo_dentro_tempo, \n                                    method = \"pairwise\", \n                                    adjust = \"tukey\")\nsummary(contrastes_grupo_dentro)\n\ntempo = t0:\n contrast        estimate   SE df t.ratio p.value\n caso - controle    -9.51 10.2 80  -0.933  0.3537\n\ntempo = t1:\n contrast        estimate   SE df t.ratio p.value\n caso - controle    12.88 10.2 80   1.263  0.2103\n\ntempo = t2:\n contrast        estimate   SE df t.ratio p.value\n caso - controle    17.32 10.2 80   1.698  0.0933\n\nDegrees-of-freedom method: containment \n\n\nEm nenhum dos tempos a diferença entre os grupos foi estatisticamente significativa. Apesar de t2 mostrar tendência de diferença (p = 0.0933), não alcança significância convencional estabelecida de 0,05. Ainda assim, pode ter relevância clínica, dependendo do contexto.\nOs resultados reforçam o achado da interação significativa entre grupo e tempo na ANOVA. O grupo “caso” apresentou evolução mais consistente ao longo das avaliações, enquanto o grupo “controle” teve melhora mais discreta e tardia. A ausência de diferenças entre os grupos em cada tempo sugere que os efeitos se acumulam ao longo do tempo, mais visivelmente nos tratados (casos).\n\n\n\n16.5.5 Relatando os resultados da ANOVA de medidas repetidas de dois fatores\nO resultado pode ser relatado da seguinte forma:\n\nUma ANOVA de medidas repetidas de dois fatores foi conduzida para avaliar o efeito de um novo corticosteroide inalatório em crianças asmáticas, com idades entre 7 e 15 anos, sobre os escores de Pico de Fluxo Expiratório (PFE), ao longo de três momentos (t0, t1, t2).\nObservou-se uma interação estatisticamente significativa entre grupo e tempo no escore de PFE, F(2, 160) = 8,34, p &lt; 0,00036. Em função dessa interação, as comparações foram realizadas separadamente dentro de cada grupo. Os valores de p foram ajustados pelo método de Bonferroni\n\nGrupo caso: houve diferenças significativas entre todos os tempos (t0 vs. t1: p &lt; 0,0001; t0 vs. t2: p &lt; 0,0001; t1 vs. t2: p &lt; 0,0001), indicando melhora progressiva.\nGrupo controle: não houve diferença significativa entre t0 e t1 (p = 0,265), mas houve entre t0 e t2 (p &lt; 0,0001) e entre t1 e t2 (p = 0,0003), sugerindo um efeito mais tardio.\n\nAs comparações entre grupos dentro de cada tempo não revelaram diferenças estatisticamente significativas (p &gt; 0,05 em todos os casos), embora uma tendência tenha sido observada em t2 (p = 0,0933), sem atingir significância convencional.\n\nRecomenda-se acompanhar as conclusões com um gráfico (Figura 16.12).\n\n# Ajuste de um modelo com rstatix::anova_test()\nmod &lt;- dadosL_bi %&gt;% \n  rstatix::anova_test(dv = escores, \n             wid = id, \n             within = tempo,\n             between = grupo,\n             type = 3) \n\nget_anova_table(mod) \n\nANOVA Table (type III tests)\n\n       Effect DFn DFd      F        p p&lt;.05   ges\n1       grupo   1  80  0.544 4.63e-01       0.006\n2       tempo   2 160 68.725 2.86e-22     * 0.120\n3 grupo:tempo   2 160  8.348 3.56e-04     * 0.016\n\n# Comparações por pares (pwc)\npwc &lt;- dadosL_bi %&gt;%\n  dplyr::group_by(tempo) %&gt;%\n  pairwise_t_test(escores ~ grupo, \n                  paired = TRUE,\n                  p.adjust.method = \"bonferroni\")\npwc\n\n# A tibble: 3 × 11\n  tempo .y.   group1 group2    n1    n2 statistic    df     p p.adj p.adj.signif\n* &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 t0    esco… caso   contr…    41    41     -1.04    40 0.305 0.305 ns          \n2 t1    esco… caso   contr…    41    41      1.27    40 0.212 0.212 ns          \n3 t2    esco… caso   contr…    41    41      1.68    40 0.101 0.101 ns          \n\n# Valores p e posição no eixo y\npvalores &lt;- dadosL_bi %&gt;%\n  group_by(tempo) %&gt;%\n  pairwise_t_test(escores ~ grupo, \n                  paired = TRUE,\n                  p.adjust.method = \"bonferroni\") %&gt;%\n  mutate(y.position = 250)\n\n# Gráfico final\nggplot(dadosL_bi, \n       aes(x = tempo, \n           y = escores, \n           color = grupo, \n           group = grupo)) +\n  stat_summary(fun = mean, \n               geom = \"line\", \n               linewidth = 0.8, \n               linetype = \"dashed\",\n               position = position_dodge(0.3)) +\n  stat_summary(fun.data = mean_cl_normal, \n               geom = \"errorbar\", \n               width = 0.2, \n               position = position_dodge(0.3)) +\n  stat_summary(fun = mean, \n               geom = \"point\", \n               size = 2, \n               color = \"black\", \n               position = position_dodge(0.3)) +\n  geom_text(data = pvalores, \n            aes(x = tempo, \n                y = y.position, \n                label = paste0(\"p = \", signif(p, 3))),\n            inherit.aes = FALSE, size = 4) +\n  labs(x = \"Tempo\", \n       y = \"Pico de Fluxo Expiratório (l/min)\", \n       title = \"Evolução dos escores por grupo\",\n       subtitle = get_test_label(mod, detailed = TRUE),\n       caption = get_pwc_label(pwc)) +\n  theme_classic(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\nFigura 16.12: Efeito do novo corticoide inalatório nos escores de Pico de Fluxo Expiratório ao longo do tempo nos em crianças asmáticas de 7 a 15 anos\n\n\n\n\n\n\n\n\n\n\n\nAnunciação, Luis. 2021. «Conceitos e análises estatisticas com R e jasp». ANOVA de medidas repetidas. https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html.\n\n\nGirden, Ellen R. 1992. «Two-Factor Studies with Repeated Measures on Both Factors». Em ANOVA: repeated measures, 84:31–40. QASS Series. Sage.\n\n\nHuynh, Huynh, e Leonard S Feldt. 1976. «Estimation of the Box correction for degrees of freedom from sample data in randomized block and split-plot designs». Journal of Educational Statistics 1 (1). Sage Publications: 69–82.\n\n\nMatsubara, Lídia, e Luis F. C. Araujo. 2019. «Cap. 13 ANOVA de medidas repetidas». https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html; Bookdown. https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html.\n\n\nMicrosoft Copilot. 2025. «Assistente de Inteligência Artificial utilizado para apoio estatístico e redação técnica». https://copilot.microsoft.com.\n\n\nMuller, Keith E, e Curtis N Barton. 1989. «Approximate power for repeated-measures ANOVA lacking sphericity». Journal of the American Statistical Association 84 (406). Taylor & Francis: 549–55.\n\n\nPinheiro, José C., e Douglas M. Bates. 2000. «Fitting Linear Mixed-Effects Models». Em Mixed-Effects Models in S and S-PLUS, 133–99. New York, NY: Springer New York.\n\n\nSingmann, Henrik, Ben Bolker, Jake Westfall, Frederik Aust, e Mattan S. Ben-Shachar. 2025. afex: Analysis of Factorial Experiments. doi:10.32614/CRAN.package.afex.\n\n\nWickham, Hadley, e Maximilian Girlich. 2022. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "16-anovaRep.html#footnotes",
    "href": "16-anovaRep.html#footnotes",
    "title": "16  Anova de Medidas Repetidas",
    "section": "",
    "text": "Veja a Seção 14.3.1.1 para mais detalhes da função↩︎\nObjeto da classe “afex_aov”.↩︎",
    "crumbs": [
      "Parte VI - Testes Paramétricos",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Anova de Medidas Repetidas</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html",
    "href": "17-correlacao.html",
    "title": "17  Correlação",
    "section": "",
    "text": "17.1 Pacotes usados neste capítulo\n#|message: false\n#|warning: false\npacman::p_load(car,\n               dplyr,\n               flextable,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               knitr,\n               lmtest,\n               readxl,\n               rstatix)",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#introdução",
    "href": "17-correlacao.html#introdução",
    "title": "17  Correlação",
    "section": "17.2 Introdução",
    "text": "17.2 Introdução\nA correlação é usada para avaliar a força e a direção da relação entre duas variáveis numéricas contínuas, normalmente distribuídas. A maneira mais comum de mostrar a relação entre duas variáveis quantitativas é através de um diagrama ou gráfico de dispersão (scatterplot). A Figura 17.1 exibe um exemplo de um gráfico de dispersão, onde se observa um padrão geral que sugere uma relação entre o estriol urinário (mg/24h) e o peso fetal em uma gravidez normal (Greene Jr e Touchstone 1963).\n\n\n\n\n\n\n\n\nFigura 17.1: Correlação entre a excreção de estriol urinário e peso fetal\n\n\n\n\n\nO gráfico de dispersão mostra que os valores de uma variável aparecem no eixo horizontal x e os valores da outra variável aparecem no eixo vertical y. Cada indivíduo nos dados aparece como o ponto no gráfico fixado pelos valores de ambas as variáveis para aquele indivíduo. Normalmente, eixo x é a variável explicativa (ou variável explanatória ou independente) e y a variável desfecho (variável resposta ou dependente).\nEm um diagrama de dispersão deve-se procurar o padrão geral e desvios marcantes desse padrão. Verifica-se o padrão geral, observando a direção, a forma e força do relacionamento. Um tipo importante de desvio é um valor atípico, um valor individual que está fora do padrão geral do relacionamento.\nA Figura 17.1 mostra uma clara direção do padrão geral que se move da esquerda inferior para a direita superior. Este comportamento é denominado de correlação positiva entre as variáveis. A forma do relacionamento é aproximadamente uma linha reta com uma ligeira curva para a direita à medida que se move para cima. A força de uma correlação em um gráfico de dispersão é determinada pela proximidade dos pontos em uma forma clara. No caso, quanto mais se aproxima de uma reta, mais forte é a associação, no caso de uma correlação linear. Duas variáveis estão negativamente associadas quando se comportam de forma oposta ao da Figura 17.1.\nObviamente, nem todos os diagramas de dispersão mostram uma direção clara que permita descrever como correlação positiva ou negativa e não tem uma forma linear, sugerindo que não há correlação, como a Figura 17.2.\n\n\n\n\n\n\n\n\nFigura 17.2: Gráfico de dispersão sugerindo ausência de correlação",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#coeficiente-de-correlação-de-pearson",
    "href": "17-correlacao.html#coeficiente-de-correlação-de-pearson",
    "title": "17  Correlação",
    "section": "17.3 Coeficiente de correlação de Pearson",
    "text": "17.3 Coeficiente de correlação de Pearson\nA correlação é quantificada pelo Coeficiente de Correlação Linear de Pearson. Este coeficiente paramétrico, denotado por r, é um número adimensional, independente das unidades usadas para medir as variáveis x e y.\nSuponha que se tenha dados sobre as variáveis x e y para n indivíduos. Os valores para o primeiro indivíduo são \\({x}_{1}\\) e \\({y}_{1}\\), os valores para o segundo indivíduo são \\({x}_{2}\\) e \\({y}_{2}\\) e assim por diante. As médias e desvios padrão das duas variáveis são \\(\\bar{x}\\) e \\({s}_{x}\\) para os valores de x e \\(\\bar{y}\\) e \\({s}_{y}\\) para os valores de y. A correlação r entre x e y é dada pela equação:\n\\[\nr = \\frac{\\sum{(x_{1} - \\bar{x})(y_{1} - \\bar{y})}}{{\\sqrt{\\sum (x_{1} - \\bar{x})^2\\times\\sum (y_{1} - \\bar{y})^2}}}\n\\]\nO Coeficiente de Correlação, r, apresenta as seguintes características:\n\nÉ um valor numérico que varia de -1 a +1 (Figura 17.3):\n\nQuando r = -1, há uma correlação linear negativa ou inversa perfeita;\nQuando r = +1, há uma correlação linear positiva ou direta perfeita;\nQuando r = 0, não há correlação entre as variáveis.\n\n\n\n\n\n\n\n\n\n\nFigura 17.3: Coeficiente de Correlação\n\n\n\n\n\n\nQuanto mais os pontos se aproximam de uma linha reta, maior a magnitude de r.\nO coeficiente de correlação r é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (leia-se rô).\nA correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação.\nComo r usa os valores padronizados das observações, r não muda se as unidades de medida de x, y ou ambos são modificados. A correlação r em si não tem unidade de medida; é apenas um número.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#sec-execor",
    "href": "17-correlacao.html#sec-execor",
    "title": "17  Correlação",
    "section": "17.4 Dados usados no exemplo",
    "text": "17.4 Dados usados no exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEstá bem definido que existe uma relação entre a idade de crianças e a sua altura (comprimento). Os dados de 40 crianças com idade entre 18 e 36 meses (20 meninos e 20 meninas) foram coletados em um ambulatório pediátrico. A idade foi registrada em meses e o comprimento em centímetros.\n\n\nOs dados estão no banco de dadosdadosReg.xlsx que pode ser obtido aqui. Baixe e salve o arquivo no seu diretório de trabalho.\n\n17.4.1 Leitura e exploração dos dados\nA função read_excel do pacote readxl será usada para carregar o arquivo. Observar os dados com a função str().\n\ndados &lt;- read_excel(\"dados/dadosReg.xlsx\")\nstr(dados)\n\ntibble [40 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ...\n $ comp  : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ...\n $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ...\n $ sexo  : chr [1:40] \"masc\" \"fem\" \"masc\" \"fem\" ...\n\n\nDe acordo com uma das exigências da correlação, as variáveis idade e comp pertencem a classe das variáveis numéricas. A variável sexo foi lida como um variável numérica e será transformada em fator:\n\ndados$sexo &lt;- as.factor(dados$sexo)\n\nApós esta pequena manipulação, será feita a sumarização dos dados.\n\n\n17.4.2 Sumarização e visualização dos dados\nEsta ação será realizada com a função get_summary_stats () do pacote rstatix que necessita dos seguintes argumentos:\n\ndados %&gt;%\n  rstatix::get_summary_stats(idade,\n                             comp,\n                             type = \"mean_sd\")\n\n# A tibble: 2 × 4\n  variable     n  mean    sd\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 idade       40  27.0  5.41\n2 comp        40  90.2  6.00\n\n\nPara visualizar os dados, será usado o gráfico de dispersão (Figura 17.4), usando a função geom_point() do pacote ggplot2:\n\n\n\n\n\n\n\n\nFigura 17.4: Coeficiente de Correlação\n\n\n\n\n\nA separação dos pontos por sexo, usando cores diferentes, não muda a análise e foi realizada apenas para treinamento (e curiosidade!), pois não há motivo para que haja diferença na correlação entre os sexos.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#sec-precor",
    "href": "17-correlacao.html#sec-precor",
    "title": "17  Correlação",
    "section": "17.5 Pressupostos da correlação",
    "text": "17.5 Pressupostos da correlação\nA primeira e mais importante etapa antes de analisar os dados, usando a correlação de Pearson, é verificar se é apropriado usar este teste estatístico.\nSerão discutidos sete pressupostos, três estão relacionados com o projeto do estudo e como as variáveis foram medidas (pressupostos 1, 2 e 3) e quatro que se relacionam com as características dos dados (pressupostos 4, 5, 6 e 7) (Kassambara 2021).\n\nVariáveis numéricas contínuas\n\nAs duas variáveis devem ser medidas em uma escala contínua (são medidas no nível intervalar ou de razão). No exemplo, tanto a variável idade como o comprimento (comp) são variáveis contínuas.\n\nVariáveis devem estar como pares\n\nAs duas variáveis contínuas devem ser emparelhadas, o que significa que cada caso (por exemplo, cada participante) tem dois valores: um para cada variável.\n\nIndependência das observações\n\nDeve haver independência de casos, o que significa que as duas observações para um caso (por exemplo, a idade e o comprimento) devem ser independentes das duas observações para qualquer outro caso.\nSe estes pressupostos forem atendidos, avalia-se os outros pressupostos:\n\nRelação linear entre as variáveis\n\nO coeficiente de correlação de Pearson é uma medida da força de uma associação linear entre duas variáveis. Dito de outra forma, ele determina se há um componente linear de associação entre duas variáveis contínuas. Por esse motivo, verifica-se a relação entre duas variáveis, em um gráfico de dispersão, para ver se a execução de uma correlação de Pearson é a melhor escolha como medida de associação.\nA variável idade é colocada como variável preditora (eixo x) e comp como desfecho (eixo y). O gráfico de dispersão anterior, mostra uma nítida correlação linear.\n\nNormalidade das variáveis\n\nPara verificar se as variáveis têm distribuição normal, é possível usar o teste de Shapiro-Wilk, usando a função shapiro_test(), incluída no pacote rstatix:\n\ndados %&gt;% shapiro_test(idade, comp)\n\n# A tibble: 2 × 3\n  variable statistic     p\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 comp         0.958 0.141\n2 idade        0.958 0.145\n\n\nO teste de Shapiro-Wilk de ambas as variáveis retorna um valorp&gt; 0,05, indicando que não é possível rejeitar a \\(H_{0}\\); os dados seguem a distribuição normal, portanto o pressuposto foi atendido.\n\nPesquisa de valores atípicos\n\nA identificação dos valores atípicos pode ser feita usando a função identify_outliers() do pacote rstatix.\n\n dados %&gt;% identify_outliers(idade)\n\n[1] id         idade      comp       irmaos     sexo       is.outlier is.extreme\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n dados %&gt;% identify_outliers(comp)\n\n[1] id         idade      comp       irmaos     sexo       is.outlier is.extreme\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n\n\nHomoscedasticidade\n\nA homocedasticidade assume que os dados são igualmente distribuídos sobre a linha de regressão. Descreve uma situação na qual o resíduo é o mesmo em todos os valores das variáveis independentes. A heterocedasticidade (a violação da homocedasticidade) está presente quando o tamanho dos resíduos difere entre os valores de uma variável independente.\nO impacto de violar o pressuposto da homocedasticidade é uma questão de grau, aumentando à medida que a heterocedasticidade aumenta. Dessa forma, avalia-se a homocedasticidade, observando os resíduos.\nUma correlação linear pode ser descrita por uma reta. Em uma correlação linear perfeita, a reta passa por todos os pontos. Normalmente, não é possível traçar uma reta que passe por todos os pontos. A melhor reta é aquela que promove o melhor ajuste,ou seja, é aquela cuja distância dos pontos até a reta é a menor possível. Os resíduos são a diferença entre o valor observado e o valor previsto pelo melhor ajuste, estabelecido pelo modelo de regressao linear.\n\nConstrução do modelo: Para ajustar a um modelo linear, usa-se a função lm que deve conter um objeto da classe formula tipo (x ~ y) como argumento. Demais características da função podem ser obtidas com ?lm ou direto na ajuda do RStudio. O modelo será atribuído a um objeto denominado mod_reg.\n\n\nmod_reg &lt;- lm(comp ~ idade, dados)\n\n\nAnálise gráfica da homoscedasticidade: Pode ser feita através dos gráficos diagnósticos para a regressão linear, utilizados para verificar se o modelo funciona bem para representar os dados. Uma forma de avaliar é verificar como as variâncias se comportam. Os gráficos diagnósticos são apresentados de quatro maneiras diferentes.1 Neste estágio, serão avaliados o primeiro e o terceiro tipo (Figura 17.5).\n\n\npar(mfrow=c(1,2))  # muda o layout do painel para 1 linha e 2 colunas\nplot(mod_reg, which=c(1,3))\npar(mfrow=c(1,1))  # retorna o layaout do painel para o padrão de 1 linha e 1 coluna\n\n\n\n\n\n\n\nFigura 17.5: Gráficos diagnósticos 1 e 3\n\n\n\n\n\nNo gráfico 1, tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero.\nNo gráfico 3 (valores ajustados x resíduos), tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero. Permite verificar se há outliers - valores de resíduos padronizados acima de 3 ou abaixo de -3. Embora o gráfico possa dar uma ideia sobre homocedasticidade, às vezes, um teste mais formal é preferido. Existem vários testes para isso, mas aqui será utilizado o Teste de Breusch-Pagan (Halunga, Orme, e Yamagata 2017). A \\(H_{0}\\) e a \\(H_{1}\\) podem ser consideradas como:\n\n\\(H_{0}\\): Homocedasticidade. Os resíduos têm variância constante sobre o modelo verdadeiro.\n\\(H_{1}\\): Heterocedasticidade. Os resíduos não têm variância constante sobre o modelo verdadeiro.\n\nSe o valor p &gt; 0,05 não se rejeita a \\(H_{0}\\) de homocedasticidade. O teste de Breusch-Pagan é encontrado na função bptest(), incluída no pacote lmtest:\n\nlmtest::bptest(mod_reg)\n\n\n    studentized Breusch-Pagan test\n\ndata:  mod_reg\nBP = 0.049988, df = 1, p-value = 0.8231\n\n\nOs resultados não indicam heteroscedasticidade e isso é bom. Desta forma, pode-se aplicar a equação final de predição.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#execução-do-teste-de-correlação",
    "href": "17-correlacao.html#execução-do-teste-de-correlação",
    "title": "17  Correlação",
    "section": "17.6 Execução do teste de correlação",
    "text": "17.6 Execução do teste de correlação\n\n17.6.1 Coeficiente de correlação de Pearson (r)\nO coeficiente de correlação, r, é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (rô).\nComo visto, no início deste capítulo, a correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação.\nComo o r usa os valores padronizados das observações, não muda nada se as unidades de medida de x, y ou ambas são modificadas. A correlação r em si não tem unidade de medida; é apenas um número.\nO cálculo pode ser realizado com a função cor_test() do pacote rstatix que usa os seguintes argumentos:\n\ndata \\(\\to\\) dataframe contendo as variáveis;\n… \\(\\to\\) Uma ou mais expressões (ou nomes de variáveis) sem aspas separadas por vírgulas. Usado para selecionar uma variável de interesse. Alternativa ao argumento vars. Ignorado quando vars é especificado;\nvars2 \\(\\to\\) • vetor de caracteres opcional. Se especificado, cada elemento em vars será testado em relação a todos os elementos em vars2. Aceita nomes de variáveis sem aspas: c(var1, var2);\nalternative \\(\\to\\) hipótese alternativa “two.sided” (bilateral) ou “greater” ou “less” (unilateral a direita ou a esquerda, respectivamente);\nmethod \\(\\to\\) ⟶ qual coeficiente de correlação deve ser usado para o teste. Um dos termos “pearson”, “kendall” ou “spearman” pode ser abreviado;\nconf.level \\(\\to\\) nivel de confiança. Padrão 0.95.\n\n\nr &lt;- dados %&gt;% cor_test(idade, \n                         comp, \n                         method = \"pearson\")\nr\n\n# A tibble: 1 × 8\n  var1  var2    cor statistic        p conf.low conf.high method \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 idade comp   0.96      21.4 7.87e-23    0.927     0.979 Pearson\n\n\nA saída do Coeficiente de Correlação de Pearson (r) é igual 0.96 (IC95%: 0.93, 0.98) o que corresponde a uma correlação linear muito forte (Tabela 17.1)) entre a idade e o comprimento de crianças (Schober, Boer, e Schwarte 2018).\nO coeficiente refere a existência de correlação linear, mas não especifica se a relação é de causa e efeito. O valorpespecifica se a correlação é igual a zero (\\(H_{0}\\)) ou diferente de zero (\\(H_{1}\\)). No caso, ela é diferente de zero.\nO importante é a magnitude do r, entretanto, o coeficiente r e o valorpdevem ser interpretados em conjunto. Se o valorp&gt; 0,05, mesmo que r seja diferente de zero, a correlação não deveria ser interpretada.\n\n\n\n\nTabela 17.1: Interpretação do Coeficiente de Correlação\n\n\n\nCoeficiente de Correlação (r)Interpretação0,0&lt;0,3desprezável0,3&lt;0,5fraca0,5,0,7moderada0,7&lt;0,9forte0,9&lt;1,0muito forte1,0perfeita\n\n\n\n\n\nTalvez a melhor maneira de interpretar a correlação linear é elevar o valor do r ao quadrado para obter o Coeficiente de Determinação (\\(R^{2}\\)). No exemplo usado, tem-se que o \\(R^{2}\\) é igual a \\(0,96^{2} = 0,922\\), então, 92,2% da variação do comprimento da criança (y) podem ser explicados, nesses dados, pela variação da sua idade (x), fato mais ou menos óbvio!.\n\n\n17.6.2 Coeficiente de correlação de Spearman (\\(\\rho\\))\nSe os pressupostos são violados é recomendado o uso de correlação não paramétrica (veja a Seção 20.2), incluindo testes de correlação baseados em postos (veja a Seção 20.3) de Spearman e Kendall (De Winter, Gosling, e Potter 2016).\nPara calcular o coeficente, Usar a mesma função da correlação de Pearson, mudando o argumento method:\n\nrho &lt;- dados %&gt;% cor_test(idade, \n                           comp, \n                           method = \"spearman\")\nrho\n\n# A tibble: 1 × 6\n  var1  var2    cor statistic        p method  \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 idade comp   0.96      448. 3.30e-22 Spearman\n\n\n\n\n17.6.3 Coeficiente de correlação de Kendall (\\(\\tau\\))\nO coeficiente de correlação de postos de Kendall ou estatística tau de Kendall é usado para estimar uma medida de associação baseada em postos. Pode ser usado com variáveis ordinais ou quando não existe relação linear entre as variáveis. Uma vantagem sobre o coeficiente de Spearman é a possibilidade de ser generalizado para um coeficiente de correlação parcial. Deve ser usada ao invés do coeficiente de Spearman quando se tem um conjunto pequeno de dados com um grande número de postos empatados (veja a Seção 20.3). Para o cálculo desse coeficiente, continua-se com a mesma função anterior, mudando o method = “kendall”.\n\ntau &lt;- dados %&gt;% cor_test(idade, \n                           comp, \n                           method = \"kendall\")\ntau\n\n# A tibble: 1 × 6\n  var1  var2    cor statistic        p method \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1 idade comp   0.85      7.57 3.64e-14 Kendall\n\n\nNo caso normal, a correlação de Kendall é mais robusta e eficiente que a correlação de Spearman. Isso significa que a correlação de Kendall é preferida quando há amostras pequenas ou alguns valores atípicos. O rho de Spearman geralmente é maior que o tau de Kendall.\n\n\n\n\n\n\nDe Winter, Joost CF, Samuel D Gosling, e Jeff Potter. 2016. «Comparing the Pearson and Spearman correlation coefficients across distributions and sample sizes: A tutorial using simulations and empirical data.» Psychological methods 21 (3). American Psychological Association: 273.\n\n\nGreene Jr, John W, e Joseph C Touchstone. 1963. «Urinary estriol as an index of placental function. A study of 279 cases». Obstetrical & Gynecological Survey 18 (3). LWW: 356–59.\n\n\nHalunga, Andreea G., Chris D. Orme, e Takashi Yamagata. 2017. «A heteroskedasticity robust Breusch–Pagan test for Contemporaneous correlation in dynamic panel data models». Journal of Econometrics 198 (2): 209–30. doi:https://doi.org/10.1016/j.jeconom.2016.12.005.\n\n\nKassambara, Alboukadel. 2021. «Correlation Test between two variables in R». STHDA - Statistical tools for high-throughput data analysis. http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r.\n\n\nSchober, Patrick, Christa Boer, e Lothar A Schwarte. 2018. «Correlation coefficients: appropriate use and interpretation». Anesthesia & Analgesia 126 (5). Wolters Kluwer: 1763–68.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "17-correlacao.html#footnotes",
    "href": "17-correlacao.html#footnotes",
    "title": "17  Correlação",
    "section": "",
    "text": "Maiores detalhes sobre os testes diagnósticos podem ser encontrados em: https://data.library.virginia.edu/diagnostic-plots/↩︎",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlação</span>"
    ]
  },
  {
    "objectID": "18-regressao.html",
    "href": "18-regressao.html",
    "title": "18  Regressão Linear Simples",
    "section": "",
    "text": "18.1 Pacotes usados neste capítulo\n#|message: false\n#|warning: false\npacman::p_load(car,\n               dplyr,\n               flextable,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               knitr,\n               lmtest,\n               readxl,\n               rstatix)",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regressão Linear Simples</span>"
    ]
  },
  {
    "objectID": "18-regressao.html#introdução",
    "href": "18-regressao.html#introdução",
    "title": "18  Regressão Linear Simples",
    "section": "18.2 Introdução",
    "text": "18.2 Introdução\nA regressão linear simples, assim como a correlação, é uma técnica usada para explorar a natureza da relação entre duas variáveis aleatórias contínuas. A principal diferença entre esses dois métodos analíticos é que a regressão permite investigar a alteração em uma variável, chamada resposta, correspondente a uma determinada alteração em outra, conhecida como variável explicativa. A regressão é um modelo matemático que permite a predição de uma variável resposta a partir de uma outra variável explicativa. A análise de correlação quantifica a força da relação entre as variáveis, tratando-as simetricamente (Sedgwick 2013).\nA regressão linear simples é chamada assim, porque existe apenas uma variável independente. Se houver mais de uma variável independente, é chamada de regressão múltipla.\nA representação matemática do modelo de regressão linear populacional é descrita pela equação da reta de melhor ajuste em um conjunto de pares de dados (x, y) em um gráfico de dispersão de pontos.\n\\[\ny = \\beta_{0} + \\beta_{1}x\n\\]\n\n\n\n\n\n\n\n\nFigura 18.1: Reta de regressão, coeficiente angular e coeficiente linear\n\n\n\n\n\nA inclinação da reta de regressão (\\(\\beta_{1}\\)) determina a variação de y para cada unidade de variação de x e recebe o nome de coeficiente angular ou de regressão. O ponto de interceptação da reta com y quando x é igual a zero é \\(\\beta_{0}\\) e é denominado de coeficiente linear (Figura 18.1). A equação da reta de regressão amostral que estima a reta de regressão populacional é igual a:\n\\[\n\\hat {y} = b_{0} + b_{1}x\n\\]\nA reta do diagrama de dispersão da Figura 18.1 é a melhor reta de ajuste aos dados.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regressão Linear Simples</span>"
    ]
  },
  {
    "objectID": "18-regressao.html#dados-usados-no-exemplo",
    "href": "18-regressao.html#dados-usados-no-exemplo",
    "title": "18  Regressão Linear Simples",
    "section": "18.3 Dados usados no exemplo",
    "text": "18.3 Dados usados no exemplo\nSerão usados os mesmos dados do exemplo da Correlação (Seção 17.4). A função read_excel do pacote readxl carregará o arquivo.\n\ndados &lt;- read_excel(\"dados/dadosReg.xlsx\")\nstr(dados)\n\ntibble [40 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ...\n $ comp  : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ...\n $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ...\n $ sexo  : chr [1:40] \"masc\" \"fem\" \"masc\" \"fem\" ...\n\n\nA variável sexo, como realizado na Correlação, será transformada em fator:\n\ndados$sexo &lt;- as.factor(dados$sexo)\n\nRevise no capítulo de correlação a sumarização dos dados.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regressão Linear Simples</span>"
    ]
  },
  {
    "objectID": "18-regressao.html#resíduos",
    "href": "18-regressao.html#resíduos",
    "title": "18  Regressão Linear Simples",
    "section": "18.4 Resíduos",
    "text": "18.4 Resíduos\nNo exemplo. usado na correlação (Seção 17.4), verificou-se que existe uma correlação linear entre o a idade e o comprimento de crianças, usando uma amostra de 40 crianças entre 18 e 36 meses. A correlação de Pearson foi muito forte (r = 0,96, p &lt; 0,00001). Esta relação linear pode ser descrita pela reta, mostrada na Figura 18.2.\n\nggplot2::ggplot(dados, \n                aes(x = idade, \n                    y = comp,\n                    color = \"tomato\")) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"steelblue\") +\n  geom_point(size = 3.5) +\n  theme_classic() + \n  xlab(\"Idade (meses\") +\n  ylab(\"Comprimento (cm)\") +\n  theme(text = element_text(size = 12)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 18.2: Reta de regressão\n\n\n\n\n\nNão é possível traçar uma reta que passe por todos os pontos. Esta reta ideal descreveria uma correlação perfeita, que não é o caso. Pode haver várias retas, a reta calculada pela regressão linear é aquela que promove o melhor ajuste, ou seja, é aquela cuja distância dos pontos até a reta é a menor possível.\nOs resíduos são a diferença entre o valor observado e o valor previsto pelo modelo de regressão linear, construído anteriormente (mod_reg). A técnica estatística para achar a melhor reta que ajusta um conjunto de dados é denominada de método dos mínimos quadrados (Ordinary Least Square). A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada.\nPara se obter os resíduos, como realizado na correlação, ao se avaliar os pressupostos (Seção 17.5), foi ajustado um modelo de reressão da seguinte maneira:\n\nmod_reg &lt;- lm(comp ~ idade, dados)\n\nE após, os seguintes comandos:\n\n# Obter e salvar os valores preditos e residuais\ndados$previsto &lt;- predict(mod_reg) \n\ndados$residuos &lt;- residuals(mod_reg)\n\nUsando as variáveis criadas pode criar o gráfico da Figura 18.3 que mostra os resíduos do modelo:\n\nggplot2::ggplot(dados, \n                aes(x = idade, \n                    y = comp,\n                    color = \"tomato\")) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"steelblue\") +\n  geom_segment(aes(xend = idade, \n                   yend = previsto), \n               linewidth = 0.8,\n               linetype = \"dotted\") +\n  geom_point(aes(y = previsto), \n             shape = 19,\n             size = 3,\n             colour = \"steelblue\") +\n  geom_point(size = 3) +\n  theme_classic() + \n  xlab(\"Idade (meses\") +\n  ylab(\"Comprimento (cm)\") +\n  theme(text = element_text(size = 12)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 18.3: Resíduos do Modelo de Regressão Linear\n\n\n\n\n\nUma boa maneira de testar a qualidade do ajuste do modelo é observar os resíduos (Kim 2019) ou as diferenças entre os valores reais (pontos vermelhos) e os valores previstos (pontos azuis). A reta de regressão, em azul no gráfico, representa os valores previstos. A linha vertical pontilhada da linha reta até o valor dos dados observados é o resíduo.\nA ideia aqui é que a soma dos resíduos seja aproximadamente zero ou o mais baixo possível. Na vida real, a maioria dos casos não seguirá uma linha perfeitamente reta, portanto, resíduos são esperados. Na saída do resumo da função lm() em (mod_reg$residuals), você pode ver estatísticas descritivas sobre os resíduos do modelo (residuals), elas mostram como os resíduos são aproximadamente zero. Pode-se observar isso, usando a função summary () e sum():\n\nsummary(mod_reg$residuals)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.20326 -1.20326  0.08994  0.00000  1.25849  3.49221 \n\nsum(mod_reg$residuals)\n\n[1] -3.538836e-15\n\n\nComo se observa, a soma dos residuos é praticamente iguais a zero (\\(-3,54 \\times 10^-15\\)).",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regressão Linear Simples</span>"
    ]
  },
  {
    "objectID": "18-regressao.html#análise-dos-pressupostos-do-modelo-de-regressão",
    "href": "18-regressao.html#análise-dos-pressupostos-do-modelo-de-regressão",
    "title": "18  Regressão Linear Simples",
    "section": "18.5 Análise dos pressupostos do modelo de regressão",
    "text": "18.5 Análise dos pressupostos do modelo de regressão\nA análise exploratória do conjunto de dados foi feita quando do estudo da Correlação. Assim como a correlação, a regressão linear faz várias suposições sobre os dados.\n\n18.5.1 Gráficos diagnóstico\nOs gráficos de diagnóstico da regressão (Figura 18.4) podem ser criados usando a função plot() do R base, como mostrado para a correlação. O modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função. A função par(mfrow = 2, 2) foi utilizada, como de outras vezes, para colocar os gráficos em duas linhas e duas colunas:\nO modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função:\n\npar(mfrow=c(2,2))\nplot (mod_reg)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\nFigura 18.4: Gráficos diagnósticos\n\n\n\n\n\nOs gráficos de diagnóstico mostram resíduos de quatro maneiras diferentes:\n\nResíduos vs. ajustados (Residuals vs Fitted). Usado para verificar os pressupostos de relação linear. Uma linha horizontal, sem padrões distintos é um indicativo de uma relação linear, o que é bom. Os dados do exemplo (linha vermelha) afastam-se muito pouco do zero, mas a acompanham e não se observa nenhum padrão distinto, como uma parábola por exemplo.\nQ-Q plot. Usado para examinar se os resíduos são normalmente distribuídos. É bom se os pontos residuais seguirem a linha reta tracejada. É possível dizer que os resíduos seguem a linha diagonal, com pequenos desvios toleráveis.\nLocalização da dispersão (scale-location). Usado para verificar a homogeneidade de variância dos resíduos (homocedasticidade). Uma linha horizontal com pontos igualmente dispersos é uma boa indicação de homocedasticidade. No exemplo usado, os resíduos parecem estar dispersos e a linha vermelha não está próxima do zero, sugerindo um problema com a homocedasticidade, entretanto, não está acima de 3.\nResíduos vs. alavancagem (leverage). Usado para identificar casos influentes, ou seja, valores extremos que podem influenciar os resultados da regressão quando incluídos ou excluídos da análise. Nem todos os outliers são influentes na análise de regressão linear. Mesmo que os dados tenham valores extremos, eles podem não ser influentes para determinar uma linha de regressão. Isso significa que os resultados não seriam muito diferentes, incluindo ou não esses valores. Por outro lado, alguns casos podem ser muito influentes, mesmo que pareçam estar dentro de uma faixa razoável de valores. Outra forma de colocar, é que eles não se entendem com a tendência na maioria dos casos. Ao contrário dos outros gráficos, desta vez os padrões não são relevantes. Deve-se estar atento aos valores distantes no canto superior direito ou no canto inferior direito. Esses pontos são os lugares onde os casos podem ter influência contra uma linha de regressão. Procurar casos fora de uma linha tracejada, distância de Cook. Quando os casos estão fora da distância de Cook (o que significa que têm pontuações altas de distância de Cook), os casos são influentes para os resultados da regressão. Os resultados da regressão serão alterados se excluirmos esses casos.\n\nA aparência dos gráficos do exemplo mostra que não há nenhum caso influente. Pouco se observa as linhas de distância de Cook (uma linha tracejada) porque todos os casos estão bem dentro das linhas de distância de Cook.\n\n\n18.5.2 Avaliação da normalidade dos resíduos\nAo analisar os pressupostos da correlação, foi realizado a avaliação da normalidade nos dados brutos que indicaram não ser possível rejeitar a hipótese nula de que os dados têm distribuição normal. Agora, isto será repetido para avaliar a normalidade dos resíduos, usando o mesmo teste, teste de Shapiro-Wilk.\nAo ser criado o modelo de regressão (mod_reg), ele fornece uma série de variáveis que pode ser listada da seguinte maneira:\n\nls(mod_reg)\n\n [1] \"assign\"        \"call\"          \"coefficients\"  \"df.residual\"  \n [5] \"effects\"       \"fitted.values\" \"model\"         \"qr\"           \n [9] \"rank\"          \"residuals\"     \"terms\"         \"xlevels\"      \n\n\nUsando a variável residuals, confirma-se o observado no QQPlot de que os resíduos apresentam distribuição normal, pois o valor de p &gt; 0,05.\n\nshapiro_test(mod_reg$residuals)\n\n# A tibble: 1 × 3\n  variable          statistic p.value\n  &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n1 mod_reg$residuals     0.979   0.655\n\n\nA saída retorna a estatística do teste de Shapiro-Wilk com um valor P = 0,655, mostrando que os dados se ajustam à distribuição normal. Isto confirma a impressão da Figura 18.4.\n\n\n18.5.3 Pesquisa de valores atípicos nos resíduos\nExiste uma função pode ser usada para verificar valores atípicos nos resíduos da regressão para modelos lineares como rstandard() do pacote stats, que analisa os resíduos padronizados.\nA função padroniza todos os resíduos e inclui no objeto residuos_p. Para analisá-los, faz-se um sumário, usando a função summary(). Esta função exibirá os a estatística dos 5 números mais a média para os resíduos padronizados:\n\nresiduos_p &lt;- rstandard(mod_reg)\nsummary(residuos_p)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-1.9327846 -0.7271178  0.0548028  0.0006059  0.7779208  2.1154118 \n\n\nEm uma amostra normalmente distribuída, ao redor de 95% dos valores estão entre –1,96 e +1,96, 99% deve estar entre –2,58 e +2,58 e quase todos (99,9%) deve se situar entre –3,09 e +3,09.\nPortanto, resíduos padronizados com um valor absoluto maior que 3 são motivo de preocupação porque em uma amostra média é improvável que aconteça um valor tão alto por acaso (Field, Miles, e Field 2012).\nSe a saída da função rstandard() for comparada com o eixo y do gráfico Residuals vs Leverage, dos gráficos diagnósticos, verifica-se valores semelhantes que variam abaixo de 3 e acima de -3, indicando que não há outliers influenciando e a mediana está próxima de zero.\n\n18.5.3.1 Homocedasticidade dos resíduos\nNa Seção 17.5, foi analisada a homocedasticidade , onde se viu que o teste de Breusch-Pagan, retornou um resultado de P = 0,8231, indicando que a variância permanece praticamente constante, havendo homocedasticidade nos resíduos.\nO problema mais sério associado à heterocedasticidade é o fato de que os erros padrão são tendenciosos. Como o erro padrão é fundamental para a realização de testes de significância e cálculo de intervalos de confiança, os erros padrão tendenciosos levam a conclusões incorretas sobre a significância dos coeficientes de regressão. No geral, no entanto, a violação da suposição de homocedasticidade deve ser bastante grave para apresentar um grande problema, dada a natureza robusta da regressão pelo método ordinary least-squares. No entanto, é importante que a equação final de predição seja aplicada apenas a populações com as mesmas características da amostra do estudo.\n\n\n18.5.3.2 Independência dos resíduos\nOs resíduos no modelo devem ser independentes, ou seja, não devem ser correlacionados entre si. Para verificar isso, pode-se executar o teste Durbin-Watson - teste dw (Durbin e Watson 1950), utilizando a função durbinWatsonTest() do pacote ´car`. O teste retorna um valor entre 0 e 4. Um valor maior que 2 indica uma correlação negativa entre resíduos adjacentes, enquanto um valor menor que 2 indica uma correlação positiva. Se o valor for dois, é provável que exista independência. Existe uma sugestão de que valores abaixo de 1 ou mais de 3 são um motivo definitivo de preocupação (Field, Miles, e Field 2012). É importante mencionar que o teste tem como pressuposto a normalidade dos dados.\n\ndurbinWatsonTest(mod_reg)\n\n lag Autocorrelation D-W Statistic p-value\n   1      -0.1044054      2.204843    0.67\n Alternative hypothesis: rho != 0\n\n\nComo na saída do teste o valor p &gt; 0,05 e a estatística DW é igual a 2,2, não se rejeita a hipótese nula de independência (rho = 0).\n\n\n\n18.5.4 Tamanho amostral na regressão\nO tamanho da amostra deve ser suficiente para suportar o modelo de regressão. É importante coletar dados suficientes para obter um modelo de regressão confiável. O tamanho da amostra necessário para suportar um modelo depende do valor do coeficiente de correlação do modelo (no caso da correlação linear simples é o r de Pearson) e do número de variáveis incluídas.\nA Tabela 18.1 mostra o número de participantes necessários em modelos com 1 a 4 preditores independentes. Como se observa, o requisito de tamanho da amostra aumenta com o número de variáveis preditoras.\n\n\n\n\nTabela 18.1: Tamanho amostral para regressão de acordo com o r de Pearson e o número de preditores\n\n\n\nNúmero de Variáveis Preditorasr de PearsonUmaDuasTrêsQuatro0.21902302652900.3801001151250.445556570\n\n\n\n\n\nExistem muitas regras práticas, sugerindo o tamanho da amostra. Uma delas, diz que se deve ter 10 a 15 casos por variável preditora no modelo. Entretanto, essas regras podem ser duvidosas e o melhor é calcular o tamanho amostral baseado no tamanho do efeito, usando, por exemplo o site StatToDo ou com o sofware GPower 3.1.9.7 (Faul et al. 2007).\n\n\n18.5.5 Realização da regressão linear\nApós analisar os pressupostos do modelo de regressão do exemplo, verificou-se que as variáveis idade e comprimento da criança têm relação linear, que os resíduos do modelo têm distribuição normal, que existe homoscedasticidade e que não há pontos influentes. E, portanto, o modelo permite que se realize uma análise de regressão linear para avaliar a relação entre as variáveis independentes e dependentes.\nPara realizar uma análise de regressão linear simples e verificar os resultados, há necessidade de executar dois comandos. O primeiro, que cria o modelo linear já foi realizado na análise dos gráficos e será repetido aqui. O segundo, imprime o resumo do modelo com a função summary():\n\nmod_reg &lt;- lm (comp ~ idade, dados)\nsummary (mod_reg)\n\n\nCall:\nlm(formula = comp ~ idade, data = dados)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2033 -1.2033  0.0899  1.2585  3.4922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 61.44408    1.36466   45.02   &lt;2e-16 ***\nidade        1.06515    0.04967   21.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.678 on 38 degrees of freedom\nMultiple R-squared:  0.9237,    Adjusted R-squared:  0.9217 \nF-statistic: 459.9 on 1 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nA saída da função summary() primeiro apresenta como o modelo foi obtido e, em seguida, resume os resíduos do modelo. Por último, tem-se os Coeficientes:\n\nAs estimativas (Estimate) para os parâmetros do modelo - o valor do intercepto y (neste caso, 61,44) e o efeito estimado da idade sobre o comprimento (1,1)- significam que para cada unidade de aumento na idade se espera um aumento de 1,1 cm no comprimento.\nO erro padrão dos valores estimados (Std. Error).\nA estatística de teste (t value)\nO valor P (Pr (&gt;| t |)), também conhecido como a probabilidade de encontrar a estatística t fornecida se a hipótese nula de nenhuma correlação for verdadeira.\n\nAs três linhas finais são os diagnósticos do modelo - o mais importante a observar é o valor P (\\(2,2\\times 10^{-16}\\)), que indica se o modelo se ajusta bem aos dados.\n\nA partir desses resultados, pode-se dizer que existe uma correlação positiva significativa entre idade e comprimento (valor P &lt; 0,001), com um aumento de 1,1 cm no comprimento para cada aumento de 1 mês no na idade , possibilitando a previsão comprimento da criança pela idade.\nEstes dados são empregados para formular a equação do modelo de regressão da seguinte maneira:\n\\[\n\\hat {y} = 61,44 + 1,1 x\n\\]\nO erro padrão das estimativas são fornecidos. Esses dados permitem calcular o IC95%. Ou pode-se usar a função confint() do pacote stats, que será colocada dentro da função round() para arredondar os valores até um digito.\n\nround (confint (mod_reg, level = 0.95), 1)\n\n            2.5 % 97.5 %\n(Intercept)  58.7   64.2\nidade         1.0    1.2\n\n\nDessa forma, é possível prever que uma criança de 30 meses, de acordo com o modelo, terá o seguinte comprimento:\n\ncomp_30m &lt;- 61.4 + 1.1 * 30\ncomp_30m\n\n[1] 94.4\n\n\n\nlim.sup &lt;- 64.2 + 1.2*30\nlim.inf &lt;- 58.7 + 1.0*30\nprint (c(lim.inf, lim.sup))\n\n[1]  88.7 100.2\n\n\nOu seja, espera-se que uma criança tenha, aos 30 meses de idade, um comprimento médio de 94,4 cm (IC95%: 88,7-100,2)\n\n\n18.5.6 Visualização dos resultados\nSerá obtido um gráfico de dispersão com a reta de regressão e seu intervalo de confiança de 95% (Figura 18.5). Além disso, adicionou-se a equação do modelo de regressão (o R arredondou os valores), juntamente com o coeficiente de determinação \\(R^{2}\\).\n\nggplot2:: ggplot (dados, aes (x = idade, y = comp)) +\n  geom_point (size = 3) +\n  geom_smooth (method = \"lm\", se = TRUE, color = \"tomato\") +\n  stat_regline_equation (label.y = 100, aes (label = (..eq.label..))) + \n  stat_regline_equation (label.y = 99, aes (label = (..rr.label..))) +       \n  theme_classic () +\n  xlab (\"Idade (meses)\") +\n  ylab (\"Comprimento(cm)\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 18.5: Resultado da Regressão Linear\n\n\n\n\n\nNo gráfico, o intervalo de previsão médio de 95% em torno da reta de regressão é um intervalo de confiança de 95%, ou seja, a área na qual há 95% de certeza de que a reta de regressão verdadeira se encontra (Altman e Gardner 1988). Esta banda de intervalo é levemente curvada porque os erros na estimativa do intercepto e da inclinação são incluídos em adição ao erro na previsão da variável desfecho.\nSe for observado, o IC95% da reta de regressão obbtida pelo ggplot2 difere um pouco do IC95% da função confint(). Isto ocorre porque:\n\nquando se usa geom_smooth(method = \"lm\", se = TRUE), o intervalo de confiança gerado é baseado na incerteza da previsão média da regressão. Ou seja, ele mostra a faixa onde se espera que a média da variável dependente (comp) esteja para um determinado valor da variável independente (idade) e\n\nquando se usa a função confint(), ela retorna o intervalo de confiança dos coeficientes do modelo de regressão. Ou seja, ela fornece a incerteza associada aos parâmetros estimados (incluindo o intercepto e os coeficientes das variáveis preditoras).\n\nA principal diferença, portanto, é que o intervalo de confiança do ggplot2 reflete a incerteza da linha de regressão ajustada, enquanto confint() fornece a incerteza dos parâmetros do modelo.\n\n\n\n\n\n\nAltman, Douglas G, e Martin J Gardner. 1988. «Statistics in Medicine: Calculating confidence intervals for regression and correlation». British Medical Journal (Clinical research ed.) 296 (6631). BMJ Publishing Group: 1238.\n\n\nDurbin, James, e Geoffrey S Watson. 1950. «Testing for serial correlation in least squares regression: I». Biometrika 37 (3/4). JSTOR: 409–28.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, e Axel Buchner. 2007. «G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences». Behavior research methods 39 (2). Springer: 175–91.\n\n\nField, Andy, Jeremy Miles, e Zoë Field. 2012. «Regression». Em Discovering statistics using R, 266–76. Sage Publications, Ltd.\n\n\nKim, Hae-Young. 2019. «Statistical notes for clinical researchers: simple linear regression 3–residual analysis». Restorative dentistry & endodontics 44 (1). Korean Academy of Conservative Dentistry.\n\n\nSedgwick, Philip. 2013. «Correlation versus linear regression». BMJ 346. British Medical Journal Publishing Group.",
    "crumbs": [
      "Parte VII - Relações entre Variáveis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regressão Linear Simples</span>"
    ]
  },
  {
    "objectID": "19-dadoscategoricos.html",
    "href": "19-dadoscategoricos.html",
    "title": "19  Análise de Dados Categóricos",
    "section": "",
    "text": "19.1 Pacotes necessários\npacman::p_load(coin,\n                DescTools,\n                dplyr,\n                expss,\n                flextable,\n                ggplot2, \n                gmodels,\n                nhstplot,\n                readxl,\n                rstatix,\n                summarytools)",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "19-dadoscategoricos.html#sec-qui",
    "href": "19-dadoscategoricos.html#sec-qui",
    "title": "19  Análise de Dados Categóricos",
    "section": "19.2 Qui-Quadrado",
    "text": "19.2 Qui-Quadrado\nDois testes de hipótese são proeminentes na pesquisa na área da saúde. Um é o teste t de duas amostras, que é usado para testar a igualdade de duas médias populacionais independentes. O segundo é o teste qui-quadrado (denotado por \\(\\chi^{2}\\)). O teste é denominado teste qui-quadrado porque usa a distribuição qui-quadrado ou \\(\\chi^{2}\\).\n\n19.2.1 Distribuição qui-quadrado\nSe uma variável X é normalmente distribuída, então a variável \\(X^{2}\\) tem uma distribuição qui-quadrado (Altman 1991). A distribuição qui-quadrado com k categorias é a distribuição de uma soma dos quadrados de k variáveis aleatórias independentes com distribuição normal. O número de categorias determina o número de graus de liberdade. O formato da distribuição qui-quadrado depende desses graus de liberdade. Em geral, ela é assimétrica com apenas valores positivos, iniciando em zero. A assimetria diminui à medida que aumentam os graus de liberdade. Para cada grau de liberdade tem-se curvas de distribuição diferentes.\n\ncurve(dchisq(x, df = 5), from = 0, to = 60, col = \"royalblue\", lwd =2, bty = \"n\")\ncurve(dchisq(x, df = 10), from = 0, to = 60, col = \"red\", lwd =2, add = T)\ncurve(dchisq(x, df = 15), from = 0, to = 40, col = \"orange\", lwd =2, add = T)\ncurve(dchisq(x, df = 20), from = 0, to = 40, col = \"cyan\", lwd =2, add = T)\ncurve(dchisq(x, df = 30), from = 0, to = 60, col = \"green3\", lwd =2, add = T)\nbox(bty = \"L\")\nlegend (legend=c (\"gl = 05\", \"gl = 10\", \"gl = 15\", \"gl = 20\", \"gl = 30\"), \n        fill = c (\"royalblue\", \"red\", \"orange\", \"cyan\", \"green3\"), \n        bty=\"n\", \n        cex = 1,\n        x =\"right\")\n\n\n\n\n\n\n\nFigura 19.1: Distribuição do Qui-Quadrado\n\n\n\n\n\nA distribuição \\(\\chi^{2}\\) converge para a distribuição normal à medida que os graus de liberdade aumentam, de acordo com o teorema do limite central, entretanto esta convergência é lenta (Figura 19.1).\nA distribuição qui-quadrado tem duas aplicações comuns: primeiro, como um teste para saber se duas variáveis categóricas são independentes ou não (Teste de independência ou associação); segundo, o teste de qualidade do ajuste do qui-quadrado (Teste de aderência ou ajuste) que é usado para comparar uma determinada distribuição com uma distribuição conhecida.\n\n\n19.2.2 Lógica da estatística do qui-quadrado\nO cálculo da estatística \\(\\chi^{2}\\) é baseado nas frequências existentes nas células da tabela de contingência. Em primeiro lugar, calcula-se as frequências que se espera em cada célula caso a hipótese nula seja verdadeira (frequências esperadas). Em segundo lugar, usando a equação geral, o teste mede o grau de discrepância entre o conjunto de frequências observadas (O) e o conjunto de frequências esperadas (E).\n\\[\n\\chi^{2}= \\sum \\left [\\frac{\\left (O_{i} - E_{i} \\right )^2}{E_{i}} \\right]\n\\]\nSe \\(O_{i}\\) é muito semelhante ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é baixo; se \\(O_{i}\\) é muito diferente em relação ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é alto.\nAs frequências observadas são o número de sujeitos ou objetos na amostra que se enquadram nas várias categorias da variável de interesse. As frequências esperadas são o número de sujeitos ou objetos na amostra que seria esperado observar se hipótese nula fosse verdadeira.\n\\[\nE = \\frac{total\\ coluna\\ \\times total\\ linha }{total\\ geral}\n\\]\nPor exemplo, suponha os dados da Tabela 19.1,\n\n\n\n\nTabela 19.1: Distribuição de Acidentes Automobilísticos por Gênero\n\n\n\nSexoCom AcidentesSem AcidentesTotal de PessoasHomens164460Mulheres43640Total2080100\n\n\n\n\n\no número de acidentes esperados para os homens\n\ntotal_c &lt;- df$Acidentes[3]\ntotal_l &lt;- df$Total[1]\ntotal_geral &lt;- df$Total[3]\nesperado &lt;- (total_c*total_l)/total_geral\nesperado \n\n[1] 12\n\n\né igual a 12, entretanto ocorreram 16. Houve uma diferença. Essa diferença é calculada para todas as células e é o importante no cálculo do qui-quadrado. Após o cálculo, acrescentando os valores esperados à Tabela 19.1, tem-se a Tabela 19.2 que é usada no cálculo do qui-quadrado.\n\n\n\n\nTabela 19.2: Distribuição de Acidentes Automobilísticos por Gênero\n\n\n\nSexoCom AcidentesSem AcidentesTotal de PessoasHomens124860Mulheres83240Total2080100\n\n\n\n\n\nPara o cálculo do \\(\\chi^2\\), como se está comparando proporções em uma tabela contingência \\(2\\times2\\) (ou seja, duas linhas e duas colunas), deve-se aplicar uma correção na fórmula, mostrada acima, para ajustar o viés de continuidade. Esta correção, denominada de correção de Yates, consiste em subtrair 0,5 do valor absoluto da diferença entre a frequência observada e a esperada em cada célula da tabela, antes de elevar ao quadrado e dividir pela frequência esperada.\n\\[\n\\chi^{2}= \\sum \\left [\\frac{\\left |(O_{i} - E_{i}|-0,5 \\right )^2}{E_{i}} \\right]\n\\]\nSubstituindo os dados na fórmula do qui-quadrado, tem-se:\n\\[\n\\chi^{2}= \\left [\\frac{\\left (|16 - 12 \\right |-0.5)^2}{12} \\right]+ \\left [\\frac{\\left (|44 - 48 \\right |-0.5)^2}{48} \\right]+\\left [\\frac{\\left (|4 - 8 \\right |-0.5)^2}{8} \\right]+\\left [\\frac{\\left (|36 - 32 \\right|-0.5)^2}{32} \\right]\n\\]\n\\[\n\\chi^{2}=1,021 + 0,255 + 1,531  +0,383 = 3,19\n\\]\nA correção de Yates reduz o valor do qui-quadrado (sem a correção o valor do qui-quadrado seria 4,17) e torna o teste mais conservador, reduzindo a probabilidade de rejeitar a hipótese nula quando ela é verdadeira.\n\n\n\n\n\n\nIndicação da correção de Yates\n\n\n\nA correção é recomendada em tabelas de contingência \\(2\\times2\\) ou quando o tamanho da amostra é pequeno (menor que 40) ou quando há pelo menos uma célula com frequência esperada menor que 5.\n\n\n\n19.2.2.1 Restrições ao qui-quadrado\n\nRegra Geral\n\nO teste pode ser usado, se a frequência observada em cada célula for maior ou igual a 5 e a frequência esperada for maior ou igual a 5.\n\nTabela 2 \\(\\times\\) 2 (gl = 1)\n\nComo mostrado no exemplo acima, neste caso, é recomendada a Correção de Continuidade de Yates, mesmo quando o n for grande.\n\nTabela l \\(\\times\\) c\n\nO teste pode ser usado se o número de células com frequência esperada inferior a 5 for menor do que 20% do total das células e nenhuma frequência esperada é igual a zero.\n\nn pequeno\n\nNeste caso, é preconizado o Teste Exato de Fisher.\n\n\n19.2.2.2 Valor crítico do qui-quadrado\nA estatística de teste (que em certo sentido é a diferença entre as frequências observadas e esperadas) deve ser comparada a um valor crítico para determinar se a diferença é grande ou pequena. Não se pode dizer se uma estatística de teste é grande ou pequena sem colocá-la em perspectiva com o valor crítico. Se a estatística de teste estiver acima do valor crítico, significa que a probabilidade de observar tal diferença entre as frequências observadas e esperadas é improvável.\nO valor crítico pode ser encontrado na tabela estatística da distribuição Qui-quadrado e depende do nível de significância, denotado \\(\\alpha\\), e dos graus de liberdade, denotado \\(gl\\). O nível de significância geralmente é igual a 5%. Os graus de liberdade para um teste de Qui-quadrado de independência são encontrados da seguinte forma:\n\\[\ngl = (numero \\ de \\ linhas - 1) \\ \\times \\ (numero \\ de \\ colunas - 1)\n\\]\nEm uma tabela de contingência 2 \\(\\times\\) 2, como a Tabela 19.1, tem \\(gl = (2 - 1) \\times (2 - 1) = 1\\). Basta agora obter o valor crítico com a função qchisq():\n\nalpha &lt;-  0.05\ngl = 1\nqchisq (1-alpha, gl)\n\n[1] 3.841459\n\n\nEsse valor é comparado com o \\(\\chi^{2}_{calculado}\\) para um nível de significância de 5%. Se ele é maior, rejeita-se se a \\(H_{0}\\); caso contrário, não se rejeita. Para obter o valor P, pode-se usar a função pchisq(), onde, como argumento, coloca-se o valor do \\(\\chi^{2}_{calculado}\\), os graus de liberdade e acrescenta-se lower.tail = FALSE para obter a probabilidade da cauda superior, uma vez que a distribuição do qui-quadrado é positiva.\nO valor crítico é igual a 3,84 e, no exemplo, o \\(\\chi^{2}_{calculado}\\) é igual a 3,19, logo a valor p é igual a:\n\npchisq (3.19, 1, lower.tail = FALSE)\n\n[1] 0.07409001\n\n\nDessa forma, conclui-se ,com uma confiança de 95%, que não se rejeita \\(H_{0}\\), ou seja, a proporção acidentes nos homens é igual a proporção de acidentes nas mulheres (\\(\\chi^{2} (1) = 3,19;P=0,074\\)). Observe na Figura 19.2 que o \\(\\chi^{2}_{calculado}\\) localiza-se a esquerda da linha vertical, da área vermelha de rejeição da \\(H_{0}\\). Atente, que se não fosse feita a correção de Yates, a conclusão seria diferente, pois o \\(\\chi^{2}_{calculado}\\) seria igual a 4,17 e estaria um pouco cima do \\(\\chi^{2}_{crítico}\\), na área de rejeição da \\(H_{0}\\). Com a correção de Yates o valor P é igual a 0,074, sem a correção de continuidade igual a 0,041, isto configura o que se chama de valor P marginal e, nesses casos, deve-se ter muito cuidado na conclusão, pois existe risco de erro tipo II, não rejeitar uma \\(H_{0}\\) quando ela é falsa.\n\nplotchisqtest(chisq = 3.84, \n              df = 1,\n              colorleft = \"aliceblue\",\n              colorright = \"red\",\n              ylabel = \"Densidade de probabilidade\")\n\n\n\n\n\n\n\nFigura 19.2: Distribuição do qui-quadrado, gl = 1, alpha = 0,05\n\n\n\n\n\nO gráfico foi criado com a função plotchisqteste() do pacote nhstplot, pacote simples e conveniente para representar graficamente os testes de significância de hipótese nula mais comuns, como testes F, testes t e testes z (Myszkowski 2020).\n\n\n\n19.2.3 Qui-quadrado de independência ou associação\n\n19.2.3.1 Dados usados no exemplo\nO exemplo da Tabela 19.1, usado para mostrar a lógica do qui-quadrado, é um teste de independência ou associação. Ali, foi mostrado que não existe uma associação estatisticamente significativa (p &gt; 0,05) entre acidentes automobilísticos e o sexo.\nComo exercício, serão usados outros dados que necessitam mais manipulação, como treinamento do qui-quadrado de associação e do próprio R. Estes dados servirão para testar a hipótese de associação entre tabagismo e baixo peso ao nascer (&lt; 2500g).\n\n19.2.3.1.1 Leitura e transformação dos dados\nOs dados serão provenientes do banco de dados dadosMater.xlsx. bastante usado neste livro 1 . A partir do diretório, será feita a leitura da seguinte maneira:\n\ndados &lt;- read_excel (\"dados/dadosMater.xlsx\")\n\nAdicione a este arquivo uma variável denominada baixoPeso, usando a função mutate() do pacote dplyr e a função ifelse (), da base do R:\n\ndados &lt;- dados %&gt;% mutate(baixoPeso = ifelse(pesoRN &lt; 2500, \"1\", \"2\"))\n\nOnde 1 = sim e 2 = não, ou seja, com peso de nascimento &lt; 2500g e \\(\\ge\\) 2500g.\nO próximo passo é selecionar, deste arquivo, apenas esta variável criada e a variável fumo, porque o objetivo da análise será verificar se existe associação entre tabagismo na gestação e baixo peso ao nascer (&lt; 2500g). Para isso, usa-se a função select() do pacote dplyr:\n\ndados &lt;- dados %&gt;% select (fumo, baixoPeso)\n\nA seguir, será extraída uma amostra deste banco de dados com n = 300, usando a função slice_sample() do pacote dplyr. A função set.seed() apenas garante que os dados selecionados aleatoriamente se mantenham os mesmos em outros sorteios (veja Seção 9.7.2):\n\nset.seed(123)\ndados &lt;- slice_sample(dados, n=300)\nstr(dados)\n\ntibble [300 × 2] (S3: tbl_df/tbl/data.frame)\n $ fumo     : num [1:300] 2 2 1 2 2 2 1 2 2 2 ...\n $ baixoPeso: chr [1:300] \"2\" \"2\" \"1\" \"2\" ...\n\n\nTodos esses passos poderiam ser feitos em um só comando, precedido pela semente (set.seed(123)), usando o operador pipe %&gt;%:\n\nset.seed(123)\ndados &lt;- read_excel (\"dados/dadosMater.xlsx\") %&gt;% \n  mutate(baixoPeso = ifelse(pesoRN &lt; 2500, \"1\", \"2\")) %&gt;% \n  select (fumo, baixoPeso) %&gt;% \n  slice_sample(n=300)\n\nTem-se, agora, um conjunto de dados com duas colunas: fumo, como uma variável numérica e baixoPeso, como caractere. Ambas devem ser transformadas em fator e, onde os rótulos são 1 e 2, passam para “sim” e “não” e mantendo a ordem “sim” e “não”.\n\ndados$fumo &lt;- factor(dados$fumo,\n                     levels = c(1, 2),\n                     labels = c(\"sim\", \"não\"))\n\ndados$baixoPeso &lt;- factor(dados$baixoPeso,\n                          levels = c(1, 2),\n                          labels = c(\"sim\", \"não\"))\nstr (dados)\n\ntibble [300 × 2] (S3: tbl_df/tbl/data.frame)\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 2 2 1 2 2 2 ...\n $ baixoPeso: Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 2 2 2 2 2 ...\n\n\n\n\n19.2.3.1.2 Construção da Tabela\nAntes de construir a Tabela 19.3, será verificado a distribuição de baixo peso de acordo com o tabagismo materno, usando a função with() e, dentro dela, a função table(), seguida da função addmargins():\n\ntab &lt;- with(data = dados, table(fumo, baixoPeso))\naddmargins(tab)\n\n     baixoPeso\nfumo  sim não Sum\n  sim  18  51  69\n  não  22 209 231\n  Sum  40 260 300\n\n\nAssim, tem-se\n\n\n\n\nTabela 19.3: Baixo peso* ao nascer e tabagismo\n\n\n\nTabagismoBaixo PesoSem Baixo PesoTotalSim185169Não22209260Total40260300* Baixo Peso = peso ao nascer &lt; 2500g\n\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nUma tabela é constituída por linhas e colunas. Para se extrair valores da tabela, usa-se os colchetes, após o nome da tabela. O primeiro valor dentro dos colchetes é referente ao número da linha; o segundo, separado pela virgula, é referente ao número da coluna. Então, tab[1,1] se refere ao valor que está na primeira linha e primeira coluna:\n\n\n\ntab[1,1]\n\n[1] 18\n\n\nA proporção de baixo peso por categoria de tabagismo (nº de casos/total da linha):\n\nfumantes &lt;-  tab[1,1]/(tab[1,1] + tab[1,2])\nfumantes\n\n[1] 0.2608696\n\nnão.fumantes &lt;- tab[2,1]/(tab[2,1]+ tab[2,2])\nnão.fumantes\n\n[1] 0.0952381\n\n\n\n\n19.2.3.1.3 Visualização gráfica\nUm gráfico de barras empilhadas, construído com o ggplot() do pacote ggplot2 (ver Seção 8.3), será usado para visualizar os dados:\n\n  ggplot(dados) +\n  aes (x = fumo, fill = baixoPeso) +\n  geom_bar (color = \"black\") +\n  scale_fill_manual(values = c(\"gray\", \"aliceblue\")) +\n  labs (title = NULL,  \n        x = \"Tabagismo\",\n        y = \"Frequência\") +\n  annotate(\"text\", x=\"sim\", y=62, label= \"26,1%\") + \n  annotate(\"text\", x = \"não\", y=223, label = \"9,5%\") +\n  theme_classic () +\n  theme (text = element_text (size = 12)) +\n  labs(fill = \"Peso ao nascer &lt; 2500g\")\n\n\n\n\n\n\n\nFigura 19.3: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer\n\n\n\n\n\nObserva-se, Figura 19.3, que 24,6% das gestantes fumantes geram bebês com baixo peso, enquanto que entre as não fumantes este percentual cai três vezes, indo para 9,5%. É uma diferença grande! Aqui, quase se tem certeza que ela é significativa, mesmo sem cálculos!\n\n\n\n19.2.3.2 Hipóteses estatísticas\n\n\\(H_{0}\\): a proporção de baixo peso é igual nos dois grupo (fumantes e não fumantes); não há associação entre as variáveis.\n\n\n\\(H_{1}\\): a proporção de baixo peso é diferente nos dois grupo (fumantes e não fumantes); existe associação entre as variáveis.\n\n\n\n19.2.3.3 Cálculo do Qui-quadrado de Pearson no R\nPara este exemplo, o \\(\\chi^{2}\\) irá verificar se existe uma associação entre as variáveis fumo e baixoPeso, assumindo um \\(\\alpha = 0,05\\) que equivale a um valor crítico de 3,84 com um grau de liberdade, em uma tabela \\(2 \\times 2\\). A função chisq_test() do pacote rstatix libera o qui-quadrado, usando os seguintes argumentos:\n\nx \\(\\to\\) vetor numérico ou matriz. Tanto x como y podem ser fatores;\ny \\(\\to\\) vetor numérico. Ignorado quando se x é uma matriz;\ncorrect \\(\\to\\) TRUE é o padrão. Indica se deve ser aplicada a correção de continuidade ao calcular a estatística de teste para tabelas 2 por 2 2;\n… \\(\\to\\) para outros argumentos consulte a ajuda do RStudio.\n\nPara executar a função chisq_test(), basta colocar como argumento as variáveis fumo e baixoPeso ou construir antes uma tabela de contingência com a função table() e depois colocá-la como argumento. Como tabela tab já existe:\n\nteste &lt;- rstatix::chisq_test(tab)\nteste\n\n# A tibble: 1 × 6\n      n statistic        p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   300      11.2 0.000809     1 Chi-square test ***     \n\n\nA saída do teste exibe tudo que é necessário. O \\(\\chi^{2}\\) = 11.2 com um valor muito maior que o valor crítico de 3,84, mostrando que a diferença é estatisticamente significativa (P = 8.09^{-4}). Essas e outras estatísticas podem ser obtidas, usando o objeto teste seguido do sinal $. Por exemplo, para o valor p:\n\nteste$p\n\n[1] 0.000809\n\n\n\n\n\n\n\n\nNota\n\n\n\nSe um aviso como Chi-squared approximation may be incorrect (Aproximação qui-quadrado pode estar incorreta) aparecer, significa que as menores frequências esperadas são inferiores a 5. Para evitar esse problema, é possível usar uma das seguintes opções:\n\nreunir alguns níveis (especialmente aqueles com um pequeno número de observações) para aumentar o número de observações nos subgrupos, ou\nusar o teste exato de Fisher.\n\n\n\n\n19.2.3.3.1 Outras maneiras de calcular o qui-quadrado no R\nNo pacote gmodels (Warnes, Bolker, et al. 2022), existe uma função muito interessante, a função CrossTable() que que imprime, além de uma tabela de frequência com as proporções, exibe vários testes, como o teste \\(\\chi^2\\), o teste exato de Fisher e o teste de McNemar com e sem correção de continuidade. Consulte a ajuda para melhor estudar esta elegante função! Neste momento, será explorado apenas o qui-quadrado e os valores esperados com três dígitos:\n\nCrossTable (tab,\n            digits = 3,\n            prop.chisq = FALSE,\n            prop.t = FALSE,\n            chisq = TRUE,\n            expected = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n|           N / Row Total |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  300 \n\n \n             | baixoPeso \n        fumo |       sim |       não | Row Total | \n-------------|-----------|-----------|-----------|\n         sim |        18 |        51 |        69 | \n             |     9.200 |    59.800 |           | \n             |     0.261 |     0.739 |     0.230 | \n             |     0.450 |     0.196 |           | \n-------------|-----------|-----------|-----------|\n         não |        22 |       209 |       231 | \n             |    30.800 |   200.200 |           | \n             |     0.095 |     0.905 |     0.770 | \n             |     0.550 |     0.804 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        40 |       260 |       300 | \n             |     0.133 |     0.867 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  12.61347     d.f. =  1     p =  0.0003829762 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  11.22084     d.f. =  1     p =  0.0008088369 \n\n \n\n\nObserve que a saída mostra em cada célula da tabela, o número de casos, o número esperado, a percentagem por linha (nº de casos/total da linha) e a percentagem por coluna (nº de casos/total da coluna). Por último, exibe o qui-quadrado de Pearson com e sem coreção de continuidade de Yates.\n\n\n\n19.2.3.4 Conclusão e relato dos resultados\nUsando a correção de continuidade de Yates, pois é uma tabela \\(2\\times2\\), vê-se que o valor p é menor que o nível de significância de 5% e, consequentemente, rejeita-se a hipótese nula e conclui-se que existe uma associação significativa entre tabagismo na gestação e o baixo peso ao nascimento (\\(\\chi^{2}_{com \\ correção \\ de \\ Yates} (1)\\) = 11.2; P = 0,00081).\nAlém disso, no relato dos resultados pode-se apresentar uma tabela ou em um gráfico.\n\n19.2.3.4.1 Tabela\nPara a construção da tabela, pode-se usar a função ctable() do pacote summarytools(Comtois 2022) para obter uma tabela com todos os dados a serem exibidos. O argumento prop = \"r\" exibe os percentuais das linhas (“c”, nas colunas). Na realidade, são maneiras diferente de se obter o mesmo resultado.\n\nctable(dados$fumo, dados$baixoPeso,\n       prop = \"r\", \n       chisq = TRUE, \n       headings = FALSE,\n       OR = TRUE)\n\nError in if (tmp_name %in% colnames(obj)) {: the condition has length &gt; 1\n\n\nError in if (tmp_name %in% colnames(obj)) {: the condition has length &gt; 1\n\n\n\n------------ ----------------- ------------ ------------- --------------\n               dados$baixoPeso          sim           não          Total\n  dados$fumo                                                            \n         sim                     18 (26.1%)    51 (73.9%)    69 (100.0%)\n         não                     22 ( 9.5%)   209 (90.5%)   231 (100.0%)\n       Total                     40 (13.3%)   260 (86.7%)   300 (100.0%)\n------------ ----------------- ------------ ------------- --------------\n\n----------------------------\n Chi.squared   df   p.value \n------------- ---- ---------\n   11.2208     1     8e-04  \n----------------------------\n\n----------------------------------\n Odds Ratio   Lo - 95%   Hi - 95% \n------------ ---------- ----------\n    3.35        1.67       6.71   \n----------------------------------\n\n\nOu seja, os bebês que se expuseram ao fator de risco (fumo) têm uma chance 3,35 (IC95% : 1,67-6,71) vezes maior de apresentar peso &lt; 2500g ao nascer, nessa amostra.\nAlém desses resultados, a tabela final pode conter (e isto é recomendado!) os intervalos de confiança para cada uma das proporções de fumantes e não fumantes. Para isso a função BinomCI() cumpre um papel suficiente (veja Seção 12.7.3).\nEntão, a proporção de baixo peso entre as fumantes é:\n\nBinomCI(18, 69,\n        conf.level = 0.95,\n        method = \"clopper-pearson\")\n\n           est    lwr.ci    upr.ci\n[1,] 0.2608696 0.1625161 0.3805962\n\n\nE, entre as não fumantes é:\n\nBinomCI(22, 231,\n        conf.level = 0.95,\n        method = \"clopper-pearson\")\n\n           est     lwr.ci    upr.ci\n[1,] 0.0952381 0.06065157 0.1406387\n\n\nFinalmente, esses dados podem ser colocados em uma tabela, como a Tabela 19.4:\n\n\n\n\nTabela 19.4: Efeito do tabagismo materno no peso ao nascer\n\n\n\nPeso ao nascerFumantesNão fumantesValor P*Baixo Peso18/6922/2310.00081IC95%16,3-38,16,1-14,1* Qui-quadrado de Pearson com correção\n\n\n\n\n\n\n\n19.2.3.4.2 Gráfico\nUma boa apresentação gráfica complementa o relatório dos resultados. Pode-se fazer isso com gráfico de barras empilhadas (), acompanhado dos percentuais e do tipo de teste realizado, usando a função get_test_label() que necessita do teste calculado com a função chisq_test() do pacote rstatix, apresentado antes. O gráfico assume o aspecto da Figura 19.4):\n\nggplot(dados) +\n    aes (x = fumo, fill = baixoPeso) +\n    geom_bar (color = \"black\") +\n    scale_fill_manual(values = c(\"gray\", \"gray95\")) +\n    labs (title = NULL,\n          subtitle = get_test_label (teste, detailed = TRUE),\n          x = \"Tabagismo\",\n          y = \"Frequência\") +\n    annotate(\"text\", x=\"sim\", y=62, label= \"24,6% (15,0-36,5)\") + \n    annotate(\"text\", x = \"não\", y=223, label = \"8,2% (5,0-12,5)\") +\n    theme_classic () +\n    theme (text = element_text (size = 12)) +\n    labs(fill = \"Peso ao nascer &lt; 2500g\")\n\n\n\n\n\n\n\nFigura 19.4: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer\n\n\n\n\n\n\n\n\n\n19.2.4 Teste Aderência ou do Melhor Ajuste\nO teste de qualidade de ajuste do qui-quadrado (chi-square goodness of fit) é usado para comparar a distribuição observada com uma distribuição esperada, em uma situação em que se tem duas ou mais categorias em dados discretos. Em outras palavras, ele compara várias proporções observadas com as probabilidades esperadas (Daniel e Cross 2013).\n\n19.2.4.1 Dados usados no exemplo\n\n\n\n\n\n\nCenário\n\n\n\nHá uma dúvida se o número de pacientes que procura uma determinada Unidade de Pronto Atendimento (UPA) é aproximadamente o mesmo em todos os dias da semana. Esta é uma informação importante sob o ponto de vista administrativo. Para se atingir este objetivo registrou-se o número de pacientes que procurou a UPA por dia da semana.\n\n\nO número de atendimentos nos sete dias (de segunda-feira à domingo) da semana está representada pela frequência observada, freq_obs:\n\nfreq_obs &lt;- c(20, 17, 22, 21, 26, 33, 36)\nfreq_obs\n\n[1] 20 17 22 21 26 33 36\n\n\nO total de atendimentos durante uma semana é igual a:\n\nsoma &lt;- sum(freq_obs)\nsoma\n\n[1] 175\n\n\nAssim, a frequência esperada diária é igual a soma total dos atendimentos dividido pelo número observações (no caso, dias da semana), representada por k:\n\nk = 7\nfreq_esp &lt;- soma/k\nfreq_esp\n\n[1] 25\n\n\nCom esses valores , pode-se criar um vetor, p, com as proporções dos atendimentos diários esperados:\n\np &lt;- rep(freq_esp/soma, 7)\np\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n19.2.4.2 Hipóteses estatísticas\n\n\\(H_{0}\\): a distribuição das frequências observadas (O) é igual a distribuição de frequências esperadas (E).\n\n\n\\(H_{1}\\): a distribuição das frequências observadas (O) não é igual a distribuição de frequências esperadas (E)\n\n\n\n19.2.4.3 Cálculo da estatística do teste\nAssumindo um \\(\\alpha = 0,05\\). Os graus de liberdade são calculados como o número de células (k) menos 1: \\(gl = (k - 1)\\). O \\(\\chi^{2}_{crítico}\\) pode ser encontrado usando:\n\nalpha = 0.05\nk = 7\ngl = k - 1\nqchisq (1 - alpha, gl)\n\n[1] 12.59159\n\n\nEm outras palavras, se o \\(\\chi^{2}_{calculado}\\) &gt; \\(\\chi^{2}_{crítico}\\), rejeita-se a \\(H_{0}\\). Na Figura 19.5), o resultado tem que ficar à direita da linha vertical vermelha para que a hipótese nula seja rejeitada. Se cair fora da área de rejeição, à esquerda da linha vertical vermelha, aceita-se a hipóteses nula.\n\nplotchisqtest(chisq = 12.6, \n              df = 6,\n              colorleft = \"aliceblue\",\n              colorright = \"red\",\n              ylabel = \"Densidade de probabilidade\",\n              colorcut = \"red\")\n\n\n\n\n\n\n\nFigura 19.5: Distribuição do qui-quadrado, gl = 6, alpha = 0,05\n\n\n\n\n\nA estatística do teste pode ser encontrada, usando a função chisq.test():\n\nteste1 &lt;- rstatix::chisq_test (x = freq_obs, y = NULL, p = p)\nteste1\n\n# A tibble: 1 × 6\n      n statistic     p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   \n1     7        12 0.062     6 Chi-square test ns      \n\n\n\n\n19.2.4.4 Conclusão\nObservando-se a saída do teste do qui-quadrado, verifica-se que o \\(\\chi^{2}_{calculado}\\) &lt; \\(\\chi^{2}_{crítico}\\), portanto, não se rejeita a \\(H_{0}\\) e conclui-se que, nesta amostra, com uma confiança de 95%, que a frequência observada de pacientes à UPA é igual a esperada (P = 0.062). Lembrando que, neste caso, como se tem um valor P limitrofe, marginal, existe a possibilidade de se estar aceitando uma \\(H_{0}\\) falsa e cometendo um erro tipo II. Seria recomendado, aumentar o tamanho amostral em uma nova coleta, usando estes dados como um piloto para o cálculo amostral.\n\n\n\n19.2.5 Qui-quadrado de Pearson para tabelas extensas\nEste teste é utilizado quando o número de grupos, k, é superior a 2.\n\n19.2.5.1 Dados usados como exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEm um hospital pediátrico, 144 recém-nascidos foram submetidos a diferentes procedimentos cirúrgicos (abdominal, cardíaco ou outro). Os cirurgiões estavam preocupados em saber se há uma tendência de maior taxa de indecção entreos neonatos que permanecem mais tempo hospitalizados.\n\n\nO banco de dados dadosCirurgia.xlsx que pode ser encontrado aqui. Salve o mesmo no seu diretório de trabalho. As variáveis disponíveis no banco de dados são:\n\nid \\(\\to\\) identificação do neonato;\nsexo \\(\\to\\) sexo do recém-nascido, fem e masc;\npeso \\(\\to\\) peso do neonato em gramas;\ntempohosp \\(\\to\\) tempo de hospitalização em dias;\ninfec \\(\\to\\) presença de infecção secundária: sim e não;\ncirurgia \\(\\to\\) tipo de cirurgia: abdominal, cardíaca, outra.\n\nA variável tempo de hospitalização (tempohosp) é contínua e assimétrica. Para ser usada aqui, será categorizada por quartis. A variável infec (presença de infecção) é uma variável dicotômica (sim, não).\n\n19.2.5.1.1 Leitura e transformação dos dados\nA leitura dos dados pode ser feita com:\n\ncirurgia &lt;- read_excel (\"dados/dadosCirurgia.xlsx\")\nstr(cirurgia)\n\ntibble [144 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:144] 1 2 3 4 5 6 7 8 9 10 ...\n $ sexo     : chr [1:144] \"masc\" \"masc\" \"masc\" \"masc\" ...\n $ peso     : num [1:144] 2020 1850 2540 1150 2900 ...\n $ ig       : num [1:144] 36 30 38 31 36 37 38 39 38 39 ...\n $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ...\n $ infec    : chr [1:144] \"não\" \"não\" \"sim\" \"sim\" ...\n $ cirurgia : chr [1:144] \"abdominal\" \"abdominal\" \"abdominal\" \"outra\" ...\n\n\nOs dados foram atribuídos a um objeto denominado de cirurgia. Característicamente, a variável tempohosp (tempo de hospitalização) é assimétrica (Figura 19.6):\n\ncirurgia %&gt;% ggplot() +\n    geom_histogram(aes(x = tempohosp,\n                       y = after_stat(density)), \n                   fill = \"tomato\",\n                   bins = 20,\n                   col=alpha(\"gray40\",0.5)) +\n  geom_function(fun=dnorm,\n                args=list(mean=mean(cirurgia$tempohosp, na.rm = T),\n                          sd= sd(cirurgia$tempohosp, na.rm = T)), \n                col='dodgerblue4',\n                lwd=1,\n                lty=2) + \n  labs(x='Tempo de hospitalização (dias)',    \n       y='Densidade de probabilidade') +\n  theme_bw()\n\n\n\n\n\n\n\nFigura 19.6\n\n\n\n\n\nA variável tempohosp será categorizada em quartis. O resumo da mesma, usando a função summary(), fornece orientação para esse procedimento com o retorno do 1º quartil, mediana e 3º quartil:\n\nsummary (cirurgia$tempohosp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   20.75   27.50   37.42   42.00  245.00 \n\n\nA categorização da variável tempohosp, que será realizada, usando a função cut(), consulte a Seção 7.2.3.3 para detalhes de construção de uma tabela de frequência:\n\ncirurgia$tempo &lt;- cut(cirurgia$tempohosp, \n                      breaks = c(1, 20.75, 27.50, 42.00, 245),\n                      labels = c(\"&lt;= 21\",\"22-28\", \"29-42\",\"&gt;42\"),\n                      right = FALSE, \n                      include.lowest = TRUE)\n\ntab1 &lt;- table (cirurgia$tempo)\ntab1\n\n\n&lt;= 21 22-28 29-42   &gt;42 \n   36    36    35    37 \n\n\nPor exemplo, 36 recém-nascidos premaneceram 21 dias ou menos no pós-operatório e 37 recém-nascidos ficarm internados mais do que 42 dias.\nAgora, a variável cirurgia$infec será colocada como um fator:\n\ncirurgia$infec &lt;- factor(cirurgia$infec, levels = c(\"sim\", \"não\"))\ntable(cirurgia$infec)\n\n\nsim não \n 56  88 \n\n\nA nova variável tempo será cruzada com a variável infec em uma tabela:\n\ntab2 &lt;- with(data = cirurgia, table(tempo, infec))\naddmargins(tab2)\n\n       infec\ntempo   sim não Sum\n  &lt;= 21   9  27  36\n  22-28  11  25  36\n  29-42  14  21  35\n  &gt;42    22  15  37\n  Sum    56  88 144\n\n\nOu seja, a proporção de recém-nascidos que se infectaram no pós-operatório foi 56/144 = 0,39 ou 39%. Se for calculada a proporção para cada um dos quartis do tempo de hospitalização, através da função ctable() do pacote summarytools, observa-se que a proporção de neonatos infectados no pós-operatório aumenta com o tempo de hospitalização:\n\n\n\n19.2.5.2 Hipóteses estatísticas\n\n\\(H_{0}\\): A presença de infecção não altera o tempo de hospitalização.\n\n\n\\(H_{1}\\): A presença de infecção altera o tempo de hospitalização.\n\n\n\n19.2.5.3 Cálculo da estatística do teste\nO teste estatístico pode ser calculado, usando o argumento chisq = TRUE na função ctable():\n\nsummarytools::ctable(cirurgia$tempo, cirurgia$infec,\n                      prop = \"r\",\n                      chisq = TRUE,\n                      headings = FALSE)\n\n\n------- ------- ------------ ------------ --------------\n          infec          sim          não          Total\n  tempo                                                 \n  &lt;= 21            9 (25.0%)   27 (75.0%)    36 (100.0%)\n  22-28           11 (30.6%)   25 (69.4%)    36 (100.0%)\n  29-42           14 (40.0%)   21 (60.0%)    35 (100.0%)\n    &gt;42           22 (59.5%)   15 (40.5%)    37 (100.0%)\n  Total           56 (38.9%)   88 (61.1%)   144 (100.0%)\n------- ------- ------------ ------------ --------------\n\n----------------------------\n Chi.squared   df   p.value \n------------- ---- ---------\n   10.5801     3    0.0142  \n----------------------------\n\n\n\n\n19.2.5.4 Conclusão\nA partir desses resultados, pode-se inferir que a menor taxa de infecção ocorre no grupo do primeiro quartil e é significativamente diferente em relação a taxa de infecção do maior quartil, mas sem indicação para os grupos intermediários. É útil fazer o teste de tendência linear (Linear-by-linear Association). Para isso, pode-se usar a função lbl_test () do pacote coin.\n\ncoin::lbl_test (cirurgia$tempo ~ cirurgia$infec)\n\n\n    Asymptotic Linear-by-Linear Association Test\n\ndata:  cirurgia$tempo (ordered) by cirurgia$infec (sim, não)\nZ = 3.1231, p-value = 0.001789\nalternative hypothesis: two.sided\n\n\nEste teste indica uma tendência significativa para a presença de infecção à medida que aumenta o tempo de hospitalização (P = 0,0018).",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "19-dadoscategoricos.html#teste-exato-de-fisher",
    "href": "19-dadoscategoricos.html#teste-exato-de-fisher",
    "title": "19  Análise de Dados Categóricos",
    "section": "19.3 Teste exato de Fisher",
    "text": "19.3 Teste exato de Fisher\nO teste do qui-quadrado não é um método apropriado de análise se a amostra é pequena. Por exemplo, se n for menor que 20 ou se n estiver entre 20 e 40 e uma das frequências esperadas for menor que 5, o teste do qui-quadrado deve ser evitado. Nesta situação, é recomendado o teste exato de Fisher.\n\n19.3.1 Dados usados como exemplo\n\n\n\n\n\n\nCenário\n\n\n\nUm estudo estabeleceu como objetivo verificar se a asma não controlada é um fator de risco para a procura da emergência. Foram acompanhados 16 escolares asmáticos durante um ano com relação ao número de visitas à emergência de acordo com o controle da sua asma.\n\n\n\n19.3.1.1 Entrando com os dados\nEm primeiro lugar, serão criadas duas variáveis:\n\nemerg &lt;- c (1,1,2,2,2,2,2,1,1,1,1,1,1,1,1,2)\ncontrole &lt;- c (1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2)\n\nA seguir, usando essas variáveis, será construído um dataframe que será atribuído ao objeto dadosAsma:\n\ndadosAsma &lt;- data.frame(emerg, controle)\n\n\n\n19.3.1.2 Transformação dos dados\nAmbas as variáveis são numéricas e serão transformadas em fatores, considerando para a variável emerg, 1 = “consultou” e 2 = “não consultou”, e, para a variável controle, 1 = “asma controlada” e 2 = “asma não controlada”. É importante mater a ordem:\n\ndadosAsma$emerg &lt;- factor (dadosAsma$emerg,\n                           levels = c (1,2),\n                           labels = c ('consultou', 'não consultou'))\ndadosAsma$controle &lt;- factor (dadosAsma$controle, \n                          levels = c (1,2),\n                          labels = c ('asma controlada', 'não controlada'))\nstr(dadosAsma)\n\n'data.frame':   16 obs. of  2 variables:\n $ emerg   : Factor w/ 2 levels \"consultou\",\"não consultou\": 1 1 2 2 2 2 2 1 1 1 ...\n $ controle: Factor w/ 2 levels \"asma controlada\",..: 1 1 1 1 1 1 1 2 2 2 ...\n\n\n\n\n19.3.1.3 Construção da tabela\nPara o cálculo da estatística do teste, é necessário uma tabela \\(2\\times2\\) (Tabela 19.5), obtida com os dados acima:\n\ntab3 &lt;- with(data = dadosAsma, table(controle, emerg))\n\n\n\n\n\nTabela 19.5: Consulta à emergência e controle da asma (tab3)\n\n\n\nVisita à EmergênciaControleSimNãoTotalAsma controlada257Não controlada819Total10616\n\n\n\n\n\n\n\n\n19.3.2 Hipóteses estatísticas\n\n\\(H_0\\): as variáveis são independentes, não há relação entre as duas variáveis categóricas.\n\n\n\\(H_1\\): as variáveis são dependentes, existe uma relação entre as duas variáveis categóricas. \n\n\n\n19.3.3 Cálculo da estatística do teste\nO teste exato de Fisher é usado quando há pelo menos uma célula na tabela de contingência das frequências esperadas abaixo de 5. Para recuperar as frequências esperadas, use a função chisq.test(), do R base, junto com $expected:\n\nteste2 &lt;- chisq.test(tab3)\n\nWarning in chisq.test(tab3): Aproximação do qui-quadrado pode estar incorreta\n\n\nA saída do teste imprime um aviso de que o qui-quadrado pode estar incorreto. Há necessidade, devido a presença de três células com valores abaixo de 5, de se usar o teste de Fisher. este pode ser obtido através da função fisher_test(), do pacote rstatix, colocando uma tabela \\(2\\times2\\) como argumento, tab3:\n\nrstatix::fisher_test (tab3, detailed = TRUE)\n\n# A tibble: 1 × 8\n      n estimate     p conf.low conf.high method            alternative p.signif\n* &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;   \n1    16   0.0646 0.035 0.000953     0.991 Fisher's Exact t… two.sided   *       \n\n\n\n\n19.3.4 Conclusão\nO valor \\(P=0,035\\) é menor que o nível de significância de 5%, previamente estabelecido, e, portanto, deve-se rejeitar a hipótese nula. No contexto, rejeitar a hipótese nula para o teste exato de independência de Fisher significa que há uma associação significativa entre as duas variáveis categóricas (controle da asma e visitas à emergência).",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "19-dadoscategoricos.html#teste-de-mcnemar",
    "href": "19-dadoscategoricos.html#teste-de-mcnemar",
    "title": "19  Análise de Dados Categóricos",
    "section": "19.4 Teste de McNemar",
    "text": "19.4 Teste de McNemar\nÉ um teste estatístico não paramétrico aplicável nos estudos tipo “antes-e-depois” em que cada indivíduo é utilizado como seu próprio controle e a medida é efetuada em escala nominal. O teste de McNemar é usado para determinar se há uma diferença estatisticamente significativa nas proporções entre os dados emparelhados.\nAs medidas coletadas nesses tipos de projetos de estudo não são independentes e, portanto, os testes do Qui-quadrado não podem ser usados porque os pressupostos serão violados.\nO teste de McNemar é usado para avaliar se há uma mudança significativa nas proporções ao longo do tempo para dados emparelhados ou se há uma diferença significativa nas proporções entre casos e controles. O resultado de interesse é a mudança dentro da pessoa (ou diferenças dentro do par) e não há variáveis explicativas.\nO teste é calculado examinando o número de respostas que são concordantes para positivo (sim em ambas as ocasiões) e negativo (não em ambas as ocasiões) e o número de pares disconcordantes (sim e não, ou não e sim). Os pares concordantes não fornecem informações sobre as diferenças e não são usados na avaliação. Em vez disso, deve-se concentrar nos pares discordantes, que podem ser divididos em dois tipos: um par discordante do tipo sim – não e um par discordante tipo não – sim (Eliasziw e Donner 1991).\n\n19.4.1 Pressupostos do teste de McNemar\nOs pressupostos para o teste de McNemar são:\n\nA variável desfecho é binária, dicotômica;\nCada participante é representado na tabela apenas uma vez;\nA diferença entre as proporções emparelhadas é o resultado de interesse;\nO teste de McNemar pode não ser confiável se houver contagens baixas nas células “discordantes”. Existe recomendação de que a soma dessas células seja \\(\\ge 20\\) (Rosner 2011).\n\n\n\n19.4.2 Dados usados como exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEm uma universidade, um professor de bioestatística comparou as atitudes de 200 estudantes de Medicina em relação à confiança que eles depositam na análise estatística antes e depois da conclusão da disciplina. A pergunta feita foi: Confiam na análise estatística utilizada nos periódicos médicos?\n\n\nAs respostas obtidas estão na Tabela 19.6:\n\n\n\n\nTabela 19.6: Confiança na análise estatística após término da disciplina\n\n\n\nPós-testePré-TesteSimNãoTotalSim20 (a)8 (b)28Não22 (c)150 (d)172Total42158200\n\n\n\n\n\n\n\n19.4.3 Hipóteses estatísticas\nConsiderando as caselas a, b, c e d da Tabela 19.6, a hipótese nula de homogeneidade marginal indica que as duas probabilidades marginais para cada resultado são as mesmas, isto é,\n\\[\np_{a} + p_{b} = p_{a} + p_{c}\n\\] e\n\\[\np_{c} + p_{d} = p_{b} + p_{d}\n\\]\nAssim, a hipótese nula e a hipótese alternativa são:\n\n\\(H_{0}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste é a mesma.\n\n\n\\(H_{1}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste não é a mesma. \n\n\n\n19.4.4 Lógica do teste\nO teste estatístico de McNemar, com correção de continuidade, é obtido utilizando a equação:\n\\[\n\\chi^{2} = \\frac {\\left (\\left |b - c  \\right |- 1  \\right )^{2}}{b + c}\n\\]\nSob a hipótese nula, com um número suficientemente grande de discordantes (células b e c), o \\(\\chi^{2}\\) tem uma distribuição qui-quadrado com um grau de liberdade. Se o resultado é significativo, isto é, fornece evidências suficientes para rejeitar a hipótese nula, significa que as proporções marginais são significativamente diferentes umas das outras.\nSubstituindo os dados da Tabela na Equação, tem-se:\n\na &lt;- 20\nb &lt;- 8\nc &lt;- 22\nd &lt;- 150\nchi &lt;- ((abs(b - c) - 1)^2)/(b + c)\nchi\n\n[1] 5.633333\n\n\nAssumindo um \\(\\alpha = 0,05\\), pode-se obter valor crítico para o \\(\\chi^{2}\\) para gl = 1, usando a função qchisq(), do pacote stats:\n\nalpha = 0.05\nqchisq(1 - alpha, 1)\n\n[1] 3.841459\n\n\nDesta maneira, rejeita-se a \\(H_{0}\\), pois o \\(\\chi_{calculado}^{2} &gt; \\chi_{crítico}^{2}\\). O valor P pode ser conseguido com a função pchisq():\n\npchisq (5.633, 1, lower.tail = FALSE)\n\n[1] 0.01762544\n\n\n\n\n19.4.5 Cálculo do teste de McNemar\nCarregar o arquivo dadosBioestatistica.xlsx, que pode ser encontrado aqui. Este conjunto de dados contem os dados da Tabela 19.6.\n\ndados &lt;- readxl::read_excel(\"dados/dadosBioestatistica.xlsx\")\n\nConstruir uma tabela de contingência\n\ndados$preteste &lt;- factor(dados$preteste, levels = c(\"sim\", \"não\"))\ndados$posteste &lt;- factor(dados$posteste, levels = c(\"sim\", \"não\"))\ntab4 &lt;- table(dados$preteste, dados$posteste, \n            dnn = c(\"Pré-teste\", \"Pós-teste\"))\ntab4\n\n         Pós-teste\nPré-teste sim não\n      sim  20   8\n      não  22 150\n\n\nAgora, pode-se calcular o teste de McNemar3 com a função mcnemar_test() do pacote rstatix:\n\nrstatix::mcnemar_test (tab4, \n                       correct = TRUE)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1   200      5.63     1 0.0176 *        McNemar test\n\n\nO resultado do teste de McNemar com correção de continuidade é exatamente igual ao calculado manualmente.\n\n\n19.4.6 Conclusão\nHouve uma modificação estatisticamente significativa na opinião dos alunos após o curso de Bioestatística em relação à confiança nas análises estatísticas (86% no pré-teste de respostas não x 79% no pós-teste, \\(\\chi^{2} = 5,63, gl = 1, P = 0,018\\)). Alguns alunos (14) mudaram de opinião em relação a sua confiança nas análises estatísticas dos periódicos médicos.\n\n\n\n\n\n\nAltman, Douglas G. 1991. «Comparing groups: categorical data». Em Practical Statistics for Medical Research, 244–47. London: Chapman & Hall/CRC.\n\n\nComtois, Dominic. 2022. «summarytools: Tools to Quickly and Neatly Summarize Data». CRAN R Project. https://github.com/dcomtois/summarytools.\n\n\nDaniel, Wayne W., e Chad L. Cross. 2013. «The chi-square distribution and analysis of frequencies». Em Practical Statistics for Medical Research, 604–19. Hoboken, NJ: John Wiley & Sons, Inc.\n\n\nEliasziw, Michael, e Allan Donner. 1991. «Application of the McNemar test to non-independent matched pair data». Statistics in medicine 10 (12). Wiley Online Library: 1981–91.\n\n\nMyszkowski, Nils. 2020. «nhstplot package». RDocumentation. https://rdocumentation.org/packages/nhstplot/versions/1.1.0.\n\n\nRosner, Bernard. 2011. «Hypothesis Testing: Categorical Data». Em Fundamentals of Biostatistics, Seventh Edition, 377. Boston: Cengage.\n\n\nWarnes, Gregory R., Ben Bolker, et al. 2022. «Gmodels: Various R programming tools for model fitting». CRAN R Project. https://rdrr.io/cran/gmodels/.",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "19-dadoscategoricos.html#footnotes",
    "href": "19-dadoscategoricos.html#footnotes",
    "title": "19  Análise de Dados Categóricos",
    "section": "",
    "text": "Ver Seção 5.6.↩︎\nQuando não se está trabalhando com uma tabela \\(2 \\times 2\\) e a regra geral for obedecida e o n for grande, pode-se usar o qui-quadrado de Pearson sem correção.↩︎\nÉ possível também obter o teste de McNemar de outras formas, como, por exemplo, usando a função CrossTable() do pacote gmodels.↩︎",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html",
    "href": "20-testes-naoParametricos.html",
    "title": "20  Métodos não paramétricos",
    "section": "",
    "text": "20.1 Pacotes necessários neste capítulo\npacman::p_load (coin,\n                confintr,\n                flextable,\n                ggpubr,\n                ggsci,\n                kableExtra,\n                knitr,\n                readxl,\n                rstatix,\n                tidyverse)",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#sec-distlivre",
    "href": "20-testes-naoParametricos.html#sec-distlivre",
    "title": "20  Métodos não paramétricos",
    "section": "20.2 Distribuição livre",
    "text": "20.2 Distribuição livre\nA maioria dos testes estatísticos, discutidos neste livro, são testes paramétricos. Nestes, o interesse estava focado em estimar ou testar uma hipótese sobre um ou mais parâmetros populacionais e ,por isso, são denominados de paramétricos. Além disso, o aspecto central desses procedimentos era o conhecimento da forma funcional da população da qual foram retiradas as amostras que forneceram a base para a inferência. Por exemplo, o teste t de Student para amostras independentes e a ANOVA são baseados no pressuposto de que os dados foram amostrados de populações que têm distribuição normal.\nOs testes não paramétricos não fazem suposições em relação à distribuição da população. Não têm, portanto, os pressupostos restritivos, comuns nos testes paramétricos. Têm distribuição livre. São baseados em uma ideia simples de ordenação por postos, do valor mais baixo ao mais alto. Analisam somente os postos, ignorando os valores. Podem ser usados tanto com variáveis ordinais como quantitativas numéricas.",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#sec-postos",
    "href": "20-testes-naoParametricos.html#sec-postos",
    "title": "20  Métodos não paramétricos",
    "section": "20.3 Postos",
    "text": "20.3 Postos\nOs métodos estatísticos não paramétricos não lidam diretamente com os valores observados. Em função disso, para poder usar a informação fornecida pelas observações, sem trabalhar diretamente com os valores observados, utiliza-se os postos das observações. Posto (rank) de uma observação é a sua posição em relação aos demais valores.\nA atribuição dos postos de uma variável é realizada da seguinte maneira:\n\nColocam-se as observações em ordem crescente;\nAssociam-se valores, correspondendo às suas posições relativas na amostra. O primeiro elemento recebe o valor 1, o segundo o valor 2 e, assim por diante, até que a maior observação receba o valor n;\nSe todas as observações são distintas, os postos são iguais aos valores associados às observações no passo anterior.\nPara observações iguais (empates), associam-se postos iguais à média das suas posições relativas na amostra.\n\nPor exemplo, suponha uma amostra contendo os escores de Apgar no primeiro minuto de 10 recém-nascidos a termo (Tabela 20.1)). Em primeiro lugar, os valores são colocados em ordem crescente e, após, atribui-se postos aos valores. Observe que os postos atribuídos aos valores das posições 4, 5 e 6 são iguais e correspondentes a média de 4, 5 e 6, que é igual a 5. O mesmo ocorreu com os outros valores onde houve empate. A soma dos postos, no exemplo, é igual a 55. Para verificar a correção do cálculo, haja ou não empates, a soma dos postos será sempre \\(\\frac {n\\ \\times \\ (n+1)}{2}\\). No exemplo, n = 10, logo \\(\\frac {10\\ \\times \\ (10+1)}{2}=55\\).\n\n\n\n\nTabela 20.1: Construção dos postos\n\n\n\napgar1ordemposto411.0522.0733.0845.0855.0865.0978.0988.01099.510109.5",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#teste-de-mann-whitney",
    "href": "20-testes-naoParametricos.html#teste-de-mann-whitney",
    "title": "20  Métodos não paramétricos",
    "section": "20.4 Teste de Mann-Whitney",
    "text": "20.4 Teste de Mann-Whitney\nO teste de Mann-Whitney é usado para analisar a diferença na variável dependente (desfecho) para dois grupos independentes. O teste classifica todos os valores dependentes, ou seja, o valor mais baixo obtém o posto um e, em seguida, usa a soma dos postos de cada grupo no cálculo da estatística de teste.\nÉ o substituto do teste t para amostras independentes quando os pressupostos deste teste são violados. Para a aplicação do teste de Mann-Whitney a variável de interesse deve ser ordinal ou numérica. Este teste é equivalente ao desenvolvido por Frank Wilcoxon (1892 – 1965), assim algumas vezes é denominado de Wilcoxon Rank Sum Test ou teste de Wilcoxon-Mann-Whitney. O R usa esta denominação e é importante não confundir com o teste não paramétrico para amostra pareadas, discutido mais adiante.\n\n20.4.1 Dados usados no exemplo\nOs dados do exemplo são do arquivo dadosCirurgia.xlsx, já usado na Seção Seção 19.2.5.1. Ele contém 144 recém-nascidos que foram submetidos a diferentes procedimentos cirúrgicos. A questão de pesquisa a ser respondida é:\n\nExiste diferença no tempo de hospitalização (tempohosp) dos recém-nascidos de acordo com a presença ou não de infecção (infec)?\n\nEssa pergunta foi respondida de outra maneira, na Seção Seção 19.2.5. Agora, será respondida pelo teste de Mann-Whitney.\n\n20.4.1.1 Leitura, exploração e visualização dos dados\nOs dados serão lidos com a função read_excel() do pacote readxl:\n\ncirurgia &lt;- readxl::read_excel (\"dados/dadosCirurgia.xlsx\")\nstr(cirurgia)\n\ntibble [144 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:144] 1 2 3 4 5 6 7 8 9 10 ...\n $ sexo     : chr [1:144] \"masc\" \"masc\" \"masc\" \"masc\" ...\n $ peso     : num [1:144] 2020 1850 2540 1150 2900 ...\n $ ig       : num [1:144] 36 30 38 31 36 37 38 39 38 39 ...\n $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ...\n $ infec    : chr [1:144] \"não\" \"não\" \"sim\" \"sim\" ...\n $ cirurgia : chr [1:144] \"abdominal\" \"abdominal\" \"abdominal\" \"outra\" ...\n\n\nA variável infec aparece como caractere e será transformada como fator:\n\ncirurgia$infec &lt;- factor(cirurgia$infec, levels = c(\"sim\", \"não\"))\n\nOs boxplots (Figura 20.1), construídos com a função ggboxplot() do pacote ggpubr com as cores da paleta do New England Journal of Medicine (NEJM), são uma boa maneira de visualizar os dados:\n\nggpubr::ggboxplot(cirurgia,\n                  x = \"infec\",\n                  y = \"tempohosp\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"infec\",\n                  palette = \"nejm\",\n                  legend = \"none\",\n                  ggtheme = theme_bw(),\n                  xlab = \"Presença de infecção\" ,\n                  ylab = \"Tempo de hospitalização (dias)\")   \n\n\n\n\n\n\n\nFigura 20.1: Impacto da infecção no pós-operatório no tempo de hopsitalização\n\n\n\n\n\nOs boxplots exibem uma série de valores atípicos, indicando que existe uma assimetria em ambos os grupos. Essa assimetria também pode ser verificada usando o teste de Shapiro-Wilk, que mostrando valores p &lt; 0,05, confirma que os dados não seguem a distribuição normal. Este teste não é pré-requisito para o teste. Foi realizado como uma demonstração.\n\ncirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  rstatix::shapiro_test(tempohosp)\n\n# A tibble: 2 × 4\n  infec variable  statistic        p\n  &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 sim   tempohosp     0.692 1.47e- 9\n2 não   tempohosp     0.565 9.87e-15\n\n\n\n\n20.4.1.2 Sumarização dos dados\nComo a variável tempohosp é assimétrica conforme mostrado acima, onde ambos os valores p são menores do que 0,05, será realizado um sumário numérico com a obtenção da mediana e IIQ. Isto será feito através da função group_by() e summarise(), incluídas no pacote dplyr.\n\nresumo &lt;- cirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (tempohosp, na.rm = TRUE),\n                   p25=quantile(tempohosp, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(tempohosp, probs = 0.75, na.rm = TRUE))\nresumo\n\n# A tibble: 2 × 5\n  infec     n mediana   p25   p75\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 sim      56      37  22.8    49\n2 não      88      23  18      37\n\n\nOs dados mostram que a mediana de tempo de internação dos neonatos infectados é bem maior do que a mediana dos não infectados.\n\n\n\n20.4.2 Hipóteses estatísticas\nDa mesma maneira que o teste t, as hipóteses estabelecidas comparam dois grupos independentes. Se não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma,\n\n\\(H_{0}\\): As duas populações são iguais.\n\n\n\\(H_{1}\\): As duas populações não são iguais.\n\nNão foi escrita a hipótese nula como sendo que as médias (ou as medianas) são iguais, pois o teste não usa as medidas de posição tradicionais e sim os postos.\n\n\n20.4.3 Pressupostos do teste de Mann_Whitney\nO teste de Mann-Whitney é baseado nos seguintes pressupostos:\n\nOs dados são aleatórios;\nAs amostras são de dois grupos independentes;\nUm dos grupos é denominado de 1 e o outro de 2;\nA variável a ser comparada nos grupos deve ser ordenável;\nO grupo 1 será o grupo de menor tamanho e, se tiverem o mesmo tamanho, o grupo 1 é aquele cuja soma dos postos é a menor.\n\n\n\n20.4.4 Cálculo da estatística de teste\n\n20.4.4.1 Lógica do teste U de Mann-Whitney\nDe acordo com as hipóteses estabelecidas, o teste é bicaudal. Se as observações nos dois grupos forem provenientes da mesma população, a soma dos postos em cada grupo devem ficar próximas.\nPara calcular o teste, procede-se da seguinte maneira:\n\nDeve haver uma variável que identifique o grupo a que pertence cada uma das observações. No exemplo proposto, a variável desfecho é tempohosp e a variável agrupadora é infec, categorizada como sim e não.\nOrdenar de forma crescente todos os valores da variável tempohosp, sem levar em consideração a que grupo pertence. Para realizar este procedimento, será usada a função rank() do R base com o método para empates igual à média dos valores empatados (ties.method=\"average). Ao executar a função, será criada uma nova variável, denotada postos.\n\n\ncirurgia$postos &lt;- rank(cirurgia$tempohosp, ties.method = \"average\") \nhead(cirurgia)\n\n# A tibble: 6 × 8\n     id sexo   peso    ig tempohosp infec cirurgia  postos\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;      &lt;dbl&gt;\n1     1 masc   2020    36        37 não   abdominal   94.5\n2     2 masc   1850    30        37 não   abdominal   94.5\n3     3 masc   2540    38        37 sim   abdominal   94.5\n4     4 masc   1150    31        46 sim   outra      120  \n5     5 masc   2900    36        37 não   abdominal   94.5\n6     6 fem    2480    37        36 sim   abdominal   91  \n\n\n\nVerificar o tamanho (n) de cada grupo (presença ou não de infecção) e somar os postos em cada um dos grupos, usando a função group_by() junto com a função summarise(),\n\n\nresumo &lt;- cirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  dplyr::summarise(n = n(),\n                   soma = sum(postos))\nresumo\n\n# A tibble: 2 × 3\n  infec     n  soma\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 sim      56 4876.\n2 não      88 5564.\n\n\n\nDenominar de grupo_1 o grupo com menor soma:\n\n\ngrupo_1 &lt;- min(resumo$soma)\ngrupo_1\n\n[1] 4875.5\n\n\n\nDenotar o grupo_1 como T\n\n\nT &lt;- grupo_1\n\nConsequentemente,\n\nn1 &lt;- resumo$n[1]\nn1\n\n[1] 56\n\nn2 &lt;- resumo$n[2]\nn2\n\n[1] 88\n\n\n\nCalcular a estística do teste, usando a fórmula preconizada por Altman (Altman 1991):\n\n\\[\nU =n_{1} \\times n_{2} \\ +\\left [\\frac{n_{1} \\times \\left (n_{1} + 1  \\right )}{2}  \\right ] - T\n\\]\n\nU &lt;- (n1*n2 + ((n1*(n1 + 1))/2)) - T\nU\n\n[1] 1648.5\n\n\nObs.: O U de Mann-Whitney aparece no teste de Wilcoxon como W, eles representam o mesmo.\n\nSe \\(n_{1}\\), \\(n_{2}\\) \\(\\ge\\) 10, a distribuição da estatística do teste pode ser aproximada por uma distribuição normal com média igual a\n\n\\[\n\\mu_{U} =\\left [\\frac{n_{S} \\times \\left (n_{L} + 1  \\right )}{2}  \\right ]\n\\]\nonde \\(n_{S}\\) e \\(n_{L}\\), são, respectivamente, o grupo de menor e maior tamanho. No exemplo, \\(n_{1}\\) e \\(n_{2}\\).\n\nm_U &lt;- (n1*(n1+n2+1))/2\nm_U\n\n[1] 4060\n\n\nE desvio padrão igual a\n\\[\n\\sigma_{U}= \\sqrt {\\frac{n_{L}\\times \\sigma_{U}}{6}}\n\\]\n\ndp_U &lt;- sqrt((n2*m_U)/6)\ndp_U\n\n[1] 244.0219\n\n\nOs resultados fornecem os dados para calcular a estatística \\(Z_{U}\\) com correção de continuidade e, a partir dela, calcular o valor P.\n\\[\nZ_{U}= \\frac{(T -0,5) - \\mu_{U}}{\\sigma_{U}}\n\\]\n\nZ_U &lt;- ((T - 0.5) - m_U)/dp_U\nround(Z_U, 2)\n\n[1] 3.34\n\n\n\nFinalmente, calcula-se o valor p, usando a função pnorm(), multiplicada por 2, pois o teste é bicaudal.\n\n\nvalor_p &lt;- pnorm(Z_U, lower.tail = FALSE) * 2\nround(valor_p, 4)\n\n[1] 8e-04\n\n\nNa prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Os cálculos foram mostrados para melhorar o entendimento de como o teste de Mann-Whitney funciona.\n\n\n20.4.4.2 Cálculo do U de Mann-Whitney no R\nO teste pode ser realizado com a função wilcox_test() 1 do pacote rstatix:\n\nteste &lt;- rstatix::wilcox_test(formula = tempohosp ~ infec, data = cirurgia)\nteste\n\n# A tibble: 1 × 7\n  .y.       group1 group2    n1    n2 statistic       p\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tempohosp sim    não       56    88     3280. 0.00083\n\n\nAssim como no cálculo manual, o teste com a função do rstatix, mostra uma diferença estatisticamente significativa (p &lt; 0,001) entre os tempos de hospitalização dos recém-nascidos que realizaram cirurgia no período neonatal que se infectaram ou não.\n\n\n\n20.4.5 Tamanho do efeito\nÉ interessante calcular o tamanho do efeito, a magnitude do efeito. O tamanho do efeito r é calculado como a estatística \\(Z_{U}\\) dividida pela raiz quadrada do tamanho da amostra (\\(n = n_{1} + n_{2}\\)).\n\\[\nr = \\frac {Z_{U}}{\\sqrt{n}}\n\\]\nO valor de \\(Z_{U}\\) é igual a 3.3398648, logo\n\nr &lt;- Z_U/sqrt(n1+n2)\nround(r,3)\n\n[1] 0.278\n\n\nO R possui a função wilcox_effsize() incluída no pacote rstatix. Necessita também do pacote coin (Hothorn et al. 2006) instalado para calcular a estatística r.2 A saída exibirá junto a magnitude o efeito, que no caso é pequena (veja Tabela 20.2).\n\nwilcox_effsize(cirurgia, tempohosp~infec)\n\n# A tibble: 1 × 7\n  .y.       group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 tempohosp sim    não      0.279    56    88 small    \n\n\n\n\n\n\nTabela 20.2: Interpretação do valor r\n\n\n\nrmagnitude0,10 &lt; 0,30pequeno0,30 &lt; 0,50médio&gt;= 0,50grandeSem considerar o sinal\n\n\n\n\n\n\n\n20.4.6 Conclusão\nO valor \\(p &lt; 0,0001\\) está bem abaixo do nível de significância estabelecido (\\(\\alpha = 0,05)\\). Pode-se concluir que o tempo de hospitalização nos dois grupos é estatisticamente diferente. Entretanto, a magnitude dessa diferença é pequena.\nIsto pode ser visualizado no gráfico (Figura 20.2):\n\nggpubr::ggboxplot(cirurgia,\n                   x = \"infec\",\n                   y = \"tempohosp\",\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1,\n                   fill = \"infec\",\n                   palette = \"nejm\",\n                   legend = \"none\",\n                   ggtheme = theme_bw(),\n                   xlab = \"Presença de infecção\" ,\n                   ylab = \"Tempo de hospitalização (dias)\") +\n  labs(subtitle = rstatix::get_test_label(teste, detailed = TRUE))\n\n\n\n\n\n\n\nFigura 20.2: Impacto da infecção no pós-operatório no tempo de hopsitalização",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#teste-de-wilcoxon",
    "href": "20-testes-naoParametricos.html#teste-de-wilcoxon",
    "title": "20  Métodos não paramétricos",
    "section": "20.5 Teste de Wilcoxon",
    "text": "20.5 Teste de Wilcoxon\nO teste de Wilcoxon, também conhecido como teste dos postos com sinais de Wilcoxon (Wilcoxon Signed-Rank Test), é um teste não paramétrico utilizado em situações em que existem dois conjuntos de dados emparelhados, ou seja, dados provenientes do mesmo participante. O teste não examina os dois grupos individualmente; em vez disso, ele se concentra na diferença existente entre cada par de observações. É um equivalente não paramétrico do teste t pareado.\n\n20.5.1 Dados usados como exemplo\n\n\n\n\n\n\nCenário\n\n\n\nPara verificar se a realização de exercícios aeróbicos modifica a função respiratória de 10 escolares asmáticos, foi medido o Pico de Fluxo Expiratório Máximo (Peak Flow Meter) no início e no final do programa, após 120 dias. O Pico de Fluxo Expiratório Máximo (PFE), medido em L/min, serve como uma forma simples de avaliar a força e a velocidade de saída do ar de dentro dos pulmões.\n\n\nOs dados têm apenas três variáveis, id(identificação), basal (PFE basal) e final(PFE final).\n\nid &lt;- c(1:10)\nbasal &lt;- c(120, 200, 140, 200, 110, 240, 150, 120, 250, 190)\nfinal &lt;- c(220, 300, 230, 180, 300, 330, 230, 250, 300, 200)\n\ndados &lt;- tibble(id, basal, final)\n\nstr (dados)\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id   : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ basal: num [1:10] 120 200 140 200 110 240 150 120 250 190\n $ final: num [1:10] 220 300 230 180 300 330 230 250 300 200\n\n\nA questão de pesquisa a ser respondida, portanto, é:\n\nExiste diferença entre as medidas iniciais e finais do PFE dos escolares asmáticos que entraram em um programa de exercícios aeróbicos?\n\n\n20.5.1.1 Exploração e transformação dos dados\nOs dados estão no formato amplo com as variáveis basal e final classificadas como númericas. Serão transformados para o formato longo, usando a função pivot_longer() do pacote tidyr. Este processo é opcional, mas, como foi feito com o teste t pareado, será repetido aqui, por questões didáticas:\n\ndadosL &lt;- dados %&gt;% \n  tidyr::pivot_longer(c(basal, final), \n                      names_to = \"momento\", \n                      values_to = \"medidas\")\nstr(dadosL)\n\ntibble [20 × 3] (S3: tbl_df/tbl/data.frame)\n $ id     : int [1:20] 1 1 2 2 3 3 4 4 5 5 ...\n $ momento: chr [1:20] \"basal\" \"final\" \"basal\" \"final\" ...\n $ medidas: num [1:20] 120 220 200 300 140 230 200 180 110 300 ...\n\n\n\n\n20.5.1.2 Medidas resumidoras\nComo o número de participantes é de apenas 10, a medida de posição mais adequada para resumir os dados é mediana e a medida de dispersão é o intervalo interquartil (IIQ). Para isso, se fará uso das funções group_by() e summarise() do pacote dplyr:\n\nresumo &lt;- dadosL %&gt;% \n  dplyr::group_by(momento) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (medidas, na.rm = TRUE),\n                   p25=quantile(medidas, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(medidas, probs = 0.75, na.rm = TRUE),\n                   media = mean (medidas, na.rm = TRUE),\n                   dp = sd (medidas, na.rm = TRUE),\n                   ep = dp/sqrt(n),\n                   me = ep * qt(1 - (0.05/2), n - 1))\nresumo\n\n# A tibble: 2 × 9\n  momento     n mediana   p25   p75 media    dp    ep    me\n  &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 basal      10     170  125    200   172  50.9  16.1  36.4\n2 final      10     240  222.   300   254  50.4  15.9  36.0\n\n\n\n\n20.5.1.3 Visualização dos dados\nPode-se fazer visualização gráfica dos dados usando um boxplot (Figura 20.3) ou um gráfico de linha (Figura 20.4).\n\n20.5.1.3.1 Boxplot\n\nggpubr::ggboxplot(dadosL,\n                  x = \"momento\",\n                  y = \"medidas\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"momento\",\n                  palette = c(\"cyan4\", \"cyan3\"),\n                  legend = \"none\",\n                  ggtheme = theme_bw(),\n                  xlab = \"Momento\" ,\n                  ylab = \"PEF (L/min) \")+\n  theme (text = element_text (size = 13),\n         axis.text.x= element_text(size = 12)) \n\n\n\n\n\n\n\nFigura 20.3: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos\n\n\n\n\n\n\n\n20.5.1.3.2 Gráfico de linha\n\nggpubr::ggline(dadosL,\n               x = \"momento\",\n               y = \"medidas\",\n               color = \"cyan4\",\n               linetype = \"dashed\",\n               size = 0.7,\n               add = \"mean_ci\",\n               point.size = 2,\n               xlab = \"Momento\" ,\n               ylab = \"PEF (L/min) \",\n               ggtheme = theme_bw()) +\n  theme (text = element_text (size = 13),\n         axis.text.x= element_text(size = 12))\n\n\n\n\n\n\n\nFigura 20.4: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos\n\n\n\n\n\n\n\n\n20.5.1.4 Criação de uma variável que represente a diferença entre os momentos\nA diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados):\n\ndados$D &lt;- dados$basal - dados$final\nhead (dados)\n\n# A tibble: 6 × 4\n     id basal final     D\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   120   220  -100\n2     2   200   300  -100\n3     3   140   230   -90\n4     4   200   180    20\n5     5   110   300  -190\n6     6   240   330   -90\n\n\nResumo da variável D\nAo resumo será atribuído ao nome resumo2:\n\nresumo2 &lt;- dados %&gt;% \n  dplyr::summarise(n = n (),\n                   mediana = median (D, na.rm = TRUE),\n                   p25=quantile(D, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(D, probs = 0.75, na.rm = TRUE))\nresumo2\n\n# A tibble: 1 × 4\n      n mediana   p25   p75\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10     -90  -100 -57.5\n\n\nO sinal negativo demonstra que houve um aumento do PFE do momento basal para o final.\n\n\n\n20.5.2 Definição das hipóteses estatísticas\nDa mesma maneira que o teste t pareado, as hipóteses estabelecidas comparam dois grupos dependentes. O teste de Wilcoxon é usado para avaliar a hipótese nula de que a distribuição das diferenças entre os grupos tem uma diferença mediana igual a 0.\n\n\\(H_{0}: D_{i} = 0\\)\n\n\n\\(H_{A}: D_{i} \\ne 0\\)\n\nNote que a \\(H_{A}\\) estabelece que a diferença pode aumentar ou diminuir. Logo, o teste é bicaudal.\n\n\n20.5.3 Execução do teste estatístico\n\n20.5.3.1 Lógica do teste de Wilcoxon\n\nA ideia do teste é verificar se as diferenças positivas são maiores ou menores, em grandeza absoluta, que as diferenças negativas. Para isso, foi criada, anteriormente, a variável D. Agora, será criada outra variável, iguala a variável D, apenas ignorando o sinal, denominada D_abs, diferença absoluta entre as variáveis final e basal.\n\n\ndados$D_abs &lt;- abs(dados$basal - dados$final)\n\n\nExcluir os casos com diferença igual a 0 (zero). Para isso, uma maneira possível é extrair um subconjunto de dados do conjunto principal (dados), criando um conjunto de dados com a função filter() do pacote dplyr, que receberá o nome de dados1. O argumento D_absbs != 0 significa todas as diferenças absolutas diferentes de 0:\n\n\ndados1 &lt;- dados %&gt;% dplyr::filter(D_abs != 0)\n\nObserve que como não há diferenças zeradas. Ou seja, o novo conjunto de dados continua o mesmo. O que pode ser confirmado, executando a função str():\n\nstr (dados1)\n\ntibble [10 × 5] (S3: tbl_df/tbl/data.frame)\n $ id   : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ basal: num [1:10] 120 200 140 200 110 240 150 120 250 190\n $ final: num [1:10] 220 300 230 180 300 330 230 250 300 200\n $ D    : num [1:10] -100 -100 -90 20 -190 -90 -80 -130 -50 -10\n $ D_abs: num [1:10] 100 100 90 20 190 90 80 130 50 10\n\n\n\nOrdenar de forma crescente todos os valores da variável D_abs do banco de dados dados1, usando a função arrange() do pacote dplyr:\n\n\ndados1 &lt;- dados1 %&gt;% dplyr::arrange(dados1$D_abs)\n\n\nEstabelecer postos para os valores ordenados da variável D_abs, do conjunto de dados dados1, fazendo a média das ordens quando houver empate. A execução deste comando cria uma nova variável, chamada postos:\n\n\ndados1$postos &lt;- rank(dados1$D_abs)\n\n\nEstabelecer sinais para os postos, criando dois subconjuntos de dados do conjuntos dados1, um com os escolares com postos positivos (pos) e outros com postos negativos(neg):\n\n\nneg &lt;- dados1 %&gt;% dplyr::filter(D &lt; 0)\npos &lt;- dados1 %&gt;% dplyr::filter(D &gt; 0)\n\n\nSomar todos os postos (variável posto) em cada um dos subconjuntos criados (neg e pos):\n\n\nsoma_neg &lt;- sum(neg$postos)\nsoma_pos &lt;- sum(pos$postos)\nprint (c(soma_neg, soma_pos))\n\n[1] 53  2\n\n\n\nAtribuir a menor soma à estatística do teste, denotada T:\n\n\nT &lt;- min (soma_neg : soma_pos)\nT\n\n[1] 2\n\n\n\nPara dados com tamanhos grandes (&gt; 20 pares), a significância de T pode ser determinada (Zar 2014b), considerando que a distribuição de T tem aproximadamente distribuição normal com média igual a\n\n\\[\n\\mu_{T} =\\frac{n \\times \\left (n + 1  \\right )}{4}\n\\]\nonde n é o tamanho da amostra.\n\nn &lt;- length(dados$D)\nmu_T &lt;- (n * (n + 1))/4\nmu_T\n\n[1] 27.5\n\n\nE desvio padrão igual a:\n\\[\n\\sigma_{T}= \\sqrt {\\frac{n\\left (n + 1  \\right )\\times \\left (2n + 1  \\right )}{24}}\n\\]\n\ndp_T &lt;- sqrt ((n*(n + 1)) * (2 * n + 1) /24) \ndp_T\n\n[1] 9.810708\n\n\nOs resultados da execução das equações fornecem os dados para calcular a estatística Z_T com correção de continuidade e, a partir dela, calcular o valor p.\n\\[\nZ_{T}= \\frac{\\left |T - \\mu_{T}  \\right | - 0,5}{\\sigma_{T}}\n\\]\n\nZ_T &lt;- (abs(T - mu_T)- 0.5)/dp_T\nZ_T\n\n[1] 2.548236\n\n\n\nConcluindo, o valor da estatística de teste T é superior ao \\(Z_{crítico} = 1,96\\), para um \\(\\alpha = 0,05\\). Dessa forma, a \\(H_{0}\\) é rejeitada. Existe uma diferença significativa entre o PFE basal e o PFE final, neste grupo de escolares asmáticos.\nO valor p pode ser obtido com a função pnorm() e multiplicando o resultado por 2, pois o teste é bilateral.\n\n\nvalor_p &lt;- pnorm (Z_T, lower.tail = FALSE) * 2\nvalor_p\n\n[1] 0.01082692\n\n\nComo já dito anteriormente, na prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Eles são apenas uma demonstração de como o teste funciona.\n\n\n20.5.3.2 Cálculo do teste de Wilcoxon no R\nUsando o conjunto de dados no formato longo (dadosL), calcula-se o teste com a função wilcox_test() do pacote rstatix. É a mesma função utilizada para o teste de U de Mann-Whitney, mudando apenas o argumento paired=FALSE para paired=TRUE:\n\nteste1 &lt;- dadosL %&gt;% \n  rstatix::wilcox_test(medidas ~ momento, paired = TRUE) %&gt;% \n  rstatix::add_significance()\nteste1\n\n# A tibble: 1 × 8\n  .y.     group1 group2    n1    n2 statistic      p p.signif\n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 medidas basal  final     10    10         2 0.0107 *       \n\n\nObserve que o resultado é o mesmo calculado manualmente.\n\n\n\n20.5.4 Tamanho do efeito\nO tamanho do efeito pode ser calculado da mesma forma que para o teste de Mann-Whitney (Seção 20.4.5), usando a mesma equação e os dados obtidos acima, onde 2.548236 e n = 10 tem-se\n\\[\nr = \\frac {Z_{T}}{\\sqrt{n}}\n\\]\nPode-se usar também a função wilcox_effsize() para calcular a estatística r. A Saída exibe junto a magnitude o efeito, que no caso é grande (&gt; 0,5 como mostra a Tabela 20.2 do teste de Mann-Whitney).\n\ndadosL %&gt;% \n  wilcox_effsize(medidas ~ momento, paired = TRUE)\n\n# A tibble: 1 × 7\n  .y.     group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 medidas basal  final    0.823    10    10 large    \n\n\n\n\n20.5.5 Conclusão\nAssumindo um \\(\\alpha = 0,05\\), se o valor p, obtido pelo teste, for menor do que 0,05, rejeita-se a hipótese nula (V = 2, p = 0,01, n = 10).\nPode-se concluir que existe diferença nas medidas do pico de fluxo expiratório máximo no início e no fim do programa de exercícios aeróbicos realizados pelos escolares asmáticos e a magnitude do efeito foi grande (r = 0,82).\nIsto pode ser visualizado na Figura 20.5:\n\nbxp &lt;- ggpubr::ggboxplot(dadosL,\n                         x = \"momento\",\n                         y = \"medidas\",\n                         bxp.errorbar = TRUE,\n                         bxp.errorbar.width = 0.1,\n                         fill = \"momento\",\n                         palette = c(\"cyan4\", \"cyan3\"),\n                         legend = \"none\",\n                         ggtheme = theme_bw(),\n                         xlab = \"Momento\" ,\n                         ylab = \"PEF (L/min) \") +\n  theme (text = element_text (size = 13),\n         axis.text.x = element_text(size = 11))\n\nteste &lt;- dadosL %&gt;% \n  rstatix::wilcox_test (medidas ~ momento, paired = TRUE) %&gt;%\n  rstatix::add_significance ()\nteste &lt;- teste %&gt;% rstatix::add_xy_position ()\n\nbxp + \n  stat_pvalue_manual (teste, \n                      tip.length = 0) +\n  labs (subtitle = get_test_label (stat.test = teste, \n                                   detailed = TRUE))\n\n\n\n\n\n\n\nFigura 20.5: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#sec-kruskalwallis",
    "href": "20-testes-naoParametricos.html#sec-kruskalwallis",
    "title": "20  Métodos não paramétricos",
    "section": "20.6 Teste de Kruskal-Wallis",
    "text": "20.6 Teste de Kruskal-Wallis\nQuando os pressupostos subjacentes à ANOVA não são atendidos, é possível usar o teste não paramétrico de Kruskal-Wallis (KW) para testar a hipótese de que os parâmetros de localização são iguais. Pode ser considerado uma extensão do teste de Wilcoxon-Mann-Whitney.\nEnquanto a ANOVA depende da hipótese de que todas as populações são independentes e com resíduos normalmente distribuídos, o teste de Kruskal-Wallis exige apenas amostras aleatórias independentes provenientes de suas respectivas populações. Entretanto, este teste somente deve ser aplicado se a amostra for pequena e/ou os pressupostos para a ANOVA forem seriamente violados.\nO teste não usa diretamente medições de quantidade conhecida, utiliza, como outros testes não paramétricos, os postos dos valores analisados. Em função disso, é também conhecido como análise de variância de um fator em postos.\n\n20.6.1 Dados usados no exemplo\n\n\n\n\n\n\nCenário\n\n\n\nUm experimento foi realizado para verificar se o álcool ou o café afetam os tempos de reação ao dirigir (Karadimitriou e Marshall 2020). O estudo tem três grupos diferentes de participantes: 10 bebendo água (controle), 10 bebendo cerveja contendo duas unidades de álcool3 e 10 bebendo café. O tempo de reação em uma simulação de direção foi medido para cada participante.\n\n\nOs dados encontram-se no arquivo dadosResposta.xlsx. Clique aqui para baixar e, após, salve o mesmo no seu diretório de trabalho.\nAs variáveis são:\n\nid \\(\\to\\) identificação do participante;\ntempo \\(\\to\\) tempo de reação na simulação de direção em segundos;\nbebida \\(\\to\\) três grupo: água, álcool e café.\n\nO estudo pretende verificar se existe diferença no tempo de reação dos participantes em um teste de direção com a ingesta de água, café e álcool.\n\n20.6.1.1 Leitura e exploração dos dados\nComo o dados estão contidos em um arquivo Excel (.xlsx), serão lidos com a função read_excel() do pacote readxl e a sua estrutura será observada com função str():\n\ndados &lt;- read_excel (\"dados/dadosResposta.xlsx\")\nstr(dados)\n\ntibble [30 × 3] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ...\n $ bebida: chr [1:30] \"agua\" \"agua\" \"agua\" \"agua\" ...\n\n\nO formato do arquivo é o longo. A variável bebida encontra-se como caracter e deve ser transformada em fator e as categorias na sequência: agua, cafe e alcool.\n\ndados$bebida &lt;- factor(dados$bebida, \n                       levels = c(\"agua\", \"cafe\", \"alcool\"))\n\nOs dados serão observados visualmente através de boxplots (Figura 20.6), usando a função ggplot() do pacote ggplot2, com cores do nejm (New England Journal of Medicine) o pacote ggsci.\n\nggpubr::ggboxplot(dados,\n                   x = \"bebida\",\n                   y = \"tempo\",\n                   bxp.errorbar = T,\n                   bxp.errorbar.width = 0.1,\n                   fill = \"bebida\",\n                   palette = \"nejm\",\n                   legend = \"none\",\n                   ggtheme = theme_bw(),\n                   xlab = \"Tipo de bebida\" ,\n                   ylab = \"Tempo de reação (seg)\") +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\nFigura 20.6: Impacto do tipo de bebida no tempo de reação ao dirigir\n\n\n\n\n\nOs boxplots exibem dados com medianas visualmente diferentes, bigodes diferentes e grupos com presença de outliers. Para verificar o impacto desses achados, pode-se usar a função identify_outliers(), do pacote rstatix que confirma, na sua Saída, a presença de outliers no grupo agua e cafe, sendo dois extremos.\n\ndados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  rstatix::identify_outliers(tempo)\n\n# A tibble: 4 × 5\n  bebida    id tempo is.outlier is.extreme\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 agua       9  1.63 TRUE       FALSE     \n2 agua      10  1.97 TRUE       TRUE      \n3 cafe      19  2.56 TRUE       FALSE     \n4 cafe      20  3.07 TRUE       TRUE      \n\n\nPara avaliar a normalidade será usado o teste de Shapiro-Wilk, com a função shapiro_test() e a função group_by() do pacote dplyr:\n\ndados %&gt;% \n  dplyr::group_by (bebida) %&gt;% \n  rstatix::shapiro_test (tempo) \n\n# A tibble: 3 × 4\n  bebida variable statistic      p\n  &lt;fct&gt;  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 agua   tempo        0.863 0.0837\n2 cafe   tempo        0.815 0.0220\n3 alcool tempo        0.875 0.114 \n\n\nA variável cafe tem uma distribuição que não se ajusta a distribuição normal.\nPara completar a exploração dos dados, será solicitado, usando as funções group_by () e summarise, do pacote dplyr, medidas de localização e dispersão adquadas para variáveis bem assimétricas.\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (tempo, na.rm = TRUE),\n                   p25=quantile(tempo, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(tempo, probs = 0.75, na.rm = TRUE))\nresumo\n\n# A tibble: 3 × 5\n  bebida     n mediana   p25   p75\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 agua      10   0.845 0.653 0.937\n2 cafe      10   1.44  1.28  1.68 \n3 alcool    10   2.25  1.77  2.85 \n\n\n\n\n\n20.6.2 Hipóteses estatísticas\nSe não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma,\n\n\\(H_{0}\\): As populações são iguais.\n\n\n\\(H_{1}\\): Pelo menos uma das populações tende a exibir valores diferentes do que as outras populações.\n\n\n\n20.6.3 Pressupostos do teste\nO teste de Kruskal-Wallis pressupõe as seguintes condições para o seu adequado uso:\n\nAs amostras são amostras aleatórias independentes de suas respectivas populações;\nA escala de medição utilizada é pelo menos ordinal e, se houver apenas três grupos, deve haver pelo menos 5 casos em cada grupo;\nAs distribuições dos valores nas populações amostradas são idênticas, exceto pela possibilidade de que uma ou mais das populações sejam compostas por valores que tendem a ser maiores do que os das outras populações.\n\n\n\n20.6.4 Execução do teste estatístico\n\n20.6.4.1 Lógica do teste de Kruskall-Wallis\nA teoria do teste Kruskal-Wallis é semelhante à do teste de Mann-Whitney, ou seja, tem como base a soma dos postos. Em primeiro lugar, os escores são ordenados do menor para o maior, independentemente do grupo que pertençam.\nO menor recebe o posto 1 e assim por diante. Após a atribuição dos postos, soma-se os postos por grupo. A soma dos postos de cada grupo é representada por \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\), …, \\(R_{i}\\). A estatística do teste, H, é calculada com a equação (Zar 2014a):\n\\[\nH =\\frac {12}{N \\times \\left (N + 1  \\right )} \\sum_{i=1}^{k} \\frac {R_{i}^{2}}{n_{{i}}}-3 \\times\\left (N + 1\\right)\n\\]\nonde \\(n_{i}\\) é o número de observações no grupo i, \\(N = \\sum_{i=1}^{k}\\times n_{i}\\) (o número total de observações em todos os k grupos) e \\(R_{i}\\) é a soma dos postos das \\(n_{i}\\) observações no grupo i.\nUma boa verificação (mas não uma garantia) de que os postos foram atribuídos corretamente é ver se a soma de todos os postos é igual a \\(\\frac {N \\times \\left (N + 1\\right )}{2}\\).\n\nCriar a variável posto com os postos ordenados de forma crescente, independente do grupo, como realizado no teste de Mann-Whitney:\n\n\ndados$posto &lt;- rank(dados$tempo, ties.method = \"average\")\nstr(dados)\n\ntibble [30 × 4] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ...\n $ bebida: Factor w/ 3 levels \"agua\",\"cafe\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ posto : num [1:30] 1 2 3 4 5 6 7 8 16 22.5 ...\n\n\n\nSomar os postos de cada grupo separadamente:\n\n\nresumo1 &lt;- dados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  dplyr::summarise(n = n(),\n                   soma = sum(posto))\nresumo1\n\n# A tibble: 3 × 3\n  bebida     n  soma\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 agua      10  74.5\n2 cafe      10 157  \n3 alcool    10 234. \n\n\n\nCálculo da estatística do teste H\n\n\nN &lt;- 30\nn &lt;- 10\nR_agua &lt;- resumo1[1,3]\nR_alcool &lt;- resumo1[2,3]\nR_cafe &lt;- resumo1[3,3]\n\nH &lt;- (12/(N*(N+1))) * ((R_agua^2/n) + (R_alcool^2/n) + (R_cafe^2/n)) - (3*(N+1))\nH\n\n      soma\n1 16.31806\n\n\n\nCálculo do Valor p\n\nSe existir três grupos, com cinco ou menos participantes em cada grupo, há necessidade de usar a tabela especial para tamanhos de amostra pequenos (Kanji 2006). Se você tiver mais de cinco participantes por grupo, trate H como qui-quadrado. A estatística H é estatisticamente significativo se for igual ou maior que o valor crítico qui-quadrado para o grau de liberdade específico, igual a \\(k - 1\\). Aqui, tem-se 10 participantes por grupo e, assumindo um \\(\\alpha = 0,05\\), o \\(H_{crítico}\\) é igual a:\n\nalpha &lt;- 0.05\nk &lt;- 3\ngl = k - 1\nH_critico &lt;- qchisq(1 - alpha, gl)\nH_critico\n\n[1] 5.991465\n\n\nUma vez que o \\(H_{calculado} = 16,3\\) é maior que \\(H_{crítico} = 6,0\\) , rejeita-se a \\(H_{0}\\). O valor P é obtido através da função pchisq():\n\nH &lt;- 16.32\npchisq(H, 2, lower.tail = FALSE)\n\n[1] 0.0002858624\n\n\nO R tem funções que fazem facilmente esses cálculos enfadonhos. Eles são colocados aqui apenas para ilustrar o raciocínio de como o teste de Kruskal-Wallis funciona. Sempre existem curiosos lendo o livro!\n\n\n20.6.4.2 Teste de Kruskal-Wallis no R\nNo R, pode-se calcular o teste, usando a função kruskal_test() do pacote rstatix, cujos argumentos podem ser consultados na ajuda do RStudio.\n\nteste &lt;- rstatix::kruskal_test (data = dados, formula = tempo ~ bebida)\nteste\n\n# A tibble: 1 × 6\n  .y.       n statistic    df        p method        \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 tempo    30      16.3     2 0.000286 Kruskal-Wallis\n\n\n\n\n\n20.6.5 Tamanho do efeito\nO eta quadrado (\\(\\eta^{2}\\)), com base na estatística H, pode ser usado como a medida do tamanho do efeito do teste de Kruskal-Wallis. É calculado pela equação:\n\\[\n\\eta_{H}^{2} = \\frac {\\left (H - k + 1 \\right)}{\\left (N - k\\right)}\n\\]\nonde H é a estatística obtida no teste de Kruskal-Wallis; k é o número de grupos; N é o número total de observações (Tomczak e Tomczak 2014).\nA estimativa eta ao quadrado assume valores de 0 a 1 e, multiplicada por 100, indica a porcentagem de variância na variável dependente explicada pela variável independente. Pode ser obtido no R com a função kruskal_effsize() do pacote rstatix:\n\ndados %&gt;% kruskal_effsize (tempo~bebida)\n\n# A tibble: 1 × 5\n  .y.       n effsize method  magnitude\n* &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;ord&gt;    \n1 tempo    30   0.530 eta2[H] large    \n\n\nUm efeito \\(\\ge 0,14\\) é considerado grande e \\(&lt;0,06\\) é pequeno (Watson 2021).\n\n\n20.6.6 Testes post hoc\nA partir do resultado do teste de Kruskal-Wallis, sabe-se que há uma diferença significativa entre os grupos, mas não se sabe quais pares de grupos são diferentes.\nUm teste de Kruskal-Wallis significativo é geralmente seguido pelo teste de Dunn (Dunn 1964) para identificar quais grupos são diferentes.\nPara realizar as múltiplas comparações, no R, pode ser usada a função dunn_test(), incluído no pacote rstatix. O ajuste de P é feito pelo método de Bonferroni:\n\npwc &lt;- dados %&gt;% \n  dunn_test (tempo ~ bebida, p.adjust.method = \"bonferroni\") \npwc\n\n# A tibble: 3 × 9\n  .y.   group1 group2    n1    n2 statistic         p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 tempo agua   cafe      10    10      2.10 0.0361    0.108    ns          \n2 tempo agua   alcool    10    10      4.04 0.0000537 0.000161 ***         \n3 tempo cafe   alcool    10    10      1.94 0.0520    0.156    ns          \n\n\nA saída do teste de Dunn, mostra que existe uma diferença estatisticamente significativa apenas entre a água e o álcool, valor p ajustado igual a 1.61^{-4}.\n\n\n20.6.7 Conclusão\nUm teste de Kruskal-Wallis foi realizado para comparar os tempos de reação em uma simulação de direção após beber água, café ou álcool. Houve evidência de uma diferença (p = 0,00029) de pelo menos um par de grupos (Figura 20.7).\nO teste de comparações de pares, usando o teste de Dunn, foi realizado para os três pares de grupos. Houve evidencia de diferença entre o grupo que consumiu duas unidades de álcool e o grupo que ingeriu água (p ajustado (Bonferroni) = 0,00016). Entre os demais pares não houve diferença significativa. O tempo mediano de reação para o grupo que recebeu água foi de 0,84 (0,65 – 0,94) segundos, em comparação com 2,25(1,77 – 2,85) segundos no grupo que bebeu cerveja equivalente a duas unidades de álcool, enquanto para o café foi de 1,45(1,28 – 1,69) segundos.\n\npwc &lt;- pwc %&gt;% rstatix::add_xy_position(x= \"bebida\")\n\nggpubr::ggboxplot(dados,\n                  x = \"bebida\",\n                  y = \"tempo\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"bebida\",\n                  palette = \"nejm\",\n                  legend = \"none\",\n                  ggtheme = theme_bw())+\n  ggpubr::stat_pvalue_manual (pwc,\n                              label = \"p = {scales::pvalue(p.adj)}\",\n                              label.size = 3.2,\n                              hide.ns = FALSE) +\n  ggplot2::labs(x = \"Tipo de bebida\", \n                y = \"Tempo de reação (seg)\",\n                subtitle = get_test_label (teste, detailed = TRUE),\n                caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 20.7: Impacto do tipo de bebida no tempo de reação ao dirigir\n\n\n\n\n\n\n\n\n\n\n\nAltman, Douglas G. 1991. «Comparing groups: continuos data». Em Practical Statistics for Medical Research, 194–97. London: Chapman & Hall/CRC.\n\n\nDunn, Olive Jean. 1964. «Multiple comparisons using rank sums». Technometrics 6 (3). Taylor & Francis: 241–52.\n\n\nHothorn, Torsten, Kurt Hornik, Mark A Van De Wiel, e Achim Zeileis. 2006. «A lego system for conditional inference». The American Statistician 60 (3). Taylor & Francis: 257–63.\n\n\nKanji, Gopal K. 2006. «The Kruskal–Wallis test». Em 100 Statiscal Tests, 3rd Edition, 220. London: Sage publications.\n\n\nKaradimitriou, Sofia Maria, e Ellen Marshall. 2020. «Kruskal-Wallis in R». Statistics Support for Students. Loughborough; Coventry Universities. https://www.statstutor.ac.uk/.\n\n\nTomczak, Maciej, e Ewa Tomczak. 2014. «The need to report effect size estimates revisited. An overview of some recommended measures of effect size». Trends in sport sciences 1 (21): 19–25.\n\n\nWatson, Peter. 2021. «Rules of thumb on magnitudes of effect sizes». MRC Cognition and Brain Sciences Unit. Cambridge University. https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize.\n\n\nZar, Jerrold H. 2014a. «Nonparametric Analysis of Variance». Em Biostatistical Analysis, 226–30. Edinburgh: Pearson.\n\n\n———. 2014b. «Paired-Sample Hypotheses». Em Biostatistical Analysis, 189–98. Edinburgh: Pearson.",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "20-testes-naoParametricos.html#footnotes",
    "href": "20-testes-naoParametricos.html#footnotes",
    "title": "20  Métodos não paramétricos",
    "section": "",
    "text": "O cálculo pode também ser realizado, usando a função wilcox.test() do pacote stats, incluído no R base.↩︎\nPara maiores detalhes consulte a ajuda da função.↩︎\nPara calcular as unidades de álcool, multiplique o volume da bebida (em mililitros) pelo teor alcoólico (ABV, do inglês Alcohol by Volume) e divida o resultado por 1000.↩︎",
    "crumbs": [
      "Parte VIII - Métodos Não Paramétricos",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html",
    "href": "21-testesDiag.html",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "",
    "text": "21.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr,\n               epiR,\n               flextable,\n               knitr,\n               pROC,\n               readxl,\n               vcd)",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#sec-diagbayes",
    "href": "21-testesDiag.html#sec-diagbayes",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.2 Raciocínio Bayesiano no diagnóstico médico",
    "text": "21.2 Raciocínio Bayesiano no diagnóstico médico\nO processo diagnóstico é o centro da atenção da atividade médica na busca de reduzir as incertezas e reconhecer a que classe pertence determinado paciente. Portanto, é extremamente importante saber quão bem os testes diagnósticos podem prever que um indivíduo é portador de certa condição ou doença. Entende-se aqui como teste diagnóstico todo o processo diagnótico, desde o exame clínico até o mais sofisticado exame de imagem ou laboratorial. A ideia é saber como o teste diagnóstico se comporta para separar um “doente” e um “não doente”; qual a sua validade neste processo?\nDeve-se sempre ter em mente que o estabelecimento do diagnóstico é um processo imperfeito que resulta em uma probabilidade ao invés de uma certeza de estar correto. Ou seja, cada vez mais os médicos têm que aplicar as leis da probabilidade na avaliação de testes diagnósticos e sinais clínicos.\nA abordagem Bayesiana denomina de probabilidade a priori a probabilidade estabelecida inicialmente, baseada apenas na experiência do médico, em seu conhecimento em relação a doença suspeitada. Diante de uma evidência de doença, pode ser solictado um teste diagnóstico. Quando ele recebe um teste positivo para uma doença, a probabilidade muda, passa a ser uma probabilidade condicional, probabilidade da doença dado que o teste é positivo, denominada probabilidade a posteriori.\nUm teste que define corretamente quem é doente e quem não é doente é denominado de padrão-ouro ou padrão de referência. Algumas vezes, o teste padrão de referência é simples e barato. Outras vezes, é caro, difícil de obter, tecnicamente complexo, arriscado ou pouco prático. Inclusive, pode não hver padrão-ouro. Em função dessas limitações, outros testes são usados e, como consequência, podem ocorrer erros. Em outras palavras, no processo diagnóstico podem ocorrer falsos positivos e falsos negativos. Esta incerteza, na utilização de testes diagnósticos, gera a necessidade de o médico conferir a probabilidade de falsos positivos e falsos negativos na elaboração de um diagnóstico ao receber o resultado positivo ou negativo de um exame. Uma maneira simples de mostrar as relações de um teste diagnóstico e o verdadeiro diagnóstico, é mostrada na tabela de contingência \\(2\\times2\\) (Tabela 21.1).\n\n\n\n\nTabela 21.1: Falsos positivos e falsos negativos",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#sensibilidade-e-especificidade",
    "href": "21-testesDiag.html#sensibilidade-e-especificidade",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.3 Sensibilidade e Especificidade",
    "text": "21.3 Sensibilidade e Especificidade\nAs estatísticas mais utilizadas para descrever a validade dos testes de diagnóstico em contextos clínicos são sensibilidade e a especificidade.\nSensibilidade é a habilidade do teste em identificar corretamente quem tem a doença. É a taxa de verdadeiros positivos (VP) de um teste e corresponde a probabilidade de um indivíduo com a doença ter um teste positivo.\nUm teste sensível raramente deixará passar pessoas que tenham a doença. Testes com sensibilidade alta são úteis para excluir a presença de uma doença. Isto é, um teste negativo exclui virtualmente a possibilidade de o paciente ter a doença de interesse, pois tem pouca probabilidade de produzir resultados falsos negativos. Isto pode ser lembrado pelo mnemônico SnNout, do inglês: High Sensivity, a Negative result rules out the diagnosis (Straus, Glasziou, et al. 2019).\nEspecificidade é a habilidade do teste em identificar corretamente quem não tem a doença. É a taxa de verdadeiros negativos (VN) de um teste e corresponde a probabilidade de um indivíduo sem a doença ter um teste negativo. Um teste específico raramente classificará de forma errônea indivíduos sendo portadores da doença quando eles não são. Os testes muito específicos são usados para confirmar a presença da doença. Se o teste é altamente específico, um teste positivo sugere fortemente a presença da doença de interesse.\nDe forma similar que a sensibilidade pode-se usar o mnemônico SpPin, do inglês: High Specificity, a Positive result rules in the diagnosis (Straus, Glasziou, et al. 2019).\nEstas estatísticas de diagnóstico podem ser calculadas a partir das equações, cujas letras representam as caselas da tabela \\(2 \\times 2\\) da Tabela 21.1;\n\\[\nSensibilidade = \\frac {a}{\\left (a + c\\right )} \\quad \\quad Especificidade = \\frac {d}{\\left (b + d\\right )}\n\\]\nA taxa de falsos negativos (TFN) é a proporção de indivíduos que têm a doença e que têm um resultado de teste negativo e a taxa de falsos positivos (TFP) é a proporção de pacientes que não possuem a doença e que apresentam resultados positivos. Podem ser expressas pelas equações:\n\\[\nTFN= \\frac {c}{\\left (a + c\\right )} \\quad ou \\quad \\left (1 - sensibilidade\\right)\n\\]\n\\[\nTFP= \\frac {b}{\\left (b + d\\right )} \\quad ou \\quad \\left (1 - especifcidade\\right)\n\\]\nIdealmente, um teste de diagnóstico deveria ter altos níveis de sensibilidade e especificidade. No entanto, isso não é possível, pois existe um balanço entre sensibilidade e especificidade. À medida que a especificidade aumenta, a sensibilidade diminui e vice-versa. As curvas ROC podem ser usadas para identificar um ponto de corte em uma medição contínua que maximize a sensibilidade e a especificidade (veja Seção 21.6).\nQuando um clínico tem um paciente cujo teste apresentou resultado positivo, a pergunta mais importante é a seguinte: dado que o teste é positivo, qual é a probabilidade de o paciente ter a doença? A sensibilidade do teste não responde a este questionamento, mas sim a probabilidade de um resultado positivo, dado que o paciente tem a doença (Altman e Bland 1994c).\n\n21.3.1 Dados do exemplo\n\n\n\n\n\n\nCenário\n\n\n\nForam avaliados 156 pacientes com diagnóstico clínico de apendicite aguda, submetidos à ultrassonografia abdominal e apendicectomia laparoscópica, acompanhado de estudo anatomopatológico dos apêndices extirpados (Peixoto, Nunes, e Gomes 2011).\n\n\nOs dados estão contidos no conjunto de dados dadosApendicite.xlsx que pode ser obtido aqui. Salve o mesmo em seu diretório de trabalho.\n\n21.3.1.1 Leitura e observação dos dados\nSerá usado a função read_excel()do pacote readxl para ler os dados e a função str() para observar a sua estrutura:\n\ndados &lt;- readxl::read_excel (\"dados/dadosApendicite.xlsx\")\nstr(dados)\n\ntibble [156 × 3] (S3: tbl_df/tbl/data.frame)\n $ id        : num [1:156] 1 2 3 4 5 6 7 8 9 10 ...\n $ apendicite: num [1:156] 1 1 1 1 1 1 1 1 1 1 ...\n $ eco       : num [1:156] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAs variáveis apendicite e eco foram exibidas como variáveis numéricas e serão transformadas em fatores:\n\ndados$apendicite &lt;- factor(dados$apendicite,\n                           levels = c(1,2),\n                           labels = c(\"Presente\", \n                                      \"Ausente\"))\n\ndados$eco &lt;- factor(dados$eco,\n                    levels = c(1,2),\n                    labels = c(\"Positivo\", \n                               \"Negativo\"))\n\n\n\n21.3.1.2 Construção de uma tabela de contingência 2$$2\n\ntab_ap &lt;- with(dados, table(eco, apendicite, dnn = c (\"Eco\", \"Apendicite\")))\naddmargins(tab_ap, FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: Eco\n2: Apendicite\n\n\n          Apendicite\nEco        Presente Ausente sum\n  Positivo       85       7  92\n  Negativo       46      18  64\n  sum           131      25 156\n\n\n\n\n21.3.1.3 Avaliação do teste diagnóstico através da sensibilidade e da especificidade\nPode-se usar a função epi.tests() do pacote epiR (, Sergeant, et al. 2022) que calcula, junto com os intervalos de confiança exatos, a prevalência aparente e verdadeira, sensibilidade, especificidade, valores preditivos positivos e negativos e razões de probabilidade positivas e negativas a partir de dados de contagem fornecidos em uma tabela \\(2\\times2\\). Utiliza os argumentos\n\ndat dados sob a forma de vetor ou matriz\n\nconf.level magnitude do intervalode confiança, entre 0 e 1.\n\nOs resultados serão atribuídos a um objeto de nome diag:\n\ndiag &lt;- epiR::epi.tests(tab_ap, \n                        conf.level = 0.95)\nprint(diag)\n\n          Outcome +    Outcome -      Total\nTest +           85            7         92\nTest -           46           18         64\nTotal           131           25        156\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.59 (0.51, 0.67)\nTrue prevalence *                      0.84 (0.77, 0.89)\nSensitivity *                          0.65 (0.56, 0.73)\nSpecificity *                          0.72 (0.51, 0.88)\nPositive predictive value *            0.92 (0.85, 0.97)\nNegative predictive value *            0.28 (0.18, 0.41)\nPositive likelihood ratio              2.32 (1.22, 4.40)\nNegative likelihood ratio              0.49 (0.35, 0.68)\nFalse T+ proportion for true D- *      0.28 (0.12, 0.49)\nFalse T- proportion for true D+ *      0.35 (0.27, 0.44)\nFalse T+ proportion for T+ *           0.08 (0.03, 0.15)\nFalse T- proportion for T- *           0.72 (0.59, 0.82)\nCorrectly classified proportion *      0.66 (0.58, 0.73)\n--------------------------------------------------------------\n* Exact CIs\n\n\nAssim, a sensibilidade é igual a 65% (IC95%: 56 – 73%) e a especificidade é igual a 72% (IC95%: 51 – 88%). Isto significa que um indivíduo com apendicite aguda tem 65% de probabilidade de ter uma ecografia alterada; um indivíduo sem apendicite aguda tem 72% de probabilidade de ter uma ecografia normal. O objetivo do teste de diagnóstico é usá-lo para fazer um diagnóstico, então há necessidade de saber a probabilidade que o teste fornece para um diagnóstico correto. A sensibilidade e a especificidade não fornecem esta informação. Para atingir esse objetivo, usa-se o valor preditivo (Altman e Bland 1994a).",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#valor-preditivo",
    "href": "21-testesDiag.html#valor-preditivo",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.4 Valor Preditivo",
    "text": "21.4 Valor Preditivo\nO propósito de um teste diagnóstico é usar seus resultados para fazer um diagnóstico, portanto, é necessário conhecer a probabilidade de que o resultado do teste forneça o diagnóstico correto (Altman e Bland 1994a).\nOs valores preditivos positivo e negativo descrevem a probabilidade de um paciente ter doença, uma vez que os resultados de seus testes são conhecidos.\nO valor preditivo positivo (VPP) de um teste é definido como a proporção de pessoas com um resultado de teste positivo que realmente têm a doença.\nO valor preditivo negativo (VPN) é a proporção de pacientes com resultados de teste negativos que não têm doença.\nComo a sensibilidade e a especificidade, estas estatísticas de diagnóstico também podem ser calculadas a partir da tabela \\(2\\times2\\), mostrada no início:\n\\[\nVPP = \\frac {a}{\\left (a + b\\right )} \\quad \\quad VPN = \\frac {d}{\\left (c + d\\right )}\n\\]\nObservando os resultados anteriores da função epi.tests(), verifica-se que 92% (85/92) dos indivíduos que tiveram teste positivo (ultrassonografia alterada) tinham doença (apendicite aguda). Isso significa que seu VPP é igual a 92% (IC95%: 18 – 41%), ou dito de outra forma, uma pessoa com ultrassonografia positiva tem 92% de probabilidade de ter a apendicite aguda. O VPP é também conhecido como probabilidade pós-teste de doença dado um teste positivo.\nDos 64 pacientes que tiveram ultrassonografia sem alterações, 18 não apresentaram apendicite aguda, portanto, um VPN de 28% (IC95%: 56 – 73%). Isso significa que uma pessoa quem tem um teste negativo tem 28,1% de probabilidade de não ter apendicite aguda.\nEntretanto, essas proporções são de validade limitada. Os valores preditivos de um teste, na prática clínica, dependem criticamente da prevalência da anormalidade nos pacientes testados. No estudo, a prevalência de apendicite aguda é igual a\n\\[\n\\frac {total\\ de\\ casos\\ de \\ apendicite \\ aguda}{total\\ de\\ casos\\ no\\ estudo} = \\frac {131}{156} = 0,84\\ ou\\ 84\\% \\left(IC_{95\\%}:77\\ a\\ 89\\%\\right)\n\\]\nLevando-se em consideração que a prevalência de apendicite aguda na população é de 7% Pereira Lima et al. (2016), mantendo a sensibilidade (64%) e a especificidade (72%) da ultrassonografia, entre 156 pacientes, selecionados aleatoriamente, se esperaria encontrar aproximadamente 11 casos (7% de 156) de apendicite aguda. Para facilitar a compreensão, observe a a tabela \\(2\\times2\\) (Tabela 21.2):\n\n\n\n\nTabela 21.2: Prevalencia e valor preditivo\n\n\n\n\n\n\n\n\n\n\n\n\n\nO VPP e o VPN são iguais a:\n\na &lt;- 7\nb &lt;- 41\nc &lt;- 4\nd &lt;- 104\nvpp = a/(a + b)\nround(vpp, 3)*100\n\n[1] 14.6\n\nvpn = d/(c + d)\nround(vpn, 3)*100\n\n[1] 96.3\n\n\nAo se comparar o VPP obtido, agora, com o VPP do estudo, observa-se que o mesmo diminuiu bastante, de 92% para 14,6%. O contrário ocorre com a VPN que aumenta substancialmente de 28% para 96,3%, mostrando claramente a influência da prevalência.\nSe a prevalência diminui, o VPP diminui e o VPN aumenta. Portanto, será errado aplicar diretamente os valores preditivos publicados de um teste ao seu pacciente, quando a prevalência da doença em sua população for diferente da prevalência da doença na população em que o estudo publicado foi realizado. Um teste pode ser útil em um lugar e não ter validade em outro onde a prevalência é muito baixa.\nPode-se chegar aos mesmos resultados, usando as equações:\n\\[\nVPP =\\frac{sens \\times prev}{\\left(sens \\times prev\\right) + \\left [\\left (1- espec\\right) \\times \\left (1- prev\\right)\\right ]}\n\\]\n\\[\nVPN =\\frac{espec\\times \\left (1- prev\\right)}{\\left[\\left (1 - sens \\right)\\times prev\\right]+\\left[espec\\times \\left (1 - prev\\right)\\right]}\n\\]\nA prevalência pode ser interpretada como a probabilidade antes da realização do teste, conhecida como probabilidade pré-teste. A diferença entre as probabilidades pré e pós-teste é uma forma de avaliar a utilidade do teste. Esta diferença pode ser mensurada pela razão de probabilidade (likelihood ratio).",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#razão-de-probabilidade",
    "href": "21-testesDiag.html#razão-de-probabilidade",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.5 Razão de Probabilidade",
    "text": "21.5 Razão de Probabilidade\nA Razão de Probabilidades (likelihood ratio) é uma forma alternativa de descrever o desempenho de um teste diagnóstico. Alguns autores a denominam de razão de verossimilhança 1.\nA razão de probabilidades para um resultado de teste é definida como a razão entre a probabilidade de observar aquele resultado em indivíduos com a doença em questão e a probabilidade desse resultado em indivíduos sem a doença (Halkin et al. 1998).\nRazões de probabilidade são, clinicamente, mais úteis do que sensibilidade e especificidade. Fornecem um resumo de quantas vezes mais (ou menos) a probabilidade de os indivíduos com a doença apresentarem aquele resultado específico do que os indivíduos sem a doença, e também podem ser usados para calcular a probabilidade de doença para pacientes individuais (Deeks e Altman 2004). Cada vez mais as razões de probabilidade estão se tornando populares para relatar a utilidade dos testes de diagnóstico.\nQuando os resultados do teste são relatados como sendo positivos ou negativos, dois tipos de razões de probabilidades podem ser descritos, a razão de probabilidades para um teste positivo (denotada LR +) e a razão de probabilidades para um teste negativo (denotada LR−).\nA razão de probabilidades para um teste positivo é definida como a probabilidade de um indivíduo com doença ter um teste positivo dividida pela probabilidade de um indivíduo sem doença ter um teste positivo. A fórmula para calcular LR + é\nOu seja,\n\\[\nLR(+)=\\frac{sensibilidade}{1 - especificidade}\n\\]\nRazão de probabilidades positiva maior que 1 significa que um teste positivo tem mais probabilidade de ocorrer em pessoas com a doença do que em pessoas sem a doença. De um modo geral, para os indivíduos que apresentam um resultado positivo, LR (+) &gt; 10 aumenta significativamente a probabilidade de doença (“confirma” a doença), enquanto LR (+) &lt; 0,1, virtualmente, exclui a probabilidade de uma pessoa ter a doença (Guyatt, Rennie, et al. 2015).\nUsando os dados incluídos no objeto diag, obtido com a função epi.tests() do pacote epiR, tem-se que a LR (+) da ultrassonografia para o diagnóstico de apendicite aguda é igual 2.32 (IC95%: 1,22 – 4,40). Significa que uma pessoa com apendicite aguda tem cerca de 2,32 vezes mais probabilidade de ter um teste positivo do que uma pessoa que não tem a doença.\nA razão de probabilidade negativa é definida como a probabilidade de um indivíduo com doença ter um teste negativo dividido pela probabilidade de um indivíduo sem doença ter um teste negativo. A fórmula para calcular a LR− é:\nOu seja,\n\\[\nLR(-)=\\frac{sensibilidade}{1-especificidade}\n\\]\nRazão de probabilidade negativa menor que 1 significa que um teste negativo é menos provável de ocorrer em pessoas com a doença do que em pessoas sem a doença. Um LR muito baixo (abaixo de 0,1) praticamente exclui a chance de que uma pessoa tenha a doença (Guyatt, Rennie, et al. 2015).\nVoltando aos dados anteriores, a LR (-) para a ultrassonografia é igual a 0.49 (IC95%: 0.35 - 0.68). Significa que a probabilidade de ter um teste negativo para indivíduos com doença é 0,49 vezes ou cerca de metade daqueles sem a doença. Dito de outra forma, os indivíduos sem a doença têm cerca o dobro probabilidade de ter um teste negativo do que os indivíduos com a doença.\n\n21.5.1 Estimando a probabilidade de doença\nUma grande vantagem das razões de probabilidade é que elas podem ser usadas para ajudar o médico a adaptar a sensibilidade e a especificidade dos testes aos pacientes individuais. Ao se atender um paciente em uma clínica, pode-se decidir realizar um teste específico, após uma anamnese e um exame físico. A decisão de fazer o teste baseia-se nos sintomas e sinais do paciente e na experiência pessoal. Existe suspeita de um determinado diagnóstico e o objetivo é excluir ou confirmar esse diagnóstico. Antes de solicitar o teste, geralmente existe uma estimativa aproximada da probabilidade do paciente de ter essa doença, conhecida como probabilidade pré-teste ou a priori, que geralmente é estimada com base na experiência pessoal do médico, dados de prevalência local e publicações científicas.\nA razão mais importante pela qual um teste é realizado é tentar modificar a probabilidade de doença. Um teste positivo pode aumentar a probabilidade pós-teste e um teste negativo pode reduzir essa probabilidade. A probabilidade pós-teste de doença é o que mais interessa aos médicos e pacientes, pois isso pode ajudar a decidir se devem confirmar, descartar um diagnóstico ou realizar outros testes.\nOs resultados dos testes clínicos são geralmente usados não para fazer ou excluir categoricamente um diagnóstico, mas para modificar a probabilidade do pré-teste a fim de gerar a probabilidade do pós-teste. O teorema de Bayes é uma relação matemática que permite estimar a probabilidade pós-teste.\nPara se compreender este conceito, é importante entender a diferença entre probabilidade e odds (Oliveira Filho 2022).\nProbabilidade é a proporção de pessoas que apresentam uma determinada característica (teste positivo, sinal clínico).\nOdds (chance) representa a razão entre duas características complementares, ou seja, a probabilidade de um evento dividido pela probabilidade do não evento (1 – evento). Ambos contêm as mesmas informações de maneiras diferentes. Por exemplo, usando os dados da tabela tab_ap (veja Seção 21.3.1), verifica-se que a probabilidade (p) de uma ultrassonografia positiva para apendicite aguda é igual\n\na &lt;- tab_ap[1,1]\nb &lt;- tab_ap[1,2]\nc &lt;- tab_ap[2,1]\nd &lt;- tab_ap[2,2]\np &lt;- (a + b)/(a + b + c + d)\np\n\n[1] 0.5897436\n\n\ne que o odds da ultrassonografia positiva2 é\n\n odds &lt;- (a + b)/(c + d) \n odds\n\n[1] 1.4375\n\n\nPara transformar a odds em probabilidades e vice-versa, procede-se da seguinte maneira:\n\\[\np=\\frac{odds}{1+odds}\n\\]\nVoltando ao exemplo (Seção 21.3.1):\n\np = odds/(1 + odds)\np\n\n[1] 0.5897436\n\n\ne\n\\[\nodds=\\frac{p}{1-p}\n\\]\n\nodds = p/(1-p)\nodds\n\n[1] 1.4375\n\n\nPelo teorema de Bayes, sabendo-se a probabilidade a priori ou probabilidade pré-teste, é possível obter a probabilidade pós-teste ou a posteriori, usando a razão de probabilidades.\nPara atingir este objetivo, basta, inicialmente, multiplicar o odds pré-teste pela razão de probabilidades:\n\\[\nodds_{pos} = odds_{pre \\quad \\times \\quad LR}\n\\]\nApós, para encontrar a probabilidade pós-teste, basta converter o odds pós-teste em probabilidade:\n\\[\np_{pos} = \\frac{odds_{pos}}{1-odds_{pos}}\n\\]\nNo exemplo (Seção 21.3.1), foi verificado que o LR (+) é igual a 2,32 e a prevalência de apendicite aguda é em torno de 7% pode-se prever a probabilidade de haver apendicite aguda, diante de uma ultrassonografia alterada:\n\nprev &lt;-  0.07\nLR &lt;-  2.32\n\nodds_pre &lt;-  0.07/(1 -0.07)\n\nodds_pos &lt;- odds_pre * LR\n\np_pos &lt;- odds_pos/(odds_pos +1)\n\nround(p_pos, 3)\n\n[1] 0.149\n\n\nOu, em outras palavras, diante de um teste positivo, a probabilidade de o paciente ter apendicite aguda passa de 7% antes do teste para praticamente 15%!\nEstes cálculos podem ser simplificados, utilizando o nomograma de Fagan (Fagan 1975), extremamente fácil de se usar (Caraguel e Vanderstichel 2013), pois basta unir a probabilidade pré-teste ao LR que a reta apontará para a probabilidade pós-teste (Figura 21.1)).\n\n\n\n\n\n\n\n\nFigura 21.1: Nomograma de Fagan",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#sec-rocurve",
    "href": "21-testesDiag.html#sec-rocurve",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.6 Curva ROC",
    "text": "21.6 Curva ROC\nNem sempre o resultado de um teste é dicotômico (positivo/negativo). Com frequência, trabalha-se com variáveis contínuas (pressão arterial, glicemia, dosagem do sódio, dosagens hormonais, etc.). Neste caso, não há um resultado “positivo” ou “negativo”. Um “ponto de corte” precisa ser criado, para definir quem será considerado positivo ou negativo.\nA escolha do ponto de corte depende das consequências de um resultado falso positivo ou de um falso negativo. Falsos positivos estão associados com custos (emocional ou financeiro) e com a dificuldade de “desrotular” alguém que recebeu o rótulo de “positivo”. Resultados falsos negativos podem “tranquilizar” pessoas doentes que não são seguidas ou tratadas precocemente.\nA distribuição dos níveis glicêmicos em diabéticos e não diabéticos não tem um ponto de corte bem nítido. As duas populações se sobrepõem (Figura 21.2)), gerando falso positivos ou falso negativos, dependendo do ponto de corte escolhido (Oliveira Filho 2022).\n\n\n\n\n\n\n\n\nFigura 21.2: Populações de indivíduos normais (curva em azul) e diabéticos (curva em vermelho)\n\n\n\n\n\nSuponha que ao se examinar uma população fosse escolhido o ponto de corte de 80mg/dL, haveria um aumento no número de indivíduos com teste positivo com uma taxa de falsos positivos elevada, diminuindo a especificidade do teste. Se, por outro lado, o ponto de corte fosse elevado para 200mg/dL, o número de falsos negativos teria um grande aumento, reduzindo a sensibilidade. Esta oscilação entre a sensibilidade e a especificidade ocorre pelo fato de a localização do ponto de corte ser uma decisão arbitrária num contínuo entre o normal e anormal.\nAo se escolher um ponto de corte deve-se fazer um balanço entre a sensibilidade e a especificidade, levando em conta as consequências da escolha. Por exemplo, a triagem para fenilcetonúria em recém-nascidos valoriza a sensibilidade em vez de especificidade; o custo da perda de um caso é alto, pois existe tratamento eficaz. Uma desvantagem é que ocorre um grande número de testes falso positivos que causam angústia e a realização de mais testes.\nEm contraste, a triagem para o câncer de mama deve favorecer a especificidade sobre a sensibilidade, uma vez que uma avaliação mais aprofundada daquelas com teste positivo, implica em biopsias dispendiosas e invasivas.\nAs curvas ROC (Receiver Operating Characteristic) são uma ferramenta inestimável para encontrar o ponto de corte em uma medida com distribuição contínua que melhor prediz se uma condição está presente, por exemplo, se pacientes são positivos ou negativos para a presença de uma doença (Altman e Bland 1994b). As curvas ROC são usadas para encontrar um ponto de corte que separa um resultado de teste “normal” de um “anormal” quando o resultado do teste é uma medida contínua. As curvas ROC são traçadas calculando a sensibilidade e a especificidade do teste na predição do diagnóstico para cada valor da medida. A curva permite determinar um ponto de corte para a medição que maximiza a taxa de verdadeiros positivos (sensibilidade) e minimiza a taxa de falsos positivos (1 – especificidade) e, portanto, maximiza a razão de probabilidades (likelihood ratio).\n\n21.6.1 Dados do exemplo\nO conjunto de dados dadosTestes.xlsx contém informações para os resultados hipotéticos de três testes bioquímicos diferentes e uma variável (doença) que indica se foi confirmada a doença (padrão-ouro). Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho.\n\n21.6.1.1 Leitura e observação dos dados\nComo é um arquivo em Excel, a leitura será realizada pela função read_excel() do pacote readxl:\n\ntestes &lt;- readxl::read_excel(\"dados/dadosTestes.xlsx\")\nstr(testes)\n\ntibble [145 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:145] 1 2 3 4 5 6 7 8 9 10 ...\n $ teste1: num [1:145] 25 2.2 46.2 9.9 46.5 36.1 34.8 44.9 36.9 7.1 ...\n $ teste2: num [1:145] 25 2.2 15.6 20.4 15.7 35.7 34.8 55.4 36.9 7.1 ...\n $ teste3: num [1:145] 15 2.2 25 20.4 15.7 36.1 24 55.4 36.9 7.1 ...\n $ doenca: num [1:145] 2 2 1 1 2 2 2 1 2 2 ...\n\n\nA variável doença será transformada em fator:\n\ntestes$doenca &lt;- as.factor(testes$doenca)\n\nAs curvas ROC são usadas para avaliar qual teste é mais útil para prever quais pacientes serão positivos para a doença. A hipótese nula é que a área sob a curva ROC é igual a 0,5, ou seja, a habilidade do teste para identificar casos positivos e negativos é a esperada por acaso.\nA Figura 21.3 mostra a quantidade de sobreposição na distribuição da medição dos testes bioquímicos contínuos em ambos os grupos doença positiva e doença negativa. No Teste 1, a sobreposição é completa e não haverá um ponto de corte que separe efetivamente os dois grupos. Nos Testes 2 e 3, há uma maior separação das medidas de teste entre os grupos, particularmente para Teste 3.\n\n\n\n\n\n\n\n\nFigura 21.3: Resultado do teste vs doença\n\n\n\n\n\n\n\n\n21.6.2 Construção da curva ROC\nA validade dos testes, na distinção entre os grupos doença-positivo e doença-negativo, pode ser quantificada pelas curvas ROC, usando a função roc() do pacote pROC (Robin et al. 2011). Este pacote tem várias funções:\n\nauc: calcula a área da curva ROC;\nci: calcula o intervalo de confiança da curva ROC;\nci.auc: calcula o intervalo de confiança da AUC;\nci.se: calcula o intervalo de confiança de sensibilidades em determinadas especificidades;\nci.sp: calcula o intervalo de confiança de especificidades em determinadas sensibilidades;\nci.thresholds: calcula o intervalo de confiança dos limites;\ncoords: Retorna as coordenadas (sensibilidades, especificidades, pontos de corte) de uma curva ROC;\n\nroc: Constroi uma curva ROC;\n\nroc.test: Compara a AUC de duas curvas ROC correlacionadas;\n\nsmooth: suaviza a curva ROC\n\nUsar a função com os argumentos variável resposta (doenca), variável preditora (teste3, teste2 e teste1), indicação de que o gráfico deve ser desenhado (plot = TRUE). Como por padrão o gráfico é plotado com a sensibilidade no eixo x e a especificidade no eixo y; deve-se acrescentar o argumento legacy.axes = TRUE para aparecer o seu complemento, os falsos positivos (\\(1 – especificidade\\)).\nAlém desses, pode-se usar vários outros argumentos como: print.auc = TRUE, que imprime no gráfico a AUC e ci que é o intervalo de confiança da AUC. Para que a sensibilidade e especificidade apareçam como uma percentagem, deve-se usar o argumento percent = TRUE, pois o padrão é FALSE. Os demais argumentos são os rótulos dos eixos, cor da curva, largura da curva (lwd).\n\nroc3 &lt;- roc (testes$doenca,\n             testes$teste3, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             print.auc.y = 0.2,\n             ci = TRUE,\n             ylab=\"Sensibilidade\",\n             xlab=\"1 - Especificdade\",\n             col=\"steelblue\",\n             smooth = TRUE,\n             lwd=2) \n\nroc2 &lt;- roc (testes$doenca,\n             testes$teste2, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             ci = TRUE,\n             print.auc.y=0.13,\n             col=\"chartreuse4\",\n             lwd=2,\n             smooth = TRUE,\n             add=TRUE)\n\nroc1 &lt;- roc (testes$doenca,\n             testes$teste1, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             ci = TRUE,\n             print.auc.y=0.06,\n             col=\"tomato\",\n             lwd=2,\n             smooth = TRUE,\n             add=TRUE)\n\n# Legendas das curvas ROC\ntext (0.73,0.80,\"Teste 3\", col=\"steelblue\", cex = 1)\ntext (0.53,0.73,\"Teste 2\", col=\"chartreuse4\", cex = 1)\ntext (0.35,0.65,\"Teste 1\", col=\"tomato\", cex = 1) \n\n\n\n\n\n\n\nFigura 21.4: Curvas ROC para os Testes 1, 2 e 3\n\n\n\n\n\n\n21.6.2.1 Interpretação do resultado\nEm uma curva ROC, a sensibilidade é calculada usando cada valor do teste no conjunto de dados como um ponto de corte e é plotada em relação à (1 – especificidade) correspondente nesse ponto, como mostrado na Figura 21.4.\nAssim, a curva são os Verdadeiros Positivos (VP) plotados em relação aos Falsos Positivos (FP), calculados usando cada valor do teste como ponto de corte. A reta diagonal indica onde o teste cairia se os resultados não fossem melhores do que o acaso para predizer a presença de uma doença. O Teste 1 está próximo desta reta, confirmando que ele tem pouca capacidade de discriminar os pacientes doentes e não doentes.\nA área abaixo da reta diagonal é equivalente a 0,5 da área total. Quanto maior a área sob a curva ROC, mais útil é o teste para predizer os pacientes que têm a doença. Uma curva que cai substancialmente abaixo da linha diagonal indica que o teste tem pouca capacidade de diagnosticar a doença. Quando há uma separação perfeita dos valores dos dois grupos, isto é, sem sobreposição das distribuições, a área sob a curva ROC é igual a 1 (a curva ROC alcançará o canto superior esquerdo do gráfico).\nA área sob a curva (Area Under the Curve – AUC) e seu intervalo de confiança de 95% podem ser obtidos com os comandos usados na construção da Figura 21.4 ou separadamente usando as funções auc() e ci.auc() do pacote pROC.\n\nauc (roc1) \n\nArea under the curve: 0.5891\n\nci.auc (roc1)\n\n95% CI: 0.4935-0.6829 (2000 stratified bootstrap replicates)\n\nauc(roc2) \n\nArea under the curve: 0.7616\n\nci.auc(roc2)\n\n95% CI: 0.6759-0.8379 (2000 stratified bootstrap replicates)\n\nauc (roc3) \n\nArea under the curve: 0.898\n\nci.auc(roc3)\n\n95% CI: 0.836-0.9386 (2000 stratified bootstrap replicates)\n\n\nA acurácia geral de um teste pode ser descrita como a área sob a curva; quanto maior for a área, melhor será o teste. Na Figura 21.4, o Teste 3 tem uma AUC maior que os outros dois testes.\nUsa-se a seguinte estimativa (Tabela 21.3) para avaliar a acurácia de um teste ou da capacidade de identificar corretamente uma condição usando curva ROC (Borges 2016):\n\n\n\n\nTabela 21.3: Acurácia do teste diagnóstico\n\n\n\nAUCQualidade do Teste&gt;0,90excelente0,80 a 0,90muito bom0,70 a 0,80bom0,60 a 0,70suficiente0,50 a 0,60ruim&lt;0,50ignorar teste\n\n\n\n\n\nDesta forma, o Teste 3 pode ser considerado um bom teste e o Teste 1 é um teste ruim.\nComparando duas curvas\nPode-se comparar duas curvas ROC com a função roc.test(), por exemplo, comparando as curvas dos Teste 3 e 2 (DeLong, DeLong, e Clarke-Pearson 1988):\n\nroc.test(roc3, roc2)\n\n\n    Bootstrap test for two correlated ROC curves\n\ndata:  roc3 and roc2\nD = 4.8312, boot.n = 2000, boot.stratified = 1, p-value = 1.357e-06\nalternative hypothesis: true difference in AUC is not equal to 0\nsample estimates:\nSmoothed AUC of roc1 Smoothed AUC of roc2 \n           0.8980454            0.7616201 \n\n\nO Teste 3 tem uma AUC que o caracteriza como um bom teste e o teste de DeLong, entregue na saída do roc.test(), resultou que a diferença entre ele o Teste 2 é estatisticamente significativa (P &lt; 0,0001).\n\n\n21.6.2.2 Melhor ponto de corte\nO melhor ponto de corte (Best Critical Value), que às vezes é chamado de ponto de diagnóstico ótimo ou de Youden, é o ponto da curva mais próximo da parte superior do eixo y (Figura 21.4, Teste 3). Este é o ponto em que a taxa de verdadeiros positivos é otimizada e a taxa de falsos positivos é minimizada. O melhor ponto de corte para o Teste 3 é mostrado na Figura 21.5. Este melhor ponto de corte pode ser identificado a partir dos pontos de coordenadas da curva, usando a função roc() com os seguintes argumentos:\n\nbest &lt;- roc (testes$doenca, \n     testes$teste3,\n     plot = TRUE,\n     ci=TRUE,\n     thresholds=\"best\", \n     print.thres=\"best\",\n     legacy.axes=TRUE,\n     main=\"\",\n     ylab=\"Sensibilidade\",\n     xlab=\"1 - Especificidade\",\n     col=\"steelblue\",\n     lwd=2)\nbest\n\n\nCall:\nroc.default(response = testes$doenca, predictor = testes$teste3,     ci = TRUE, plot = TRUE, thresholds = \"best\", print.thres = \"best\",     legacy.axes = TRUE, main = \"\", ylab = \"Sensibilidade\", xlab = \"1 - Especificidade\",     col = \"steelblue\", lwd = 2)\n\nData: testes$teste3 in 48 controls (testes$doenca 1) &gt; 97 cases (testes$doenca 2).\nArea under the curve: 0.8973\n95% CI: 0.8444-0.9502 (DeLong)\n\n\n\n\n\n\n\n\nFigura 21.5: Melhor ponte de corte\n\n\n\n\n\nAssim, para o Teste 3, o ponto de corte ideal é 24,8, onde a especificidade é igual a 0,854 e a sensibilidade é igual 0,845. Estes dados, fornecem um LR para um resultado positivo igual a:\n\\[\nLR \\left(+\\right) = \\frac{0.845}{\\left (1-0.854\\right)} = 5,79\n\\]\nAs coordenadas da curva ROC podem ser obtida com a seguinte programação, a partir de uma sensibilidade e especificidade acima de 0 (zero):\n\ncoordenadas &lt;- testes %&gt;% roc(doenca, teste3) %&gt;% coords (transpose = F)\nhead(coordenadas, 10)\n\n   threshold specificity sensitivity\n1        Inf  0.00000000           1\n2      57.50  0.02083333           1\n3      54.65  0.04166667           1\n4      53.45  0.06250000           1\n5      52.80  0.08333333           1\n6      51.30  0.10416667           1\n7      49.65  0.12500000           1\n8      48.65  0.16666667           1\n9      47.50  0.18750000           1\n10     46.50  0.35416667           1\n\n\nA estatística J de Youden (Youden 1950) é calculada deduzindo 1 a partir da soma de sensibilidade e especificidade do teste e não é expressa como porcentagem, mas como parte de um número inteiro: \\(\\left (sensibilidade + especificidade\\right) - 1\\). A estatística J de Youden no melhor ponto de corte do Teste 3 é igual a \\(\\left (0,845+ 0,854\\right) - 1 = 0,699\\).\nEste é o maior valor de todos os valores das coordenadas (91 valores) usadas.\n\nyouden &lt;- max(coordenadas$sensitivity + coordenadas$specificity) - 1\nyouden\n\n[1] 0.6995275\n\n\nA Figura 21.5 mostra o ponto de corte ideal. Ele também pode ser obtido com a função coords() do pacote pRoc:\n\nroc3 &lt;- testes %&gt;% roc(doenca, teste3)\ncoords(roc3, x = \"best\", ret=\"threshold\", transpose = FALSE, \n       best.method=\"youden\")\n\n  threshold\n1      24.8\n\n\nO método para obter o melhor ponto de corte (best.method) pode ser pelo método de youden ou closest.topleft. No exemplo, o resultado é o mesmo. Para maiores detalhes consulte a ajuda da função coord(), do pacote pROC.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#estatística-kappa",
    "href": "21-testesDiag.html#estatística-kappa",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "21.7 Estatística kappa",
    "text": "21.7 Estatística kappa\nA estatística de concordância kappa (k) de Cohen é utilizada para descrever a concordância entre dois ou mais avaliadores quando realizam uma avaliação nominal ou ordinal de uma mesma amostra (Cohen 1960). A estatística kappa corrige a chance do acaso nas avaliações e é obtida pela fórmula igual a:\n\\[\nk= \\frac{p_{o} - p_{e}}{1 - p_{e}}\n\\]\nOnde \\(p_{o}\\) = proporção observada de concordância e \\(p_{e}\\) = proporção esperada de concordância apenas pelo acaso.\nPor exemplo, dois radiologistas podem revisar independentemente uma série de radiografias do tórax de pacientes para determinar a presença ou ausência de pneumonia. Para avaliar o grau de concordância entre as classificações dos dois médicos, pode ser relatado o percentual de concordância entre os avaliadores (por exemplo, 50% dos avaliadores responderam “sim” nas duas ocasiões). No entanto, esse percentual pode ser enganoso, pois não leva em conta o nível de concordância entre os dois avaliadores que pode ocorrer por acaso. A estatística kappa pode ser usada para avaliar a concordância das respostas para dois ou mais avaliadores após considerar a concordância casual. Portanto, a estatística kappa é uma estimativa da proporção de concordância entre avaliadores que excede a concordância que ocorreria por acaso.\nA interpretação dos valores de kappa é mostrada na Tabela 21.4. Quando a proporção observada de concordância é menor que a esperada por acaso, o kappa terá um valor negativo indicando não concordância. Um valor de kappa igual a 0 indica que a concordância observada é igual à concordância casual.\nO teste de hipóteses testa a hipótese de que a concordância entre os dois avaliadores seja puramente aleatória. Quando o valor P é menor que 0,05, rejeitamos a hipótese de que a concordância foi puramente aleatória. As premissas para o kappa de Cohen são que os participantes ou itens a serem classificados são independentes e também que os avaliadores e categorias são independentes.\n\n\n\n\nTabela 21.4: Valor Kappa e nível de concordância\n\n\n\nValor kappaConcordância&lt;0,00pobre0,00 - 0,20leve0,21 - 0,40razoável0,41 - 0,60moderada0,61 - 0,80substancial0,81 - 1,00quase perfeita\n\n\n\n\n\nExistem diferentes tipos de estatísticas kappa. Para dados com três ou mais categorias possíveis (por exemplo, concordo, concordo parcialmente, discordo) ou para dados categóricos ordenados, o kappa ponderado deve ser usado para que as respostas que estão mais distantes da concordância tenham maior peso do que aquelas próximas à concordância. No exemplo usado, as categorias possíveis são dicotômicas (sim e não), portanto, o kappa não ponderado (unweighted) e o ponderado (weighted) retornam o mesmo resultado.\n\n21.7.1 Dados do exemplo\n\n\n\n\n\n\nCenário\n\n\n\nCinquenta e quatro crianças com suspeita de pneumonia realizaram radiografias de tórax para confirmar o diagnóstico. Essas radiografias foram avaliadas por dois radiologistas.\n\n\nO arquivo dadosPneumonia.xlsx contém os dados dos diagnósticos independentes dos dois radiologistas. O objetivo foi medir a concordância diagnóstica dos dois profissionais. Para o cálculo do coeficiente kappa, será usada a função Kappa() do pacote vcd (Zeileis, Meyer, e Hornik 2007). Essa função tem os seguintes argumentos:\n\nx \\(\\longrightarrow\\) matriz ou tabela\nweights \\(\\longrightarrow\\) matriz especificada pelo usuário com as mesmas dimensões de x, desnecessário para kappa não ponderado.\n\nNa impressão do kappa pode-se usar print (k, digits = 3, CI = TRUE, level = 0.95). Onde k é o coeficiente de kappa, calculado pela função Kappa(), CI é o intervalo de confiança e o nível de confiança padrão é 95%.\n\n21.7.1.1 Leitura e exploração dos dados\nO conjunto de dados dadosPneumonia.xlsx pode ser obtido aqui. Após salvar o arquivo em seu diretório, ele pode ser carregado com a função read_excel() do pacote readxl:\n\ndados &lt;- readxl::read_excel(\"dados/dadosPneumonia.xlsx\")\n\n\n\n21.7.1.2 Construção da tabela\nO cálculo do kappa com a função Kappa() exige uma tabela, onde os dados dos dois radiologistas são cruzados. As variáveis a serem cruzadas são rx1 e rx2:\n\ndados$rx1 &lt;- factor(dados$rx1,\n                        ordered=TRUE,\n                        levels = c(\"sim\", \"não\"))                           \ndados$rx2 &lt;- factor(dados$rx2,\n                        ordered=TRUE,\n                        levels = c(\"sim\", \"não\"))\n\ntabk &lt;- with(dados, table(rx1, rx2, dnn = c (\"Radiologista 1\", \"Radiologista 2\")))\naddmargins(tabk, FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: Radiologista 1\n2: Radiologista 2\n\n\n              Radiologista 2\nRadiologista 1 sim não sum\n           sim  32   5  37\n           não   3  14  17\n           sum  35  19  54\n\n\n\n\n21.7.1.3 Cálculo do kappa\nO kappa é dado pela execução da função:\n\nk &lt;- vcd::Kappa(tabk)\nprint (k, \n       digits= 3, \n       CI=TRUE, \n       level=0.95)\n\n           value   ASE    z Pr(&gt;|z|) lower upper\nUnweighted 0.667 0.107 6.21 5.42e-10 0.456 0.878\nWeighted   0.667 0.107 6.21 5.42e-10 0.456 0.878\n\n\nA saída exibe o kappa pontual e os intervalos de confiança de 95%, podendo-se concluir, desses resultados, que existe uma boa confiabilidade nos diagnósticos dos radiologistas (k = 0,67, concordância substancial,de acordo com a Tabela 21.4).\n\n\n\n\n\n\nAltman, Douglas G, e J Martin Bland. 1994a. «Diagnostic tests 2: predictive values». Bmj 309 (6947). British Medical Journal Publishing Group: 102.\n\n\n———. 1994b. «Diagnostic tests 3: receiver operating characteristic plots.» BMJ: British Medical Journal 309 (6948). BMJ Publishing Group: 188.\n\n\n———. 1994c. «Diagnostic tests. 1: Sensitivity and specificity.» BMJ: British Medical Journal 308 (6943). BMJ Publishing Group: 1552.\n\n\nBorges, Leonardo Silva Roever. 2016. «Diagnostic accuracy measures in cardiovascular research». Int J Cardiovasc Sci 29 (3): 218–22.\n\n\nCaraguel, Charles GB, e Raphael Vanderstichel. 2013. «The two-step Fagan’s nomogram: ad hoc interpretation of a diagnostic test result without calculation». BMJ Evidence-Based Medicine 18 (4). Royal Society of Medicine: 125–28.\n\n\nCohen, Jacob. 1960. «A coefficient of agreement for nominal scales». Educational and Psychological Measurement 20 (1). Sage Publications: 37–46.\n\n\nDeeks, Jonathan J, e Douglas G Altman. 2004. «Diagnostic tests 4: likelihood ratios». Bmj 329 (7458). British Medical Journal Publishing Group: 168–69.\n\n\nDeLong, Elizabeth R, David M DeLong, e Daniel L Clarke-Pearson. 1988. «Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach». Biometrics 44. JSTOR: 837–45.\n\n\nFagan, TJ. 1975. «Nomogram for Bayes’s theorem». New England Journal of Medicine 293: 257.\n\n\nGuyatt, Gordon, Drummond Rennie, et al. 2015. «Diagnostic Tests». Em User’s Guides to Medical Literature: A Manual for Evidence-Based Clinical Practice, 3rd Edition, 607–31. New York: JAMA.\n\n\nHalkin, A, J Reichman, M Schwaber, O Paltiel, e M Brezis. 1998. «Likelihood ratios: getting diagnostic testing into perspective.» QJM: monthly journal of the Association of Physicians 91 (4): 247–58.\n\n\nMark, Evan Sergeant, et al. 2022. epiR: Tools for the Analysis of Epidemiological Data. https://CRAN.R-project.org/package=epiR.\n\n\nOliveira Filho, Petronio Fagundes de. 2022. «Testes Diagnósticos». Em Epidemiologia e Bioestatística: Fundamentos para a leitura crítica, Segunda Edição, 89–105. Rio de Janeiro: Editora Rubio.\n\n\nPeixoto, Rodrigo de Oliveira, Tarcizo Afonso Nunes, e Carlos Augusto Gomes. 2011. «Indices diagnósticos da ultrassonografia abdominal na apendicite aguda: influência do genero e constituição física, tempo evolutivo da doença e experiencia do radiologista». Revista do Colégio Brasileiro de Cirurgiões 38. SciELO Brasil: 105–11.\n\n\nPereira Lima, Amanda, Felipe José Vieira, Gabriela P.de Moraes Oliveira, et al. 2016. «Perfil clinico-epidemiologico da apendicite aguda: analise retrospectiva de 638 casos». Revista do Colegio Brasileiro de Cirurgiões 43. SciELO Brasil: 248–53.\n\n\nRobin, Xavier, Natacha Turck, Alexandre Hainard, et al. 2011. «pROC: an open-source package for R and S+ to analyze and compare ROC curves». BMC Bioinformatics 12 (1): 1–8.\n\n\nStraus, Sharon E, Paul Glasziou, et al. 2019. «Diagnosis and screening». Em Evidence-Based Medicine: How to Practice and Teach EBM, Fifth Edition, 185–218. Edinburgh: Elsevier.\n\n\nYouden, William J. 1950. «Index for rating diagnostic tests». Cancer 3 (1). Wiley Online Library: 32–35.\n\n\nZeileis, Achim, David Meyer, e Kurt Hornik. 2007. «Residual-based shadings for visualizing (conditional) independence». Journal of Computational and Graphical Statistics 16 (3). Taylor & Francis: 507–25.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "21-testesDiag.html#footnotes",
    "href": "21-testesDiag.html#footnotes",
    "title": "21  Determinação da validade e confiabilidade de testes diagnósticos",
    "section": "",
    "text": "Verossimilhança no sentido de a qualidade de algo que parece verdadeiro ou provável, que não contraria a verdade↩︎\nExistem duas maneiras de descrever uma estimativa de odds: ou como um número isolado, por exemplo, 0,25, subentendendo que expressa uma razão, 0,25:1,0, ou de forma clara como uma razão 1:4. Ou seja, para cada indivíduo com o fator existem quatro sem o fator. Tradicional e comumente usados no mundo das apostas em corridas de cavalos.↩︎",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Determinação da validade e confiabilidade de testes diagnósticos</span>"
    ]
  },
  {
    "objectID": "22-medidasOcorrencia.html",
    "href": "22-medidasOcorrencia.html",
    "title": "22  Medidas de ocorrência de doença",
    "section": "",
    "text": "22.1 Pacotes necessários neste capítulo\nBiocManager::install(\"limma\")\n\npacman::p_load(BiocManager,\n               dplyr,\n               epiR,\n               epitools,\n               flextable,\n               knitr,\n               limma,\n               pROC,\n               readxl,\n               vcd)",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Medidas de ocorrência de doença</span>"
    ]
  },
  {
    "objectID": "22-medidasOcorrencia.html#medidas-de-frequência",
    "href": "22-medidasOcorrencia.html#medidas-de-frequência",
    "title": "22  Medidas de ocorrência de doença",
    "section": "22.2 Medidas de frequência",
    "text": "22.2 Medidas de frequência\n\n22.2.1 Prevalência\nA prevalência, ou mais adequadamente, a prevalência pontual de uma doença é a proporção da população portadora da doença em um determinado ponto do tempo. É uma medida instantânea por excelência e fornece uma medida estática da frequência da doença. É também conhecida como taxa de prevalência e é expressa em percentagem ou por \\(10^{n}\\) habitantes. As medidas de prevalência geram informações úteis para o planejamento e administração de serviços de saúde.\nA prevalência por período descreve os casos que estavam presentes em qualquer momento durante um determinado período de tempo. Diz o número total de casos de uma doença que se sabe haver existido durante um período de tempo.\nUm tipo especial de prevalência de período é a prevalência ao longo da vida, que mede a frequência cumulativa ao longo da vida de um resultado até o momento presente (ou seja, a proporção de pessoas que tiveram o evento em qualquer momento no passado).\nAs doenças, quanto a sua duração, podem ser agudas e de longa duração ou crônicas. A prevalência é proporcional ao tempo de duração da doença. Hipoteticamente, se o surgimento de novos casos de doença ocorre em ritmo constante e igual para doenças agudas e crônicas, estas últimas acumularão casos, aumentando a prevalência. As doenças agudas tenderão a manter uma prevalência constante. A terapêutica, diminuindo o tempo de duração das doenças, também reduz a prevalência. A prevalência é dada pela razão:\n\\[\nprevalência = \\frac{número \\ de \\ casos \\ conhecidos \\ da \\ doença}{total \\ da \\ População} \\times 10^{n}\n\\]\n\n22.2.1.1 Exemplo\nComo exemplo, será verificada a frequência de tabagismo entre as puérperas da maternidade do HGCS. O banco de dados dadosMater.xlsx contém informação de 1368 nascimentos e pode ser consultado na Seção 5.6. Clique aqui para baixar e depois de salvar em seu diretório de trabalho, carregue-o com a função read_excel() do pacote readxl.\n\ndados &lt;- readxl::read_excel (\"dados/dadosMater.xlsx\")\n\nInicialmente, será verificado quantas fumantes existem. O conjunto de dados contém uma variável fumo, onde 1 = fumante e 2 = não fumante. Portanto, há necessidade de transformar a variável numérica em um fator:\n\ndados$fumo &lt;- factor (dados$fumo,\n                      ordered = TRUE, \n                      levels = c(1,2),\n                      labels = c(\"fumante\", \"não fumante\"))\n \ntabFumo &lt;- with(data = dados, table(fumo))\naddmargins(tabFumo, FUN = sum)\n\nfumo\n    fumante não fumante         sum \n        301        1067        1368 \n\n\nAlém de relatar a estimativa pontual da frequência da doença, é importante fornecer uma indicação da incerteza em torno dessa estimativa pontual. A função epi.conf(), do pacote epiR (, Sergeant, et al. 2022), permite calcular intervalos de confiança para prevalência, motivo da escolha dessa função.\nA função epi.conf() usa os seguintes argumentos:\n\ndat \\(\\longrightarrow\\) matriz ou tabela;\nctype \\(\\longrightarrow\\) tipo de intervalo de confiança a ser calculado. Opções: mean.single, mean.unpair, mean.pair, prop.single, prop.unpaired, prevalence, inc.risk, inc.rate, odds e smr (standardized mortality rate);\n\nmethod \\(\\longrightarrow\\) método a ser usado. Quando ctype = \"inc.risk\" ou ctype = \"prevalence\", as opções são exact, wilson e fleiss Quando ctype = \"inc.rate\" as opções são exact e byar;\nN \\(\\longrightarrow\\) tamanho da população;\nconf.level \\(\\longrightarrow\\) magnitude do intervalo de confiança retornado. Deve ser um único número entre 0 e 1.\n\nConstrução da matriz\nCom os dados da tabFumo, constrói-se uma matriz de duas colunas:\n\nn1 &lt;- tabFumo[1]\nN1 &lt;- tabFumo[1] + tabFumo[2]\nmat1 &lt;- as.matrix(cbind (n1, N1))\nmat1\n\n         n1   N1\nfumante 301 1368\n\n\nCálculo da prevalência\nUsando a função epiR(), tem-se:\n\nepiR::epi.conf(mat1, \n               ctype = \"prevalence\", \n               method = \"exact\", \n               conf.level = 0.95) \n\n        est     lower     upper\n1 0.2200292 0.1983313 0.2429365\n\n\nA saída mostra que a prevalência de fumantes entre as puérperas do HGCS é igual a 22,0% (IC95%: 19,8 – 24,3%).\n\n\n\n22.2.2 Incidência\nA incidência fornece uma medida da frequência com que os indivíduos suscetíveis se tornam casos de doenças, à medida que são observados ao longo do tempo.\nUm caso incidente ocorre quando um indivíduo deixa de ser suscetível e passa a ser doente. A contagem de casos incidentes é o número de tais eventos que ocorrem em uma população durante um período de acompanhamento definido. Existem duas maneiras de expressar a incidência:\nA incidência cumulativa (risco) é a proporção de indivíduos inicialmente suscetíveis em uma população que se tornam novos casos durante um período de acompanhamento definido.\nPara calcular a incidência cumulativa, é necessário primeiro identificar os doentes e após acompanhar por um determinado tempo os não doentes (Figura 22.1).\n\n\n\n\n\n\n\n\nFigura 22.1: Incidência\n\n\n\n\n\nA taxa de incidência (densidade de incidência ou taxa de incidência) é o número de novos casos da doença que ocorrem por unidade de tempo em risco durante um período de acompanhamento definido. Este período é expresso como pessoas-tempo (pessoas-ano, por exemplo).\nO conceito de pessoas-tempo pode ser ilustrado com o seguinte exemplo: a Figura 22.2 representa um estudo epidemiológico hipotético com duração de cinco anos, onde D é o desfecho e C representa os sujeitos que deixaram o estudo por migração ou morte (censurados) por causa não relacionada ao desfecho\n\n\n\n\n\n\n\n\nFigura 22.2: Pessoas-tempo (estudo epidemiológico hipotético)\n\n\n\n\n\nNesse estudo hipotético, o indivíduo 1 permaneceu no estudo 3,5 anos; o indivíduo 2, ficou 5 anos; o indivíduo 3, 4,5 anos e, assim por diante, totalizando 32,5 pessoas-anos. Em outras palavras, ocorreram 4 desfechos durante os 5 anos do estudo, consequentemente, a taxa de incidência (TI) foi de\n\\[\nTI = \\frac{4}{32,5} \\times 1000 = \\frac{123}{1000\\ pessoas-ano}\n\\]\nIsto significa que se fossem acompanhadas 1000 pessoas por um ano, 123 delas apresentariam o desfecho D.\n\n22.2.2.1 Exemplo\nAparentemente, pessoas cegas tem uma menor incidência de câncer e esse efeito parece ser mais pronunciado em pessoas totalmente cegas do que em pessoas com deficiência visual grave.\nPara testar essa hipótese, foi identificada uma coorte de 1.567 pessoas totalmente cegas e 13.292 sujeitos com deficiência visual grave. As informações sobre a incidência de câncer foram obtidas do Registro Sueco de Câncer (Feychting, Osterlund, e Ahlbom 1998). Foram diagnosticados de 136 casos de câncer em 22050 pessoas-ano em risco totalmente cegas e 1709 casos de câncer em 127650 pessoas-anos em risco com deficiência visual grave.\nA taxa de incidência pode ser calculada, usando-se a mesma função epi.conf(), usada para o cálculo da prevalência, mudando o argumento ctype = “prevalence” para ctype = “inc.rate”, conforme recomendado:\nPessoas totalmente cegas\nInicialmente, contrói-se a matriz:\n\nn2 &lt;- 136\nN2 &lt;- 22050\nmat2 &lt;- as.matrix(cbind (n2, N2))\nmat2\n\n      n2    N2\n[1,] 136 22050\n\n\nLogo, a incidência de câncer nos totalmente cegos é:\n\nepiR::epi.conf(mat2, \n               ctype = \"inc.rate\", \n               method = \"exact\", \n               conf.level = 0.95)*1000\n\n      est    lower    upper\nn2 6.1678 5.174806 7.295817\n\n\nPessoas com grave deficiência visual\nInicialmente, contrói-se a matriz:\n\nn3 &lt;- 1709\nN3 &lt;- 127650\nmat3 &lt;- as.matrix(cbind (n3, N3))\nmat3\n\n       n3     N3\n[1,] 1709 127650\n\n\nLogo, a incidência de câncer nos com grave deficiência visual é:\n\nepiR::epi.conf(mat3, \n               ctype = \"inc.rate\", \n               method = \"exact\", \n               conf.level = 0.95)*1000\n\n        est    lower    upper\nn3 13.38817 12.76088 14.03832\n\n\nAs saídas mostram que para cada 1000 pessoas cegas (a função foi multiplicada por 1000) acompanhadas por um ano, ocorreu 6,2 ((IC95%: 5,2 – 7,3) casos de câncer. Uma taxa de incidência, praticamente, metade da taxa de incidências das pessoas com deficiência visual grave. Os IC95% não são coincidentes, o que significa que essa diferença é significativa. Houve, na amostra, uma incidência menor de câncer entre os indivíduos totalmente cegos, sugerindo que a melatonina possa ser um fator protetor contra o câncer.\n\n\n22.2.2.2 Relação entre prevalência e incidência\nA incidência é uma medida de risco. A prevalência, por não levar em consideração o tempo de duração da doença (t), não tem esta capacidade. Em uma população onde a situação da doença encontra-se em estado estacionário (ou seja, sem grandes migrações ou mudanças ao longo do tempo na incidência/prevalência), a relação entre prevalência e incidência e duração da doença pode ser expressa pela seguinte fórmula (Szklo e Nieto 2019d):\n\\[\nprevalência \\ pontual = incidência \\times duração \\ da \\ doença \\ (t)\n\\]\nPor exemplo, se a incidência da doença for de 0,8% ao ano e sua duração média (sobrevida após o diagnóstico) for de 10 anos, a prevalência pontual será de aproximadamente 8%.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Medidas de ocorrência de doença</span>"
    ]
  },
  {
    "objectID": "22-medidasOcorrencia.html#medidas-de-associação",
    "href": "22-medidasOcorrencia.html#medidas-de-associação",
    "title": "22  Medidas de ocorrência de doença",
    "section": "22.3 Medidas de associação",
    "text": "22.3 Medidas de associação\n\n22.3.1 Odds Ratio\nOdds Ratio (OR) é a razão entre dois odds. A Odds Ratio, traduzida como Razão de Chances, está associada, usualmente, com estudos retrospectivos tipo caso-controle com desfechos dicotômicos.\nA odds ratio (OR) expressa a odds de exposição entre os que têm o desfecho (casos) pela odds de exposição nos livres de desfecho (controles).\n\n\n\n\nTabela 22.1: Tabela de contingência 2 x 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsando a Tabela 22.1, a fórmula \\(odds =\\frac{p}{1 -p}\\) e que\n\\[\np_{exp \\ doentes} = \\frac{a}{a+c}\n\\] \\[\np_{exp \\ não \\ doentes} = \\frac{b}{b+d}\n\\] tem-se:\n\\[\nodds_{exp} \\ {casos} = \\frac{\\frac{a}{a+c}}{1- \\frac{a}{a+c}}=\\frac{a}{c}\n\\]\n\\[\nodds_{exp} \\ {controles} = \\frac{\\frac{b}{b+d}}{1- \\frac{b}{b+d}}=\\frac{b}{d}\n\\]\nPortanto, a OR é igual a:\n\\[\nOR = \\frac{odds_{exp}\\ {casos}}{odds_{exp}\\ {controles}}=\\frac{\\frac{a}{c}}{\\frac{b}{d}}=\\frac{a \\times d}{c \\times b}\n\\]\nEm decorrência da última fórmula, a OR é definida como a razão dos produtos cruzados em uma tabela de contingência 2×2.\n\n22.3.1.1 Exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEm um estudo de caso-controle hipotético, a distribuição das exposições entre os casos e um grupo de pessoas saudáveis (“controles”) é comparada entre si. Os casos correspondem a um tipo raro de câncer, onde se suspeita que exista uma associação à exposição a um determinado fator de risco.\n\n\nOs dados desse estudo hipotético estão no arquivo dadosCasoControle.xlsx. O conjunto de dados pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl.\n\ncc &lt;- readxl::read_excel (\"dados/dadosCasoControle.xlsx\")\n\nAs variáveis cc$exposto e cc$desfecho devem ser transformadas em fatores e na ordem sim, não, uma vez que o R coloca em ordem alfabética (não, sim):\n\ncc$exposto &lt;- factor (cc$exposto,\n                      levels = c(\"sim\", \"não\"))\n\ncc$desfecho &lt;- factor (cc$desfecho,\n                       levels = c(\"sim\", \"não\"))\n\nApós essa etapa, construir uma tabela \\(2 \\times 2\\):\n\ntab_cc &lt;- table (cc$exposto, \n                 cc$desfecho, \n                 dnn = c(\"Exposição\", \"Desfecho\"))\naddmargins(tab_cc)         \n\n         Desfecho\nExposição sim não Sum\n      sim  48  20  68\n      não  12  40  52\n      Sum  60  60 120\n\n\nA OR será obtida utilizando a função epi.2by2() do pacote epiR (, Sergeant, et al. 2022). Esta função tem os seguintes argumentos:\n\ndat \\(\\longrightarrow\\) tabela de contingência \\(2 \\times 2\\);\nmethod \\(\\longrightarrow\\) as opções são “cohort.count”, “cohort.time”, “case.control” ou “cross.sectional”.;\nconf.level \\(\\longrightarrow\\) padrão = 0.95;\nunits \\(\\longrightarrow\\) multiplicador para incidência e prevalência;\noutcome \\(\\longrightarrow\\) indicação de como a variável desfecho é representada na tabela de contingência (“as.columns” ou “as.rows”).\n\n\nepiR::epi.2by2(tab_cc, \n               method = \"case.control\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n             Outcome+    Outcome-      Total                 Odds\nExposed +          48          20         68  2.40 (1.43 to 4.23)\nExposed -          12          40         52  0.30 (0.13 to 0.53)\nTotal              60          60        120  1.00 (0.69 to 1.45)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nExposure odds ratio                            8.00 (3.49, 18.34)\nAttrib fraction (est) in the exposed (%)      87.50 (71.51, 94.51)\nAttrib fraction (est) in the population (%)   70.00 (63.46, 81.10)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 26.606 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval \n\n\nA saída exibe os dados em uma tabela \\(2 \\times 2\\), mostrando as odds e os IC95% e outras estatísticas epidemiológicas relacionadas.\nA OR varia de zero ao infinito. Quando o valor da OR se aproxima de 1, a doença e o fator de risco não estão associados. Acima de 1 significa que existe associação e valores menores de 1 indicam uma associação negativa (efeito protetor).\nNo exemplo hipotético, os indivíduos que se expuseram ao fator de risco têm uma chance 8 vezes maior de apresentar este tipo de câncer. O valor p do qui-quadrado é altamente significativo (p &lt; 0,001).\n\n\n\n22.3.2 Risco Relativo\nO Risco relativo (RR) é a razão entre a incidência de desfecho em indivíduos expostos e a incidência de desfecho em indivíduos não expostos. O RR estima a magnitude da associação entre a exposição e o desfecho (doença). Em outras palavras, compara a probabilidade de ocorrência do desfecho entre os indivíduos expostos com a probabilidade de ocorrência do desfecho nos indivíduos não expostos.\nA partir da tabela de contingência \\(2 \\times 2\\) (Tabela 22.1), tem-se que o estimador do RR é dado por:\n\\[\nRR = \\frac{incidência_{exp}}{incidência_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}}\n\\]\n\n22.3.2.1 Exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEm 18 de abril de1940, ocorreu um surto de gastroenterite, após um jantar, em uma igreja (Figura 22.3), na vila de Lycoming, Condado de Oswego, Nova York. Das 80 pessoas que compareceram ao jantar, 46 das 75 entrevistadas desenvolveram posteriormente doença gastrointestinal aguda. Devido à sua natureza direta, o incidente tornou-se um estudo de caso de ensino padrão para gerações de epidemiologistas.\n\n\n\n\n\n\n\n\nFigura 22.3: Igreja de Oswego, 1940\n\n\n\n\n\n\n\nAs taxas de ataque (incidência) foram calculadas para aqueles que comeram e não comeram cada um dos 14 itens alimentares consumidos na ceia (Gross 1976). O pacote epitools (Aragon 2020) contém os dados desta investigação no arquivo oswego.\n\ndata(oswego)\nstr(oswego)\n\n'data.frame':   75 obs. of  21 variables:\n $ id                 : int  2 3 4 6 7 8 9 10 14 16 ...\n $ age                : int  52 65 59 63 70 40 15 33 10 32 ...\n $ sex                : chr  \"F\" \"M\" \"F\" \"F\" ...\n $ meal.time          : chr  \"8:00 PM\" \"6:30 PM\" \"6:30 PM\" \"7:30 PM\" ...\n $ ill                : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ onset.date         : chr  \"4/19\" \"4/19\" \"4/19\" \"4/18\" ...\n $ onset.time         : chr  \"12:30 AM\" \"12:30 AM\" \"12:30 AM\" \"10:30 PM\" ...\n $ baked.ham          : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ spinach            : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ mashed.potato      : chr  \"Y\" \"Y\" \"N\" \"N\" ...\n $ cabbage.salad      : chr  \"N\" \"Y\" \"N\" \"Y\" ...\n $ jello              : chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ rolls              : chr  \"Y\" \"N\" \"N\" \"N\" ...\n $ brown.bread        : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ milk               : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ coffee             : chr  \"Y\" \"Y\" \"Y\" \"N\" ...\n $ water              : chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ cakes              : chr  \"N\" \"N\" \"Y\" \"N\" ...\n $ vanilla.ice.cream  : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ chocolate.ice.cream: chr  \"N\" \"Y\" \"Y\" \"N\" ...\n $ fruit.salad        : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n\nExistem 75 observações de 21 variáveis, algumas características dos indivíduos como idade, sexo, etc. Importante para a análise é a variável ill (Y – sim, doente; N – não doente) e a variáveis relacionadas aos alimentos ingeridos durante o jantar na igreja. O sorvete de baunilha foi considerado o principal responsável pelo surto.\nA seguir, as variáveis oswego$vanilla.ice.cream e oswego$ill 1 serão transformadas em fator e os níveis colocados na ordem Y, N, uma vez que o R coloca em ordem alfabética (N, Y) :\n\noswego$ill &lt;- factor (oswego$ill,\n                      levels = c (\"Y\", \"N\"))\noswego$vanilla.ice.cream &lt;- factor (oswego$vanilla.ice.cream,\n                                    levels = c (\"Y\", \"N\"))\n\nRealizada essa etapa, será construída uma tabela para o cálculo do RR:\n\ntab_vanilla &lt;- table (oswego$vanilla.ice.cream, \n                      oswego$ill, \n                      dnn = c (\"Vanilla\", \"Ill\"))\ntab_vanilla            \n\n       Ill\nVanilla  Y  N\n      Y 43 11\n      N  3 18\n\n\nO RR será obtido, utilizando a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR, mudando a tabela para tab_vanilla e method = “cohort.count”:\n\nepiR::epi.2by2(tab_vanilla, \n               method = \"cohort.count\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n             Outcome+    Outcome-      Total                 Inc risk *\nExposure+          43          11         54     79.63 (66.47 to 89.37)\nExposure-           3          18         21      14.29 (3.05 to 36.34)\nTotal              46          29         75     61.33 (49.38 to 72.36)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 5.57 (1.94, 16.03)\nInc odds ratio                                 23.45 (5.84, 94.18)\nAttrib risk in the exposed *                   65.34 (46.92, 83.77)\nAttrib fraction in the exposed (%)            82.06 (55.87, 93.79)\nAttrib risk in the population *                47.05 (28.46, 65.63)\nAttrib fraction in the population (%)         76.71 (49.78, 93.83)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 27.223 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nOs resultados da saída indicam que os indivíduos que ingeriram sorvete de baunilha (n = 54) tiveram um risco maior de desenvolver gastrenterite aguda quando comparado aos que não ingeriram (n = 21). Dividindo o risco dos indivíduos expostos (incidência = 79,6) pelo risco dos não expostos (incidência = 14,3), encontra-se o RR = 5,57. Isso confirma que o sorvete de baunilha foi o principal responsável.\nQuanto maior o RR mais forte é a associação entre a doença em questão e a exposição ao fator de risco. Um RR = 1 indica que a doença e a exposição ao fator de risco não estão associadas. Valores &lt; 1 indicam uma associação negativa entre o fator de risco e a doença (efeito protetor).\n\n\n\n22.3.3 Odds Ratio vs Risco Relativo\nA OR não deve ser entendida como uma medida aproximada do RR, exceto para doenças raras (doenças, em geral com prevalência menor do que 10%). Caso contrário, a OR tenderá a superestimar a magnitude da associação e o OR afasta-se da hipótese nula da não associação (OR =1), independentemente de ser um fator de risco ou de proteção. A discrepância (d)2 entre as estimativas do RR e OR pode ser definido como a razão entre o OR e o RR estimados (Szklo e Nieto 2019a). Em outras palavras, a discrepância corresponde a uma proporção do RR (Davies, Crombie, e Tavakoli 1998).\n\\[\nd = \\frac {1- p_{não \\ exp}}{1- p_{exp}}= \\frac{\\frac{c}{c + d}}{\\frac{a}{a + b}}\n\\]\nLogo,\n\\[\nOR = RR \\times d\n\\]\nPara finalizar, uma comparação entre OR e RR é mostrada na Tabela 22.2.\n\n\n\n\nTabela 22.2: Força de associação do RR comparado com a OR.\n\n\n\nORRRMagnitude1,01,0insignificante1,51,2pequena3,51,9moderada9,03,0grande325,7muito grande36019quase perfeitainfinitoinfinitoperfeita\n\n\n\n\n\n\n\n22.3.4 Razão de Prevalência\nQuando dados transversais estão disponíveis, muitas vezes as associações são avaliadas, usando a razão de prevalência pontual (RPP).\nTendo o mesmo princípio das duas medidas anteriores, a razão de prevalência (RPP) compara a prevalência do desfecho entre os expostos com a prevalência do desfecho entre os não expostos.\nMatematicamente, a RPP é calculada de maneira semelhante ao RR. Apenas, deve-se ter em mente que o desfecho e a exposição foram medidos no mesmo momento, enquanto para o cálculo do RR há necessidade de calcular a incidência.\nTomando como base a estrutura da tabela de contingência 2 x 2 , Tabela 22.1, tem-se:\n\\[\nRPP = \\frac{prevalência \\ de \\ doença_{exp}}{prevalência \\ de \\ doença_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}}\n\\]\nTambém é possível verificar a prevalência de exposição entre doentes e não doentes:\n\\[\nRPP = \\frac{prevalência \\ de \\ exposição_{doentes}}{prevalência \\ de \\ exposição_{não \\ doentes}}=\\frac{\\frac{a}{a + c}}{\\frac{b}{b + d}}\n\\]\n\n22.3.4.1 Exemplo\n\n\n\n\n\n\nCenário\n\n\n\nEm um estudo transversal (Madi et al. 2010), foi verificada a prevalência de infecções congênitas entre as puérperas com idade igual ou acima de 20 anos comparadas às mulheres com menos de 20 anos (adolescentes). A hipótese foi de que as adolescentes tinham uma prevalência maior de infecções.\n\n\nParte dos dados estão no arquivo dadosMater.xlsx, que contém, como já mencionado, informações de 1368 nascimentos. Entre essas, tem-se a idade das mães (idadeMae) e se foi diagnosticada infecção congênita (infCong).\nO arquivo pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl.\n\ndados &lt;- readxl::read_excel (\"dados/dadosMater.xlsx\")\n\nA partir da variável idadeMae, criar a variável faixaEtaria, dividindo as parturientes em menores de 20 anos (adolescentes) e ≥ 20 anos. Para isso, usou-se a função cut() do pacote base. Revise os argumentos desta função.\n\ndados$faixaEtaria &lt;- cut (dados$idadeMae,\n                          breaks=c(13,20,46),\n                          labels = c(\"&lt;20a\",\"=&gt;20a\"),\n                          right = FALSE,\n                          include.lowest = TRUE)\n\nA variável ìnfCong encontra-se como uma variável numérica e deve ser transformada em fator:\n\ndados$infCong &lt;- factor (dados$infCong,\n                         ordered = TRUE, \n                         levels = c (1,2),\n                         labels = c (\"sim\", \"não\"))\n\nApós estes procedimentos, constroi-se uma tabela \\(2 \\times 2\\):\n\ntab_infCong &lt;- table(dados$faixaEtaria,\n                     dados$infCong,\n                     dnn = c(\"Faixa Etária\", \"Inf. Cong.\"))\naddmargins(tab_infCong)            \n\n            Inf. Cong.\nFaixa Etária  sim  não  Sum\n       &lt;20a     7  212  219\n       =&gt;20a  119 1030 1149\n       Sum    126 1242 1368\n\n\nCálculo da RPP\nUsando a tabela tab_infCong com a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR e RR, e mudando a tabela para tab_infCong e method = “cross.sectional”, obtem-se:\n\nepiR::epi.2by2(tab_infCong, \n               method = \"cross.sectional\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n          Outcome+ Outcome- Total               Prev risk *\nExposure+        7      212   219       3.20 (1.29 to 6.47)\nExposure-      119     1030  1149     10.36 (8.65 to 12.26)\nTotal          126     1242  1368      9.21 (7.73 to 10.87)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nPrev risk ratio                                0.31 (0.15, 0.65)\nPrev odds ratio                                0.29 (0.13, 0.62)\nAttrib prev in the exposed *                   -7.16 (-10.08, -4.24)\nAttrib fraction in the exposed (%)            -224.02 (-577.43, -57.46)\nAttrib prev in the population *                -1.15 (-3.48, 1.19)\nAttrib fraction in the population (%)         -12.45 (-12.85, -11.96)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 11.278 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nA saída exibe várias informações. Foi feita a hipótese de uma maior prevalência entre as mulheres com menos de 20 anos. Por este motivo, elas aparecem como as expostas (Exposed +) e tem uma prevalência de 3,20/100, enquanto as mulheres com mais de 20 anos tiveram uma prevalência de 10,36/100. Isto mostra que a razão de prevalência é igual a 0,31 (IC95%: 0,15-0,65)3, ou seja, abaixo de 1, sugerindo que ao contrário da hipótese inicial, as adolescentes têm, neste estudo, uma menor prevalência de infecções congênitas.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Medidas de ocorrência de doença</span>"
    ]
  },
  {
    "objectID": "22-medidasOcorrencia.html#medidas-de-impacto",
    "href": "22-medidasOcorrencia.html#medidas-de-impacto",
    "title": "22  Medidas de ocorrência de doença",
    "section": "22.4 Medidas de impacto",
    "text": "22.4 Medidas de impacto\n\n22.4.1 Risco Atribuível\nO Risco Atribuível (RA) possui características de medida de impacto. O RA, ao invés de concentrar-se na associação em si, refere-se mais às consequências e às repercussões da exposição sobre a ocorrência do desfecho.\nO RA é a medida do excesso ou acréscimo absoluto de risco que pode ser atribuído à exposição (Szklo e Nieto 2019b). Com o RA é possível estimar o número de casos que podem ser prevenidos se a exposição for eliminada e assim estimar a magnitude do impacto, em termos de saúde pública, imposto por esta exposição.\nO risco de desenvolver o desfecho (incidência) está aumentado em RA nos indivíduos expostos em comparação com os que não estão expostos. Nos estudos de coorte, costuma-se usar mais a expressão Risco Atribuível ou Diferença de Risco. Nos ensaios clínicos, usa-se mais a expressão Redução Absoluta do Risco (RAR), pois se espera que a intervenção reduza o risco.\nCalcula-se o RA ou a RAR pela diferença absoluta entre as incidências dos expostos e não expostos:\n\\[\nRA = \\left|I_{expostos} - I_{não \\ expostos}\\right|\n\\]\nUtilizando a tabela de contingência \\(2 \\times 2\\) (Tabela 22.1), o RA fica expresso da seguinte maneira:\n\\[\nRA = \\left|\\frac{a}{a + b} -\\frac{c}{c + d}\\right|\n\\]\nNo exemplo do Risco Relativo, o RA pode ser calculado usando a mesma tabela de contingência, repetida aqui para facilitar a leitura (Tabela 22.3):\n\n\n\n\nTabela 22.3: Taxa de ataque de gastrenterite com sorvete de baunilha - Oswego\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogo,\n\\[\nRA = \\left|\\frac{43}{43 + 11} -\\frac{3}{3 + 18}\\right| = \\left|0,796 - 0,143\\right| = 0,653\n\\]\nO risco atribuível na exposição mede o excesso de risco associado a uma determinada categoria de exposição. Por exemplo, com base no exemplo, a incidência cumulativa de gastrenterite aguda entre os indivíduos que comeram o sorvete de baunilha é de 79,6% e para os que não ingeriram o sorvete (categoria de referência ou não exposta) foi de 14,3%. Desta forma, o risco excessivo associado à exposição 79,6 – 14,3 = 65,3%. Ou seja, assumindo uma associação causal (sem confusão ou viés), a não ocorrência da festa diminuiria o risco no grupo exposto de 79,6% para 14,3%.\nO RA expresso em relação à incidência nos expostos e apresentado em percentual é denominado de Risco Atribuível Proporcional (RAP) ou Fração Atribuível nos Expostos.\nO RAP informa qual a proporção de desfecho, expresso em percentagem, entre os expostos que poderia ter sido prevenida se a exposição fosse eliminada. É dado pela fórmula:\n\\[\nRAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100\n\\]\nNo exemplo do surto de gastrenterite aguda no jantar da igreja de Oswego (Seção 22.3.2), tem-se:\n\\[\nRAP = \\left(\\frac{0,796 - 0,143}{0,796}\\right) \\times 100 = 82,06 \\%\n\\]\nSe a causalidade foi estabelecida, essa medida pode ser interpretada como a porcentagem do risco total de gastrenterite aguda que é atribuível à ingesta de sorvete de baunilha.\nOutra maneira de se chegar a este mesmo resultado é através do RR, usando a seguinte fórmula\n\\[\nRAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(\\frac{I_{expostos}}{I_{expostos}} - \\frac{I_{não \\ expostos }}{I_{expostos}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(1 - \\frac{1}{\\frac{I_{expostos }}{I_{não \\ expostos}}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(1 - \\frac{1}{RR}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(\\frac{RR - 1}{RR}\\right) \\times 100\n\\]\nNo exemplo, o RR é igual a 5,57, logo:\n\\[\nRAP = \\left(\\frac{5,57 - 1}{5,57}\\right) \\times 100 = 82,05\\%\n\\]\n\n\n22.4.2 Redução Relativa do Risco\nQuando se avalia um tratamento ou alguma intervenção em que se suponha haver uma redução do risco — por exemplo, o uso da aspirina para reduzir a ocorrência de infarto agudo do miocárdio —, o termo Risco Atribuível é substituído por Redução do Risco Atribuível e é calculado da mesma forma apresentada na equação do Risco Atribuível.\nNeste caso, ao invés de usar o Risco Atribuível Proporcional (RAP), onde se pressupõe que a exposição é um fator de risco para a doença e o RR \\(&lt;\\) 1, usa-se a Redução Relativa do Risco, pois a exposição é supostamente um fator protetor, como se espera que ocorra nos ensaios clínicos.\nEsta medida, análoga ao RAP, é também chamada de Eficácia, definida como a proporção da incidência nos indivíduos não tratados (por exemplo, o grupo controle) que é reduzida pela intervenção (Szklo e Nieto 2019c).\nO cálculo da Redução Relativa do Risco (RRR) é semelhante ao Risco Atribuível Proporcional (RAP), onde a incidência nos expostos é a incidência no grupo que recebeu a intervenção (ou taxa de eventos no grupo tratamento) e a incidência nos não expostos é incidência nos controles (ou taxa de eventos nos controles – TEC). Como se supõe que a incidência nos controles seja maior que a incidência no grupo de tratamento, a equação fica:\n\\[\nRRR = \\left(\\frac{I_{controle} - I_{tratamento}}{I_{controle}}\\right) \\times 100\n\\]\nAlternativamente, a RRR pode ser estimada pela equação:\n\\[\nRRR = \\left(1 - RR\\right) \\times 100\n\\]\nO Physicians’ Health Study (Physicians’ Health Study Research Group* 1989) é um ensaio clinico randomizado controlado, duplo cego, desenhado com o objetivo de determinar se uma dose baixa de aspirina (325 mg a cada 48 horas) diminui a mortalidade cardiovascular e se o betacaroteno reduz a incidência de câncer. Participaram deste estudo 22071 indivíduos por uma média de 60,2 meses.\nO estudo do componente aspirina mostrou os seguintes resultados (Tabela 22.4):\n\n\n\n\nTabela 22.4: Physicians’ Health Study, componente aspirina e IAM\n\n\n\n\n\n\n\n\n\n\n\n\n\nA incidência cumulativa de Infarto Agudo de Miocárdio (IAM) em ambos os grupos foi:\n\\[\nIncidencia_{aspirina} = \\frac{139}{11037} = 0,0126\n\\]\n\\[\nIncidencia_{placebo} = \\frac{239}{11034} = 0,0217\n\\]\n\\[\nRR = \\frac{0,0126}{0,0217} = 0,58\n\\]\nLogo, a RRR é igual a:\n\\[\nRRR = \\left(1 - 0,58\\right) \\times 100 = 42\\%\n\\]\nOu seja, houve uma redução de 42% no risco de IAM no grupo que usou aspirina e a conclusão dos autores foi que este ensaio clínico demonstrou, em relação à prevenção primária de doença cardiovascular, uma diminuição no risco de IAM.\nEstes cálculos podem ser realizados com a função risks() do pacote MKmisc (Kohl 2019). Esta função calcula o risco relativo (RR), odds ratio (OR), redução relativa do risco (RRR) e outras estatísticas epidemiológicas, como RAR, NNT.\nA função risks() usa como argumento:\n\np0 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo não exposto;\np1 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo exposto.\n\nAlém disso, para o seu funcionamento, deve-se ter instalado o pacote BiocManager para poder instalar o pacote limma, necessário para a execução do pacote MKmisc. Veja início do capítulo em pacotes usados neste capítulo.\nA função risks() será usada dentro da função round() para reduzir o número de dígitos decimais:\n\np0 &lt;- 0.0217\np1 &lt;- 0.0126\nround(MKmisc::risks(p0,p1), 4)\n\n      p0       p1       RR       OR      RRR      ARR      NNT \n  0.0217   0.0126   0.5806   0.5753   0.4194   0.0091 109.8901 \n\n\n\n\n22.4.3 Número Necessário para Tratar\nOs resultados da função risks() entrega junto o Número Necessário para Tratar (NNT) que deve ser arredondado para o número inteiro mais próximo (no caso, 110) e significa a estimativa do número de indivíduos que devem receber uma intervenção terapêutica, durante um período específico de tempo, para evitar um efeito adverso ou produzir um desfecho positivo.\nO NNT equivale à recíproca do RAR (Redução Absoluta do Risco ou Diferença de Risco):\n\\[\nNNT = \\frac{1}{RAR} = \\frac{1}{I_{não \\ expostos} - I_{expostos}}\n\\]\nNo exemplo do Physicians’ Health Study Physicians’ Health Study Research Group* (1989), o RAR igual a:\n\\[\nRA = \\left|I_{expostos} - I_{não \\ expostos}\\right| = \\left|0,0126 - 0,0217\\right| = 0,0091\n\\]\n\\[\nNNT = \\frac{1}{0,0091} = 109,89 \\simeq 110\n\\]\nPode-se calcular os IC95%, calculando o NNT para os limites do RAR usando a seguinte equação (Bender 2001):\n\\[\nIC_{95\\%} \\longrightarrow RAR \\pm z_{\\left({1 - \\frac{\\alpha}{2}}\\right)} \\times EP_{RAR}\n\\] Onde,\n\\[\nEP_{RAR} = \\sqrt{\\frac{p0\\left(1 - p0\\right)}{n_{1}}+\\frac{p1\\left(1 - p1\\right)}{n_{2}}}\n\\]\nUsando esses dados, pode-se criar um script no RStudio para os cálculos:\nVetor dos dados\n\na &lt;- 139\nb &lt;- 10898\nc &lt;- 239\nd &lt;- 10795\ndados &lt;- c (a, b, c, d)\n\nMatriz dos dados4\n\nmat_iam &lt;- matrix (dados, byrow = TRUE, nrow = 2)\ntratamento &lt;- c (\"aspirina\", \"placebo\")\ndesfecho &lt;- c (\"IAM\", \"s/IAM\")\nrownames (mat_iam) &lt;- tratamento\ncolnames (mat_iam) &lt;- desfecho\nmat_iam\n\n         IAM s/IAM\naspirina 139 10898\nplacebo  239 10795\n\n\nCálculo das incidências no grupo tratamento e no grupo placebo\nNa matriz o que está entre colchetes [1,1] significa: linha 1 e coluna 1, ou seja, o valor 139.\n\nn1 &lt;-mat_iam [1,1] + mat_iam [1,2]\nn1\n\n[1] 11037\n\np1 &lt;- mat_iam [1,1] / n1\nround (p1, 4)\n\n[1] 0.0126\n\nn0 &lt;- mat_iam [2,1] + mat_iam [2,2]\nn0\n\n[1] 11034\n\np0 &lt;- mat_iam [2,1] / n0\nround (p0, 4)\n\n[1] 0.0217\n\n\nOs resultados da matriz de dados e o cálculo das incidências p0 (incidência no grupo placebo) e p1 (incidência no grupo de tratamento) já eram conhecidos e foram repetidos apenas para entrar na programação do cálculo do IC95%.\nCálculo do erro padrão da RAR\n\nRAR &lt;- abs(p0 - p1)\nNNT &lt;- 1/RAR\n\nalpha &lt;- 0.05\nz &lt;- qnorm (1 - (alpha/2))\nround (z, 3)\n\n[1] 1.96\n\nEP_RAR &lt;- sqrt((((p0*(1-p0)) / n0)) + (((p1*(1-p1)) / n1)))\n\n# Limite inferior\nli_RAR &lt;- RAR - (z * EP_RAR)\nround (li_RAR, 4)\n\n[1] 0.0056\n\n# Limite superior\nls_RAR &lt;- RAR + (z * EP_RAR)\nround (ls_RAR, 4)\n\n[1] 0.0125\n\nround(print(c(li_RAR, RAR, ls_RAR), 4))\n\n[1] 0.005645 0.009066 0.012488\n\n\n[1] 0 0 0\n\n\nPortando, ao Redução Absoluta do Risco foi igual a 0,0091 (IC95%: 0,0056-0,0125). A partir destes resultados, pode-se calcular o intervalo de confiança para o NNT:\n\nli_NNT &lt;- 1/ls_RAR\nls_NNT &lt;- 1/li_RAR\n\nli_NNT \n\n[1] 80.07881\n\nls_NNT\n\n[1] 177.1497\n\n\nConcluindo, o uso da aspirina no Physicians’ Health Study reduziu o risco de infarto agudo do miocárdio em 42% (RRR), ou seja, foi eficaz. Por outro lado, para ter este impacto será necessário tratar 110 (IC95%: 80-177) pacientes para que um tenha benefício. Este NNT é grande; o ideal é um NNT &lt; 10. Apesar disso, como a aspirina tem baixo custo e seus benefícios suplantam os efeitos adversos, seu uso pode estar justificado.\n\n\n22.4.4 Número Necessário para Causar Dano\nDeve-se comparar o NNT com o Número Necessário para causar Dano (NND), em inglês, Number Needed to Harm (NNH). Deve ser interpretado como o número de pacientes tratados para que um deles apresente um efeito adverso.\nO NND é calculado pela recíproca do aumento absoluto do risco (ARA), equivalente a diferença de risco ou redução absoluta do risco:\n\\[\nNND = \\frac{1}{ARA} = \\frac{1}{I_{expostos} - I_{não \\ expostos}}\n\\]\n\n22.4.4.1 Exemplo\nNo Physicians’ Health Study (Physicians’ Health Study Research Group* 1989) sobre o uso de aspirina na prevenção de IAM, foi verificado também os efeitos colaterais da aspirina, como acidentes vasculares cerebrais (AVC), Tabela 22.5.\n\n\n\n\nTabela 22.5: Physicians’ Health Study, componente aspirina e AVC.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCálculo das incidências\n\np0 &lt;- 98/11034\nround(p0, 4)\n\n[1] 0.0089\n\np1 &lt;- 119/11037\nround(p1, 4)\n\n[1] 0.0108\n\n\nPara o cálculo do NND, usa-se a função risk(), como mencionado antes:\n\np0 &lt;- 0.0089\np1 &lt;- 0.0108\nround (MKmisc::risks (p0, p1), 4)\n\n      p0       p1       RR       OR      RRI      ARI      NNH \n  0.0089   0.0108   1.2135   1.2158   0.2135   0.0019 526.3158 \n\n\nOs resultados mostram que o NND5 é igual a 526. Ou seja, para evitar um IAM há necessidade de tratar 110 pacientes e a cada 526 tratados espera-se um caso de AVC, havendo um benefício bem maior quando comparado ao risco de AVC.\n\n\n\n\n\n\nAragon, Tomas J. 2020. «epitools: Epidemiology Tools». https://CRAN.R-project.org/package=epitools.\n\n\nBender, Ralf. 2001. «Calculating confidence intervals for the number needed to treat». Controlled clinical trials 22 (2). Elsevier: 102–10.\n\n\nDavies, Huw Talfryn Oakley, Iain Kinloch Crombie, e Manouche Tavakoli. 1998. «When can odds ratios mislead?» BMJ 316 (7136). British Medical Journal Publishing Group: 989–91.\n\n\nFeychting, Maria, Bill Osterlund, e Anders Ahlbom. 1998. «Reduced cancer incidence among the blind». Epidemiology 9 (5). LWW: 490–94.\n\n\nGross, Michael. 1976. «Oswego County revisited.» Public health reports 91 (2). SAGE Publications: 168.\n\n\nKohl, Matthias. 2019. «Package “MKmisc”». https://github.com/stamats/MKmisc.\n\n\nMadi, José Mauro, Ricardo da Silva de Souza, Breno Fauth de Araujo, Petrônio Fagundes Oliveira Filho, et al. 2010. «Prevalence of toxoplasmosis, HIV, syphilis and rubella in a population of puerperal women using Whatman 903 filter paper». The Brazilian Journal of Infectious Diseases 14 (1). Elsevier: 24–29.\n\n\nMark, Evan Sergeant, et al. 2022. epiR: Tools for the Analysis of Epidemiological Data. https://CRAN.R-project.org/package=epiR.\n\n\nPhysicians’ Health Study Research Group*, Steering Committee of the. 1989. «Final report on the aspirin component of the ongoing Physicians’ Health Study». New England Journal of Medicine 321 (3). Mass Medical Soc: 129–35.\n\n\nSzklo, Moyses, e F Javier Nieto. 2019a. «Measuring Associations Between Exposures and Outcomes». Em Epidemiology: beyond the basics, Fourth Edition, 88–94. Burlington, MA: Jones & Bartlett Learning.\n\n\n———. 2019b. «Measuring Associations Between Exposures and Outcomes». Em Epidemiology: beyond the basics, Fourth Edition, 84–102. Burlington, MA: Jones & Bartlett Learning.\n\n\n———. 2019c. «Measuring Associations Between Exposures and Outcomes». Em Epidemiology: beyond the basics, Fourth Edition, 97–98. Burlington, MA: Jones & Bartlett Learning.\n\n\n———. 2019d. «Measuring Disease Occurrence». Em Epidemiology: beyond the basics, Fourth Edition, 80–81. Burlington, MA: Jones & Bartlett Learning.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Medidas de ocorrência de doença</span>"
    ]
  },
  {
    "objectID": "22-medidasOcorrencia.html#footnotes",
    "href": "22-medidasOcorrencia.html#footnotes",
    "title": "22  Medidas de ocorrência de doença",
    "section": "",
    "text": "Foi mantido o nome das variáveis em inglês, pois no banco de dados oswego elas estão nessa língua.↩︎\nem inglês, built-in bias↩︎\nObservem que todo o intervalo de confiança de 95% encontra-se abaixo de 1, indicando que existe significância estatística.↩︎\nAproveite para revisar como construir matriz↩︎\nEm inglês, NNH (number needed to harm).↩︎",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Medidas de ocorrência de doença</span>"
    ]
  },
  {
    "objectID": "23-sobrevida.html",
    "href": "23-sobrevida.html",
    "title": "23  Análise de Sobrevida",
    "section": "",
    "text": "23.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr,\n               flextable,\n               ggplot2,\n               knitr,\n               readxl,\n               survival, \n               survminer)",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Análise de Sobrevida</span>"
    ]
  },
  {
    "objectID": "23-sobrevida.html#introdução",
    "href": "23-sobrevida.html#introdução",
    "title": "23  Análise de Sobrevida",
    "section": "23.2 Introdução",
    "text": "23.2 Introdução\nA análise de sobrevida é utilizada quando se pretende investigar o tempo entre o início de um estudo e a ocorrência subsequente de um evento que modifica o estado de saúde do indivíduo. É bastante usada em estudos sobre câncer, por exemplo, analisando o tempo desde a cirurgia até a morte, o tempo desde o início do tratamento até a progressão da doença, o tempo desde a resposta até a recorrência da doença. Ela também é usada para medir a ocorrência de outros eventos como o tempo desde a infecção pelo vírus da imunodeficiência humana (HIV) até o desenvolvimento da Síndrome de Imunodeficiência Adquirida (SIDA), o tempo de hospitalização, tempo de amamentação, etc.\nO interesse está centrado na verificação do efeito dos fatores de risco ou de prognóstico sobre o tempo de sobrevida de um indivíduo ou de um grupo, bem como definir as probabilidades de sobrevida em diversos momentos no seguimento do grupo. Considera-se tempo de sobrevida, ou simplesmente sobrevida, o tempo a entre a entrada do indivíduo no estudo e a ocorrência do evento de interesse. Com relação aos dados relacionados ao tempo, podem ocorrer problemas. O tempo para um evento geralmente não tem distribuição normal. Além disso, nem sempre se pode esperar até que o evento ocorra em todos os pacientes e alguns pacientes abandonam o estudo mais cedo. Todos devem ser considerados e as análises de sobrevida contornam esses problemas.\nEm estudos de sobrevida, os indivíduos são observados até a ocorrência de um evento final que, geralmente, corresponde à morte, ou à variação de um parâmetro biológico ou outro evento que indique a modificação do estado inicial (cura, recorrência, retorno ao trabalho, etc.) O evento final é denominado de falha, por referir-se, em geral, a algo indesejável.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Análise de Sobrevida</span>"
    ]
  },
  {
    "objectID": "23-sobrevida.html#dados-censurados",
    "href": "23-sobrevida.html#dados-censurados",
    "title": "23  Análise de Sobrevida",
    "section": "23.3 Dados Censurados",
    "text": "23.3 Dados Censurados\nQuando, em um estudo de sobrevida, os pacientes que saem do estudo ou que não vivenciam o evento são chamados de observações censuradas.\nEsses tempos de sobrevida censurados subestimam o verdadeiro (mas desconhecido) tempo para o evento. Quando o evento (supondo que ocorreria) está além do final do período de acompanhamento, a censura costuma ser chamada de censura à direita.\nA censura também pode ocorrer quando se observa a presença de um evento, mas não se sabe onde começou. Por exemplo, considere um estudo que investigue o tempo para a recorrência de um câncer após a remoção cirúrgica do tumor primário. Se os pacientes forem examinados 3 meses após a cirurgia e já tinham recorrência, então o tempo de sobrevida será censurado a esquerda, porque o tempo real (desconhecido) de recorrência ocorreu menos de 3 meses após a cirurgia.\nOs dados de tempo do evento também podem ser censurados em intervalos, o que significa que os indivíduos entram e saem da observação. Se considerarmos o exemplo anterior e os pacientes também forem examinados aos 6 meses, aqueles que estão livres da doença aos 3 meses e perdem o acompanhamento entre 3 e 6 meses são considerados censurados no intervalo. A maioria dos dados de sobrevivência incluem observações censuradas à direita (Clark et al. 2003).",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Análise de Sobrevida</span>"
    ]
  },
  {
    "objectID": "23-sobrevida.html#método-de-kaplan-meier",
    "href": "23-sobrevida.html#método-de-kaplan-meier",
    "title": "23  Análise de Sobrevida",
    "section": "23.4 Método de Kaplan-Meier",
    "text": "23.4 Método de Kaplan-Meier\nO método de Kaplan-Meier (KM) é um método não paramétrico usado para estimar a probabilidade de sobrevivência a partir dos tempos de sobrevivência observados (Kaplan e Meier 1958).\nA função de sobrevida é a probabilidade de sobreviver a pelo menos um determinado ponto no tempo e o gráfico desta probabilidade é a curva de sobrevida. O método de sobrevida de Kaplan-Meier pode ser usado para comparar as curvas de sobrevida de dois ou mais grupos, como comparar um grupo tratado a um grupo não tratado (placebo), ou homens comparados a mulheres.\nA curva de sobrevida KM, um gráfico da probabilidade de sobrevida de Kaplan-Meier em relação ao tempo, fornece um resumo útil dos dados que podem ser usados para estimar medidas como a mediana de sobrevida.\n\n23.4.1 Pressupostos do método de Kaplan-Meier\nOs pressupostos para o uso da análise de sobrevida são as seguintes (Peat e Barton 2014):\n\nos participantes devem ser independentes, ou seja, cada participante aparece apenas uma vez no grupo;\nos grupos devem ser independentes, ou seja, cada participante está apenas em um grupo;\ntodos os participantes são livres de eventos quando se inscrevem no estudo;\na medição do tempo até o evento deve ser precisa;\no ponto inicial e o evento são claramente definidos;\nas perspectivas de sobrevida dos participantes permanecem constantes, ou seja, os participantes inscritos no início ou no final do estudo devem ter as mesmas perspectivas de sobrevida;\na probabilidade de censura não está relacionada à probabilidade do evento.\n\nComo em todas as análises, se o número total de pacientes em qualquer grupo for pequeno, digamos menos de 30 participantes em cada grupo, os erros padrão em torno das estatísticas resumidas serão grandes e, portanto, as estimativas de sobrevida serão imprecisas. Para estudos de sobrevida, recomenda-se fazer o cálculo do tamanho amostral previamente. O R dispõe de um pacote que possibilita este cálculo, o powerSurvEpi (Qiu et al. 2015).\n\n\n23.4.2 Dados do exemplo\n\n\n\n\n\n\nCenário\n\n\n\nUm ensaio clínico randomizado hipotético selecionou 60 pacientes para participar de dois tratamentos. Aleatoriamente, um novo medicamento foi administrado a 32 pacientes e 28 usaram o tratamento padrão durante 65 meses. Desses paciente, 33 eram mulheres e 27 homens. Um total de 21 pacientes morreram (7 mulheres e 14 homens)\n\n\nOs dados se encontram no arquivo dadosSobrevida.xlsx que pode ser obtido aqui . Salve o mesmo em seu diretório de trabalho.\n\n23.4.2.1 Carregar o conjunto de dados\nA partir do diretório de trabalho, os dados serão carregados o e atribuídos a um objeto, denominado de sobrevida.\n\nsobrevida &lt;- readxl::read_excel(\"dados/dadosSobrevida.xlsx\")\nstr (sobrevida)\n\ntibble [60 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:60] 22 21 19 13 50 20 51 6 26 31 ...\n $ evento: num [1:60] 0 0 0 0 1 1 1 0 1 0 ...\n $ tempo : num [1:60] 5 7 8 9 9 12 15 16 16 16 ...\n $ sexo  : chr [1:60] \"fem\" \"masc\" \"fem\" \"fem\" ...\n $ grupo : chr [1:60] \"novo\" \"novo\" \"novo\" \"novo\" ...\n\n\nA Saída exibe um banco de dados com cinco variáveis:\n\nid \\(\\longrightarrow\\) Identificação do indivíduo\nevento \\(\\longrightarrow\\) Desfecho. 0 = censurado; 1 = morte\ntempo \\(\\longrightarrow\\) Sobrevida em meses\nsexo \\(\\longrightarrow\\) 1 = masculino; 2 = feminino\ngrupo \\(\\longrightarrow\\) Grupo de tratamento: 1 = novo; 2 = padrão\n\n\n\n23.4.2.2 Construir uma tabela tratamento vs evento\n\ntable (sobrevida$grupo, \n       sobrevida$evento, \n       dnn = c(\"Tratamento\", \"Evento\"))\n\n          Evento\nTratamento  0  1\n    novo   24  8\n    padrão 15 13\n\n\nA saída mostra o número em cada grupo, o número de eventos e o número censurados. Houve menos eventos, mas mais pacientes censurados no grupo do tratamento novo.\n\n\n\n23.4.3 Tabela de Sobrevida\nDeve-se calcular as estimativas de sobrevida de Kaplan-Meier que servirão para a construção da Curva de Sobrevida de cada tratamento. Para isso, pode-se usar a função survfit() do pacote survival(T. Therneau et al. 2015). Seus principais argumentos incluem:\n\nobjeto de sobrevida, criado usando a função Surv(), aninhada na função survfit()\ne o conjunto de dados contendo as variáveis.\n\nPara a construção da tabela e da curva de sobrevida, digite e execute o seguinte:\n\ntabsurv &lt;- survfit (Surv (tempo, evento) ~ grupo, data = sobrevida)\n\nsummary(tabsurv) \n\nCall: survfit(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n                grupo=novo \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    9     29       1    0.966  0.0339       0.9013        1.000\n   12     27       1    0.930  0.0479       0.8404        1.000\n   15     26       1    0.894  0.0579       0.7874        1.000\n   16     25       1    0.858  0.0657       0.7387        0.997\n   32     15       1    0.801  0.0826       0.6545        0.980\n   36     13       1    0.739  0.0965       0.5725        0.955\n   40     11       1    0.672  0.1086       0.4897        0.923\n   58      2       1    0.336  0.2438       0.0811        1.000\n\n                grupo=padrão \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     28       3    0.893  0.0585       0.7853        1.000\n    2     25       1    0.857  0.0661       0.7369        0.997\n    3     24       1    0.821  0.0724       0.6911        0.976\n    4     23       2    0.750  0.0818       0.6056        0.929\n    7     20       1    0.712  0.0859       0.5625        0.902\n   17     19       1    0.675  0.0892       0.5210        0.875\n   21     17       2    0.596  0.0947       0.4361        0.813\n   38      9       1    0.529  0.1048       0.3592        0.780\n   52      2       1    0.265  0.1944       0.0628        1.000\n\n\nA Tabela de sobrevida é uma tabela descritiva com a coluna time, indicando o dia em que o evento ocorreu. A coluna n.risk indica o número de pacientes sob risco naquele momento. A coluna denominada n.event indica o número total de pacientes que sofreram o evento desde o início do estudo até o momento avaliado. A coluna survival indica a proporção de pacientes que sobreviveram desde o início do estudo até aquele momento. Por exemplo, a sobrevida cumulativa é de 0,801 aos 32 meses no grupo tratamento novo e de 0,529 aos 38 meses no grupo tratamento padrão.\nO método Kaplan-Meier produz uma única estatística resumida do tempo de sobrevida, isto é, a média ou mediana. O tempo médio de sobrevida é estimado a partir dos tempos observados e é mostrado para cada grupo na tabela de médias e medianas para o tempo de sobrevida.\nA sobrevida média é calculada como a soma do tempo dividido pelo número de pacientes que permanecem sem censura. Essa estatística pode ser usada para indicar o período de tempo em que um paciente pode sobreviver. O tempo mediano de sobrevida é o ponto em que metade dos pacientes experimentou o evento. Se a curva de sobrevida não cair para 0,5 (ou seja, probabilidade de sobrevida de 50%), o tempo mediano de sobrevida não poderá ser calculado.\nEstes dados podem ser visualizados na saída, obtida com o comando:\n\nsummary(tabsurv)$table\n\n             records n.max n.start events    rmean se(rmean) median 0.95LCL\ngrupo=novo        32    32      32      8 49.92533  4.078218     58      40\ngrupo=padrão      28    28      28     13 36.62437  5.403382     52      21\n             0.95UCL\ngrupo=novo        NA\ngrupo=padrão      NA\n\n\n\n23.4.3.1 Visualização da curva de sobrevida\nPode-se visualizar a curva (Figura 23.1) de uma maneira simples, utilizando a função plot() do pacote básico do R:\n\nplot (tabsurv, col = c (\"steelblue\", \"rosybrown\"), lwd = 2)\nlegend (legend = c (\"Tratamento Novo\", \"Tratamento Padrão\"), \n        fill = c (\"steelblue\", \"rosybrown\"), \n        bty=\"n\", \n        cex = 1, \n        y = 0.3,\n        x = 5)\n\n\n\n\n\n\n\nFigura 23.1: Curva de sobrevida comparando dois grupos de tratamento\n\n\n\n\n\nOutra maneira, mais sofisticada, de produzir a curva de KM é usando a função ggsurvplot(), incluída no pacote survminer (Kassambara et al. 2021) que utiliza o pacote ggplot2 (Figura 23.2)\nCom essa função é possível mostrar:\n\nos intervalos de confiança de 95% da função de sobrevida, usando o argumento conf.int = TRUE;\no número e/ou a porcentagem de indivíduos em risco por tempo, utilizando a opção risk.table. Os valores permitidos para a risk.table incluem:\nTRUE ou FALSE especificando se deve mostrar ou não a tabela de risco. O padrão é FALSE.\nabsolute ou percentage: para mostrar o número absoluto e o percentual de sujeitos em risco por tempo, respectivamente. Use abs_pct para mostrar o número absoluto e a porcentagem.\no nrisk_cumcensor e nrisk_cumevents . Mostra o número em risco e o número acumulado de censura e eventos, respectivamente.\no valor p do teste Log-Rank comparando os grupos usando pval = TRUE.\nlinha horizontal/vertical na sobrevida mediana usando o argumento surv.median.line. Os valores permitidos incluem um de c(“nenhum”, “hv”, “h”, “v”). Onde v = vertical, h = horizontal.\n\n\nggsurvplot (tabsurv,\n            pval = TRUE, \n            conf.int = FALSE,\n            risk.table = \"abs_pct\",\n            risk.table.col = \"strata\", \n            surv.median.line = \"hv\", \n            ggtheme = theme_bw (), \n            legend.labs = c (\"Tratamento Novo\", \n                             \"Tratamento Padrão\"),\n            palette = c (\"steelblue\", \"tomato\"))\n\n\n\n\n\n\n\nFigura 23.2: Curva de sobrevida comparando dois grupos de tratamento, usando ggsurvplot()\n\n\n\n\n\nO teste Log Rank pondera todos os pontos de tempo igualmente e é a estatística de sobrevida mais usada (Bland e Altman 2004). O teste de log rank é um teste não paramétrico, que não faz suposições sobre as distribuições de sobrevivência. Os pressupostos deste teste são os mesmos do método de Kaplan-Meier. No exemplo, o valor p do teste é fornecido na Figura 23.2 e é igual a 0,083, ou seja, acima de 0,05, indicando não rejeição da \\(H_{0}\\). A hipótese nula diz que não há diferença na sobrevivência entre os dois grupos.\nEssencialmente, o teste de log rank compara o número observado de eventos em cada grupo com o que seria esperado se a hipótese nula fosse verdadeira (ou seja, se as curvas de sobrevivência fossem idênticas). A estatística de log rank é aproximadamente distribuída como uma estatística de teste qui-quadrado.\nA função survdiff(), também do pacote survival (T. M. Therneau 2024), pode ser usada para calcular o teste de log-rank comparando duas ou mais curvas de sobrevida e pode ser usado da seguinte forma:\n\ndif_sobrevida &lt;- survdiff (Surv (tempo, evento) ~ grupo, data = sobrevida)\ndif_sobrevida\n\nCall:\nsurvdiff(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n              N Observed Expected (O-E)^2/E (O-E)^2/V\ngrupo=novo   32        8    11.91      1.28      3.01\ngrupo=padrão 28       13     9.09      1.68      3.01\n\n Chisq= 3  on 1 degrees of freedom, p= 0.08 \n\n\nA suposição de que o risco de um evento em um grupo em comparação com o outro grupo não muda ao longo do tempo é chamado de risco proporcional. Se as curvas de sobrevida se cruzam, isso sugere que os riscos não são proporcionais. Nessa situação, o teste log rank será menos poderoso e um teste alternativo deve ser considerado, como a Regressão de Cox ou Modelo de Riscos Proporcionais.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Análise de Sobrevida</span>"
    ]
  },
  {
    "objectID": "23-sobrevida.html#regressão-de-cox-ou-modelo-de-riscos-proporcionais",
    "href": "23-sobrevida.html#regressão-de-cox-ou-modelo-de-riscos-proporcionais",
    "title": "23  Análise de Sobrevida",
    "section": "23.5 Regressão de Cox ou Modelo de Riscos Proporcionais",
    "text": "23.5 Regressão de Cox ou Modelo de Riscos Proporcionais\nO modelo tem como objetivo a examinar simultaneamente como os fatores especificados influenciam a taxa de ocorrência de um determinado evento (por exemplo, infecção, morte) em um determinado ponto no tempo. Essa taxa é referida como hazard ratio.\nGeralmente, as variáveis preditoras (ou fatores) são denominadas covariáveis. O modelo de Cox é expresso pela função de risco denotada por h(t). Pode ser interpretada como o risco de morrer no tempo t e estimada da seguinte forma:\n\\[\nh\\left(t\\right) = h_{0} \\left(t\\right) \\times e^{\\left( {b_{1}x_{1}+b_{2}x_{2}+...+b_{n}x_{n}} \\right)}\n\\]\nOnde,\n\nt é o tempo de sobrevida, indica que o risco varia com o tempo;\nh(t) é a função de risco (hazard) determinada por um conjunto de n covariáveis (\\(x_{1}, x_{2}, ..., x_{n}\\));\nOs coeficientes (\\(b_{1}, b_{2}, ..., b_{n}\\)) medem o tamanho do efeito das covariáveis;\nh(0) é o risco basal, o valor do risco se todos os \\(x_{i}\\) fossem iguais a zero (\\(exp(0) = 1\\)).\n\nAs quantidades exp(\\(b_{i}\\)) são chamadas de hazard ratio (HR). Uma hazard ratio acima de 1 indica uma covariável que está positivamente associada à probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida.\nResumindo,\n\nHR = 1: Sem efeito\nHR &lt;1: Redução do risco\nHR&gt; 1: Aumento do risco\n\nPara calcular o modelo de Cox no R serão utilizados os mesmos dados do arquivo dadosSobrevida.xlsx.\nO pacote survival tem uma função para calcular o modelo de Cox, coxph(), que usa os argumentos:\n\nformula \\(\\longrightarrow\\) é o modelo linear com um objeto de sobrevivida como variável desfecho. O objeto de sobrevida é criado usando a função Surv() como segue: Surv(tempo, evento).\ndata \\(\\longrightarrow\\) um banco de dados contendo as variáveis.\n\n\nmod.cox &lt;- coxph (Surv (tempo, evento) ~ grupo, data = sobrevida)\nmod.cox\n\nCall:\ncoxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n              coef exp(coef) se(coef)     z      p\ngrupopadrão 0.7698    2.1593   0.4505 1.709 0.0875\n\nLikelihood ratio test=3.03  on 1 df, p=0.08171\nn= 60, number of events= 21 \n\n\nA função summary() fornece um relatório mais completo:\n\nsummary(mod.cox)\n\nCall:\ncoxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n  n= 60, number of events= 21 \n\n              coef exp(coef) se(coef)     z Pr(&gt;|z|)  \ngrupopadrão 0.7698    2.1593   0.4505 1.709   0.0875 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ngrupopadrão     2.159     0.4631     0.893     5.221\n\nConcordance= 0.637  (se = 0.054 )\nLikelihood ratio test= 3.03  on 1 df,   p=0.08\nWald test            = 2.92  on 1 df,   p=0.09\nScore (logrank) test = 3.06  on 1 df,   p=0.08\n\n\nOs resultados da regressão de Cox, podem ser interpretados da seguinte forma:\n\nSignificância estatística. A coluna marcada com z fornece o valor da estatística Wald. Corresponde à razão de cada coeficiente de regressão para seu erro padrão (\\(z = \\frac{coef}{EP_{coef}}\\)). A estatística Wald avalia se o coeficiente beta (\\(\\beta\\)) de uma determinada variável é estatisticamente diferente de 0. A partir da saída, pode-se concluir que não há diferença estatisticamente significativa entre os grupos (p = 0,0875).\nCoeficientes de regressão. A seguir deve-se observar, no modelo de Cox, o sinal dos coeficientes de regressão (coef). Um sinal positivo significa que o hazard (risco) é maior e, portanto, pior o prognóstico, para sujeitos com valores mais elevados dessa variável. No exemplo, a variável grupo é codificada como 1=novo, 2=padrão. O resumo do modelo de Cox fornece a hazard ratio (HR) para o segundo grupo em relação ao primeiro grupo, ou seja, tratamento padrão versus tratamento novo. O coeficiente beta para grupo = 0,7698 indica que os indivíduos do tratamento padrão têm maior risco de morte (taxas de sobrevivência mais baixas) do que os do grupo tratamento novo, nesses dados. Entretanto, esta diferença não é estatisticamente significativa.\nHazard ratios. Os coeficientes exponenciados (exp(coef) = exp(0,7698) = 2,1593), também conhecidos como hazard ratio, fornecem o tamanho do efeito das covariáveis. Por exemplo, ser do grupo padrão aumenta o risco por um fator de 2,1593. Se esta diferença fosse significativa (P &lt; 0,05), pertencer ao grupo padrão estaria associado a um mau prognóstico.\nIntervalos de confiança das taxas de risco. O resultado do resumo também fornece intervalos de confiança de 95% para a razão de risco (exp(coef)), limite inferior de 95% = 0,893, limite superior de 95% = 5,221, mostrando a não significância estatística, pois cruza o 1.\nSignificância estatística global do modelo. Finalmente, a saída fornece valores de p para três testes alternativos para significância geral do modelo: O teste de razão de verossimilhança (Likelihood ratio test), teste de Wald e a estatística logrank. Esses três métodos são equivalentes. Para um tamanho amostral grande, eles darão resultados semelhantes. Para n pequeno, eles podem diferir um pouco. O teste de razão de verossimilhança tem melhor comportamento para tamanhos de amostra pequenos, por isso é geralmente preferido.\n\n\n\n\n\n\n\nBland, J Martin, e Douglas G Altman. 2004. «The logrank test». BMJ 328 (7447). British Medical Journal Publishing Group: 1073.\n\n\nClark, Taane G, Michael J Bradburn, Sharon B Love, e Douglas G Altman. 2003. «Survival analysis Part I: basic concepts and first analyses». British Journal of Cancer 89 (2). Nature Publishing Group: 232–38.\n\n\nKaplan, Edward L, e Paul Meier. 1958. «Nonparametric Estimation from Incomplete Observations». Journal of the American Statistical Association 53 (282). Taylor & Francis: 457–81.\n\n\nKassambara, Alboukadel, Marcin Kosinski, Przemyslaw Biecek, e Scheipl Fabian. 2021. «Survminer: Drawing Survival Curves Using ggplot2». URL https://CRAN. R-project. org/package= survminer. R package version 0.4 9.\n\n\nPeat, Jennifer, e Belinda Barton. 2014. «Survival analyses». Em Medical statistics : a guide to SPSS, data analysis, and critical appraisal, 352–53. New York, NY: John Wiley & Sons.\n\n\nQiu, W, J Chavarro, R Lazarus, B Rosner, e J Ma. 2015. «powerSurvEpi: Power and Sample Size Calculation for Survival Analysis of Epidemiological Studies». R package version 0.0 9.\n\n\nTherneau, Terry et al. 2015. «A package for Survival Analysis in R». R package version 2 (7).\n\n\nTherneau, Terry M. 2024. A Package for Survival Analysis in R. https://CRAN.R-project.org/package=survival.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Análise de Sobrevida</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html",
    "href": "24-reglogbin.html",
    "title": "24  Regressão Logística Binária",
    "section": "",
    "text": "24.1 Pacotes necessários neste capítulo\npacman::p_load(Amelia, \n               caret, \n               dplyr,\n               corrplot,\n               flextable,\n               ggplot2,\n               gtsummary,\n               knitr,\n               performance,\n               pROC,\n               quesrionr,\n               readxl,\n               sjPlot)",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#introduçao",
    "href": "24-reglogbin.html#introduçao",
    "title": "24  Regressão Logística Binária",
    "section": "24.2 Introduçao",
    "text": "24.2 Introduçao\nOs dois tipos de regressão mais comuns são a regressão linear e a regressão logística. A regressão linear é utilizada quando a variável dependente é quantitativa e contínua, enquanto a regressão logística é utilizada quando a variável dependente é qualitativa (categórica).\nA regressão logística binária é utilizada quando o objetivo é estimar a probabilidade de um indivíduo pertencer a uma das duas categorias possíveis (o desfecho binário, 0 ou 1), dada uma ou mais variáveis independentes ou preditoras (de qualquer tipo). A regressão logística binária é uma abordagem classificatória que estima a relação entre uma variável dependente dicotômica e um conjunto de preditores. Foi desenvolvida, em 1958, como um a extensão do modelo linear pelo estatístico britânico David Cox (Cox 1958). Pertence a uma família, denominada Modelo Linear Generalizado (GLM).",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#sec-logisticfunction",
    "href": "24-reglogbin.html#sec-logisticfunction",
    "title": "24  Regressão Logística Binária",
    "section": "24.3 Função Logistica ou Função logit",
    "text": "24.3 Função Logistica ou Função logit\nA regressão linear simples (RLS) estima a relação entre uma variável dependente quantitativa contínua (Y) e uma variável explicativa contínua (X). Por exemplo, a relação entre o Comprimento (Y) e a Idade (X) de crianças é estimada pelo método dos Mínimos Quadrados Ordinários (MQO) e descrita pela equação da reta de regressão. Esta reta tenta predizer a variável resposta contínua a partir de uma variável preditora também contínua (Figura 24.1).\n\\[\ny = \\beta_{0} + \\beta_{1} \\times x\n\\]\n\n\n\n\n\n\n\n\nFigura 24.1: Regressão Linear Simples\n\n\n\n\n\nLimitação da RLS para Desfechos Binários\nAgora, se o objetivo é estimar o impacto da idade de um paciente na probabilidade de ele ter ou não um determinado desfecho (por exemplo, uma doença). A idade é uma variável preditora contínua, mas o desfecho (doença) é uma variável categórica dicotômica, assumindo apenas os valores 0 (não) e 1 (sim) (Figura 24.2).\n\n\n\n\n\n\n\n\nFigura 24.2: Relação de uma variável desfecho dicotômica e uma variável preditora contínua.\n\n\n\n\n\nSe for ajustada uma reta de regressão aos pontos usando o método dos Mínimos Quadrados Ordinários, o gráfico (Figura 24.3) terá o seguinte aspecto: A reta de regressão se estende abaixo de 0 e acima de 1 em relação ao eixo Y.\n\n\n\n\n\n\n\n\nFigura 24.3: Reta de regressão quando a variável desfecho é dicotômica.\n\n\n\n\n\nNo entanto, a variável Y (Probabilidade de Doença) não pode assumir valores fora do limite de 0 e 1. Além disso, a variável desfecho dicotômica viola os pressupostos de normalidade e homoscedasticidade dos erros, pois segue a Distribuição de Bernoulli. Em consequência, a regressão linear não é adequada nessa situação.\nA Solução: A Função Logit\nA solução é usar a regressão logística binária, que lida com essas limitações, usando uma função de ligação (link function) que transforma a probabilidade em um valor que pode ser modelado linearmente (e que pode variar de - a +. A função de ligação utilizada no modelo de regressão logística é a Logit (ou função logística), que resulta em uma curva em forma de ‘S’ chamada curva sigmoide (Figura 24.4):\n\\[\nlogit_i = \\ln\\left(\\frac{p}{1-p}\\right)\n\\]\n\n\n\n\n\n\n\n\nFigura 24.4: Curva de regressão logística.\n\n\n\n\n\nO termo \\(\\frac{p}{1-p}\\) é a Razão de Chances (Odds Ratio), que representa a razão entre a probabilidade de ocorrência do evento (p) e a probabilidade de sua não ocorrência (\\(1-p\\)). O logit é o logaritmo natural dessa razão de chances. Na regressão logística, o logit é modelado como uma função linear dos preditores, permitindo a estimação dos coeficientes (\\(\\beta\\)):\n\\[\nlogit_{i} = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_{0} + \\beta_{1}x_{1} + \\dots + \\beta_{n}x_{n}\n\\]\nRetornando à probabilidade p\nAtravés de manipulação algébrica, podemos isolar p para obter a probabilidade do evento Y=1 (sucesso) diretamente, garantindo que o valor final sempre esteja entre 0 e 1:\n\\[\np = \\frac{e^{\\beta_{0} + \\beta_{1}x_{1} + \\dots + \\beta_{n}x_{n}}}{1 + e^{\\beta_{0} + \\beta_{1}x_{1} + \\dots + \\beta_{n}x_{n}}}\n\\]",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#dados-para-o-exemplo",
    "href": "24-reglogbin.html#dados-para-o-exemplo",
    "title": "24  Regressão Logística Binária",
    "section": "24.4 Dados para o exemplo",
    "text": "24.4 Dados para o exemplo\n\n\n\n\n\n\nTragédia do Titanic\n\n\n\nO RMS Titanic (Figura 24.5), considerado o navio inafundável de sua época, sofreu um trágico destino em sua viagem inaugural com destino a Nova Iorque. Construído com o intuito de ser o maior e mais luxuoso transatlântico, o Titanic colidiu com um iceberg na noite de 14 de abril de 1912, no Oceano Atlântico Norte, afundando rapidamente e causando a morte de inúmeras pessoas.\n\n\n\n\n\n\n\n\nFigura 24.5: Titanic parado em Queenstown, 11 de abril.de 1912\n\n\n\n\n\nO Titanic partiu em sua primeira e única viagem com 1316 passageiros a bordo: 325 na primeira classe, 285 na segunda e 706 na terceira. Deles, 922 embarcaram em Southampton, 274 em Cherbourg-Octeville, na França, e 120 em Queenstown, na Irlanda. Além desses passageiros, havia 908 tripulantes, totalizando m 2224 pessoas. O número total de mortos mais aceito é 1514, quase 70% dos que embarcaram na viagem (Wikipédia 2025).\n\n\nOs dados de 1309 passageiros estão em um arquivo denominado dadosTitanic.xlsx, obtido no pacote titanic, modificado estruturalmente e traduzido, sem alterar os dados, para chegar a este arquivo que pode ser baixado aqui. Possui 1309 observações e 12 variáveis:\n• id \\(\\to\\) identificação do passageiro\n• sobreviveu \\(\\to\\) 0 = não; 1 = sim\n• classe \\(\\to\\) classe do passageiro (categórica): 1 = 1ª classe; 2 = 2ª classe e 3 = 3ª classe (qualitativa)\n• nome \\(\\to\\) nome do passageiro (nominal)\n• sexo \\(\\to\\) masc = masculino; fem = feminino (binária)\n• idade \\(\\to\\) idade em anos (numérica contínua)\n• irco \\(\\to\\) número de irmãos/cônjuges a bordo (numérica discreta)\n• pafi \\(\\to\\) número de pais/filhos a bordo (numérica discreta)\n• ticket \\(\\to\\) número do bilhete de embarque (nominal)\n• tarifa \\(\\to\\) valor pago pela passagem em dólares (numérica contínua)\n• cabine \\(\\to\\) número de identificação da cabine (nominal)\n• porto_embarque \\(\\to\\) porto de embarque: C = Cherbourg, Q = Queenstown, S = Southampton\n\n24.4.1 Leitura dos dados\nApós fazer o download do banco de dados em seu diretório 1, carregue-o no RStudio, usando a função read_excel() do pacote readxl:\n\ndadosTitanic &lt;- readxl::read_excel(\"dados/dadosTitanic.xlsx\")\n\n\n\n24.4.2 Explorando e preparando os dados\nPara observar as variáveis do banco de dados, pode-se usar a função str():\n\nstr(dadosTitanic)\n\ntibble [1,309 × 12] (S3: tbl_df/tbl/data.frame)\n $ id            : num [1:1309] 1 2 3 4 5 6 7 8 9 10 ...\n $ sobreviveu    : num [1:1309] 0 1 1 1 0 0 0 0 1 1 ...\n $ classe        : num [1:1309] 3 1 3 1 3 3 1 3 3 2 ...\n $ nome          : chr [1:1309] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ sexo          : chr [1:1309] \"masc\" \"fem\" \"fem\" \"fem\" ...\n $ idade         : num [1:1309] 22 38 26 35 35 NA 54 2 27 14 ...\n $ irco          : num [1:1309] 1 1 0 1 0 0 0 3 0 1 ...\n $ pafi          : num [1:1309] 0 0 0 0 0 0 0 1 2 0 ...\n $ ticket        : chr [1:1309] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ tarifa        : num [1:1309] 7.25 71.28 7.92 53.1 8.05 ...\n $ cabine        : chr [1:1309] NA \"C85\" NA \"C123\" ...\n $ porto_embarque: chr [1:1309] \"S\" \"C\" \"S\" \"S\" ...\n\n\nA saída exibe um tibble com 1309 linhas (casos = passageiros) e 12 colunas (variáveis).\n\n24.4.2.1 Remoção de variáveis irrelevantes\nAlgumas dessas variáveis não terão utilidade para a análise de regressão logística: por exemplo, a coluna índice (id), o nome do passageiro, o número do ticket de embarque e o número da cabine. Elas serão removidas do banco de dados, usando a função select() do pacote dplyr (Seção 5.6):\n\ndadosTitanic &lt;- dplyr::select(dadosTitanic, \n                              -c(id, nome, ticket, cabine))\n\n\n\n24.4.2.2 Conversão de variáveis para fatores\nAlgumas variáveis estão classificadas como numéricas (classe, sexo, porto_embarque e sobreviveu), mas são fatores. Esta modificação será realizada, usando a função mutate() do pacote dplyr:\n\nlibrary(dplyr)\ndadosTitanic &lt;- dadosTitanic %&gt;%\n  mutate(sexo = factor(sexo)) %&gt;%\n  mutate(classe = factor(classe)) %&gt;% \n  mutate(porto_embarque = factor(porto_embarque)) %&gt;% \n  mutate(sobreviveu = factor(sobreviveu))\n\nstr(dadosTitanic)\n\ntibble [1,309 × 8] (S3: tbl_df/tbl/data.frame)\n $ sobreviveu    : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ classe        : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ sexo          : Factor w/ 2 levels \"fem\",\"masc\": 2 1 1 1 2 2 2 2 1 1 ...\n $ idade         : num [1:1309] 22 38 26 35 35 NA 54 2 27 14 ...\n $ irco          : num [1:1309] 1 1 0 1 0 0 0 3 0 1 ...\n $ pafi          : num [1:1309] 0 0 0 0 0 0 0 1 2 0 ...\n $ tarifa        : num [1:1309] 7.25 71.28 7.92 53.1 8.05 ...\n $ porto_embarque: Factor w/ 3 levels \"C\",\"Q\",\"S\": 3 1 3 3 3 2 3 3 3 1 ...\n\n\n\n\n24.4.2.3 Divisão dos dados em Treino e Teste\nA divisão dos dados em conjuntos de treino e teste é uma prática frequente em aprendizado de regressão logística. Serve para avaliar a performance de um modelo de forma imparcial precisa. O conjunto treino é a parte dos dados que o modelo utiliza para aprender as relações entre as variáveis independentes e a variável dependente. O modelo analisa esses dados para encontrar os melhores coeficientes que descrevem a relação entre as variáveis. O conjunto teste é a parte dos dados que o modelo não analisou durante o treinamento. Ele é utilizado para avaliar a capacidade do modelo de fazer previsões em novos dados. Ao comparar as previsões do modelo com os valores reais no conjunto teste, pode-se medir a sua precisão e generalização. Essa técnica é utilizada para evitar o excesso de ajuste dos dadosconhecido como overfitting (Subramanian e Simon 2013), estimar a acurácia do modelo e comparação de modelos. A divisão mais comum é 70% para treinamento e 30% para teste, mas essa proporção pode variar dependendo do tamanho do conjunto de dados e da complexidade do problema.\n\n# Definindo a semente para reprodutibilidade (essencial)\nset.seed(123)\n\n# Embaralhando os dados\ndados_embaralhados &lt;- dadosTitanic[sample(nrow(dadosTitanic)),]\n\n# Dividindo os dados em treino e teste\nlibrary(caret)\ntrainIndex &lt;- createDataPartition(dados_embaralhados$classe, p = 0.7, list = FALSE)\ntreino &lt;- dados_embaralhados[trainIndex, ]\nteste  &lt;- dados_embaralhados[-trainIndex, ]\n\n# Observar o tamanho das amostras\ndim(treino)\n\n[1] 918   8\n\ndim(teste)\n\n[1] 391   8\n\n\n\n\n24.4.2.4 Verificação e tratamento dos dados faltantes\nO próximo passo é investigar dados faltantes (NA) ou valores vazios2 em cada conjunto separadamente. Isso garante que o tratamento seja feito corretamente sem contaminar o teste.\nPara verificar os dados faltantes e espaços vazios (strings ““) no banco de dados, pode-se usar o comando que soma os dados faltantes e espaços vazios em cada coluna do banco de dados:\nVerificar valores faltantes\n\n# Contagem de valores faltantes por variável\ncolSums(is.na(treino))\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0            175              0 \n          pafi         tarifa porto_embarque \n             0              1              2 \n\ncolSums(is.na(teste))\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0             88              0 \n          pafi         tarifa porto_embarque \n             0              0              0 \n\n\nVerificar valores vazios (strings ““)\n\ncolSums(treino == \"\")\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0             NA              0 \n          pafi         tarifa porto_embarque \n             0             NA             NA \n\ncolSums(teste == \"\")\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0             NA              0 \n          pafi         tarifa porto_embarque \n             0              0              0 \n\n\nNo subconjunto treino, existem 175 NAs na variável idade, um NA na variável tarifa e dois na variável porto_embarque; no subconjunto teste, têm 88 NAs na variável idade. Não existem espaços vazios em nenhum dos subconjuntos.\nUma visualização rápida (Figura 24.6, Figura 24.7) pode ser feita com a função missmap() do pacote Amelia:\n\nlibrary(Amelia)   \nmissmap(treino, main = \"\")\n\n\n\n\n\n\n\nFigura 24.6: Dados faltantes no subconjunto treino\n\n\n\n\n\n\nlibrary(Amelia)   \nmissmap(teste, main = \"\")\n\n\n\n\n\n\n\nFigura 24.7: Dados faltantes no subconjunto teste\n\n\n\n\n\nTratamento dos dados faltantes\nA melhor forma de tratar dados faltantes depende de diversos fatores (Prabhakaran 2016), como:\n\nMecanismo de geração dos dados faltantes: Por que os dados estão faltando? É aleatório, relacionado a outras variáveis ou a alguma característica da população?\nQuantidade de dados faltantes: 263 valores faltantes representam uma proporção considerável dos dados (20%).\nImpacto na análise: Como a presença de dados faltantes pode afetar os resultados da sua análise?\n\nO que fazer?\n\nExclusão de casos:\n\n\n\nListwise deletion: Remover todas as observações com algum dado faltante. Não recomendado neste caso, pois você perderia uma quantidade significativa de dados.\nPairwise deletion: Utilizar todas as observações disponíveis para cada análise. Pode gerar resultados inconsistentes.\n\n\n\nImputação:\n\n\n\nImputação por média, mediana ou moda: Substituir os valores faltantes pela média, mediana ou moda da variável. Simples, mas pode subestimar a variância.\nImputação por regressão: Utilizar um modelo de regressão para prever os valores faltantes com base em outras variáveis. Mais preciso, mas pode ser enviesado se o modelo não for adequado.\nImputação múltipla: Criar múltiplos conjuntos de dados, cada um com diferentes valores imputados, e combinar os resultados das análises. Método mais robusto e permite estimar a incerteza.\nImputação por K-Nearest Neighbors: Substituir os valores faltantes pela média dos k vizinhos mais próximos. Útil para dados numéricos e pode capturar padrões locais.\n\nInicialmente, será removido os valores omissos nas variáveis tarifa e porto_embarque, no subconjunto treino:\n\ntreino &lt;- treino[-which(is.na(treino$tarifa)),]\ntreino &lt;- treino[-which(is.na(treino$porto_embarque)),]\n\nDessa forma, permanecem os valores faltantes na variáveis idade em ambos subconjuntos. Será feita a imputação, usando a mediana por ser uma estratégia simples.\n\ntreino$idade[is.na(treino$idade)] &lt;- median(treino$idade, na.rm=TRUE)\nteste$idade[is.na(teste$idade)] &lt;- median(teste$idade, na.rm=TRUE)\n\nNovamente, se usarão as funções colSums(is.na()) e str()para pesquisar se existem dados faltantes:\n\n# Contagem de valores faltantes por variável\ncolSums(is.na(treino))\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0              0              0 \n          pafi         tarifa porto_embarque \n             0              0              0 \n\ncolSums(is.na(teste))\n\n    sobreviveu         classe           sexo          idade           irco \n             0              0              0              0              0 \n          pafi         tarifa porto_embarque \n             0              0              0 \n\n\nOs dados (treino e teste) não apresentam mais dados faltantes e estão pronts para serem usados na análise.\n\n\n\n\n\n\nImportante\n\n\n\nApós o tratamento dos dados, tem-se dois conjuntos de dados que servirão para:\n\nTreino - que será usado para realizar a regressão logística. Corresponde a aproximadamente 70% dos dados originais.\n\nTeste - que será usado para avaliar a performance do modelo e a sua capacidade de predição.\n\nVariável dependente ou resposta: sobreviveu\n\nVariáveis explicativas ou preditoras: as demais variáveis\n\nO objetivo da análise é gerar um modelo de regressão logística capaz de predizer qual a probabilidade de um passageiro com determinasdas características sobreviver ao naugráfio do Titanic, ocorrido em 14 de abril de 1912",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#construção-do-modelo-de-regressão-logística",
    "href": "24-reglogbin.html#construção-do-modelo-de-regressão-logística",
    "title": "24  Regressão Logística Binária",
    "section": "24.5 Construção do modelo de Regressão Logística",
    "text": "24.5 Construção do modelo de Regressão Logística\nAntes de criar o modelo, alguns passos são importantes:\n\nVerificar se existe correlação entre as variáveis numéricas, para evitar problemas de multicolinearidade, pois se as variáveis forem altamente correlacionadas pode causar:\n\n\nCoeficientes instáveis (pequenas mudanças nos dados podem gerar grandes variações nos coeficientes estimados);\nTorna difícil o entendimento o efeito individual de cada variável sobre a probabilidade do desfecho;\nRedução da precisão estatística, pois aumentam os erros padrão dos coeficientes, podendo tornar variáveis significativas aparaentemente insignificantes.\n\nPara isso, deve-se avaliar a correlação de Pearson entre as variáveis numéricas. Valores acima de 0,8 ou abaixo de -0,8 são preocupantes.\n\n# Selecionar apenas as variáveis numéricas\ndados_numericos &lt;- treino[c(\"idade\", \"irco\", \"pafi\", \"tarifa\")]\n\n# Calcula a matriz de correlação de Pearson\nmatriz_cor &lt;- cor(dados_numericos, use = \"all.obs\", method = \"pearson\")\n\n# Exibe a matriz\nprint(matriz_cor)\n\n            idade       irco       pafi    tarifa\nidade   1.0000000 -0.1745660 -0.1218267 0.1914591\nirco   -0.1745660  1.0000000  0.3914691 0.1755954\npafi   -0.1218267  0.3914691  1.0000000 0.2194998\ntarifa  0.1914591  0.1755954  0.2194998 1.0000000\n\n\nÉ possível visualizar essa matriz com a função corrplot(Figura 24.8)\n\nlibrary(corrplot)\n\n# Visualiza a matriz de correlação\ncorrplot(matriz_cor, method = \"circle\", type = \"upper\", tl.cex = 0.8)\n\n\n\n\n\n\n\nFigura 24.8: Matriz de correlação\n\n\n\n\n\n\nVerificar se há observações incompletas, apesar de todo tratamento dos dados faltantes ou vazios:\n\n\n# Se todas informações forem completas, retorna FALSE\nany(!complete.cases(treino))\n\n[1] FALSE\n\n\n\nVerificar se existe um desbalanceamento preocupante, ou seja, quando uma das classes do desfecho reperesenta menos de 10% dos casos (desfecho raro), pois pode comprometer a preformance e a interpretação do modelo, reduzindo a capacidade preditiva.\n\n\ntab &lt;- table(treino$sobreviveu)\ntab\n\n\n  0   1 \n575 340 \n\nprop.table(tab)\n\n\n        0         1 \n0.6284153 0.3715847 \n\n\n\n24.5.1 Modelos de Regressão Logística\n\n24.5.1.1 Modelo 1 - modelo completo\nO modelo 1 (mod1) será construído com o subconjunto treino, usando a função nativa glm() – generalized linear model - usada para aplicar uma regressão logística no R. Sua funcionalidade é idêntica à função lm() da regressão linear. Necessita alguns argumentos:\n\nformula \\(\\to\\) objeto da classe formula. Um preditor típico tem o formato resposta ~ preditor em que resposta, na regressão logística binária, é uma variável dicotômica e o preditor pode ser uma série de variáveis numéricas ou categóricas;\nfamily \\(\\to\\) uma descrição da distribuição de erro e função de link a ser usada no modelo glm, pode ser uma string que nomeia uma função de family. O padrão é family = gaussian(). No caso da regressão logística binária, family = binomial() ou family = binomial (link =”logit”). Para outras informações, use help(glm) ou help(family);\ndata \\(\\to\\) banco de dados.\n\nDentro dos parênteses da função glm(), são fornecidas informações essenciais sobre o modelo. À esquerda do til (~), encontra-se a variável dependente, que deve estar codificada como 0 e 1 para que a função a interprete corretamente como binária. Após o til, são listadas as variáveis preditoras. Quando se utiliza um ponto (~.), isso indica a inclusão de todas as variáveis preditoras disponíveis. Já o uso do asterisco (*) entre duas variáveis preditoras especifica que, além dos efeitos principais, também deve ser considerado um termo de interação entre elas. No exemplo apresentado, nesta análise inicial, não será solicitada os efeitos da interação. Por fim, após a vírgula, define-se que a distribuição utilizada é a binomial. Como a função glm usa logit como link padrão para uma variável de desfecho binomial, não há necessidade de especificá-lo explicitamente no modelo.\nO modelo inicial de regressão logística do tipo entrada forçada (enter), método padrão de conduzir uma regressão, que consiste em simplesmente colocar todos os preditores no modelo de regressão em um bloco (modelo completo) e estimar parâmetros para cada um (Field, Miles, e Field 2012). O dataframe treino será usado com todos os preditores dentro da função. O objeto criado será denominado de mod1.\n\nmod1 &lt;- glm(sobreviveu ~., \n               data = treino, \n               family = binomial(link = \"logit\"))\n\nA saída da função glm() fornece os coeficientes, da mesma forma como na regressão linear, que estima o efeito das variáveis preditoras sobre a chance de ocorrência do desfecho (no exemplo, sucesso = 1).\nInterpretação Coeficientes\nOs coeficientes podem ser interpretados da seguinte maneira (Anderson 2023):\n\nCoeficiente positivo (\\(\\beta \\gt 0\\)) : Aumenta o log das chances do evento ocorrer. Ou seja, aumenta a probabilidade de desfecho = 1, mantendo as outras variáveis constantes.\nCoeficiente negativo (\\(\\beta \\lt 0\\)) : Diminui o log das chances do evento ocorrer. Ou seja, reduz a probabilidade de desfecho = 1, mantendo as outras variáveis constantes.\nCoeficiente zero (\\(\\beta = 0\\)) : A variável não tem efeito sobre o desfecho.\n\nInterpretação das outras métricas\nO Null deviance representa o quão bem a variável resposta é prevista por um modelo que inclui apenas o intercepto e não as variáveis independentes. Representa o erro do modelo sem preditores. Seria como dizer: “Qual o erro se eu apenas chutar a média geral?”;\nO Residual deviance mostra quão bem a variável de resposta é prevista por um modelo que inclui todas as variáveis. Representa o erro do modelo com preditores. Quanto menor o Residual Deviance em relação ao Null Deviance, melhor o modelo ajusta os dados. No mod1, o Residual Deviance é igual a 708.95, bem menor que o Null Deviance (1207.42), ou seja, o modelo está explicando bem os dados. A diferença entre os dois (1207.42 − 708.95 = 498.47) indica o ganho de ajuste ao incluir as variáveis. Quanto maior essa redução, melhor o modelo. Podemos testar essa diferença com um teste qui-quadrado para verificar se a melhora é estatisticamente significativa:\n\nanova(mod1, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: sobreviveu\n\nTerms added sequentially (first to last)\n\n               Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                             914    1207.42              \nclasse          2    71.16       912    1136.26 3.527e-16 ***\nsexo            1   395.18       911     741.08 &lt; 2.2e-16 ***\nidade           1    12.34       910     728.74 0.0004435 ***\nirco            1    13.68       909     715.07 0.0002172 ***\npafi            1     2.96       908     712.11 0.0853955 .  \ntarifa          1     1.84       907     710.27 0.1754262    \nporto_embarque  2     1.32       905     708.95 0.5161721    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO teste estatístico compara o Null Deviance (modelo com apenas o intercepto) com a Residual Deviance (modelo com preditores), usando o teste qui-quadrado para verificar se a inclusão das variáveis melhora significativamente o ajuste. Se o valor p for menor que 0.05, pode-se concluir que o modelo com preditores é significativamente melhor que o modelo nulo até a inclusão da variável irco. A partir daí a inclusão de novas variáveis não melhora estatisticamente o modelo.\nO AIC (Akaike Information Criterion) é uma medida estatística de ajuste que penaliza o modelo logístico pelo número de variáveis preditivas. Um modelo com valor mínimo de AIC é considerado um modelo bem ajustado. Um AIC = 728.95 isolado é difícil de interpretar, pois é útil para comparar modelos, quanto menor, melhor. Útil quando se testa diferentes combinações de variáveis.\nPara ver todo o resultado do modelo, executa-se a função summary():\n\nsummary(mod1)\n\n\nCall:\nglm(formula = sobreviveu ~ ., family = binomial(link = \"logit\"), \n    data = treino)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      3.810669   0.482866   7.892 2.98e-15 ***\nclasse2         -0.784488   0.317787  -2.469  0.01356 *  \nclasse3         -1.842339   0.316591  -5.819 5.91e-09 ***\nsexomasc        -3.483054   0.216705 -16.073  &lt; 2e-16 ***\nidade           -0.032623   0.008303  -3.929 8.52e-05 ***\nirco            -0.327286   0.117324  -2.790  0.00528 ** \npafi            -0.200766   0.113950  -1.762  0.07809 .  \ntarifa           0.002822   0.002310   1.222  0.22176    \nporto_embarqueQ  0.265542   0.409950   0.648  0.51715    \nporto_embarqueS -0.125789   0.255076  -0.493  0.62191    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.42  on 914  degrees of freedom\nResidual deviance:  708.95  on 905  degrees of freedom\nAIC: 728.95\n\nNumber of Fisher Scoring iterations: 5\n\n\nUso do Odds Ratio (OR) na interpretação dos coeficientes\nNumericamente, os coeficientes de regressão logística não são facilmente interpretáveis em escala bruta, pois estão representados como log (odds) ou logit . Para tornar mais simples, inverte-se a transformação logística, exponenciando os coeficientes (exp(coeficiente)). Isso faz com que os coeficientes se transformem em razões de chance (odds ratio), ficando mais intuitivos facilitando a interpretação. Isso pode ser realizado pela função odds.ratio() do pacote questionr (Barnier, Briatte, e Larmarange 2025), que retorna as OR e seus intervalos de confiança de 95%, além dos respectivos valores p:\n\nlibrary(questionr)\nodds.ratio(mod1)\n\n                       OR     2.5 %   97.5 %         p    \n(Intercept)     45.180634 17.845512 118.7689 2.979e-15 ***\nclasse2          0.456353  0.244196   0.8507  0.013564 *  \nclasse3          0.158446  0.084837   0.2944 5.909e-09 ***\nsexomasc         0.030713  0.019839   0.0464 &lt; 2.2e-16 ***\nidade            0.967903  0.952070   0.9836 8.521e-05 ***\nirco             0.720878  0.564399   0.8952  0.005277 ** \npafi             0.818104  0.648255   1.0181  0.078091 .  \ntarifa           1.002826  0.998331   1.0077  0.221764    \nporto_embarqueQ  1.304138  0.582937   2.9124  0.517151    \nporto_embarqueS  0.881801  0.536235   1.4594  0.621912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuando o OR é menor que 1, significa que o evento (no exemplo: sobrevivência) é menos provável no grupo em questão comparado ao grupo de referência. Para interpretar em termos percentuais:\n\\[\n\\text{Redução percentual} = \\left(1 - OR  \\right) \\times 100\n\\]\nOu seja, considerando os passageiros da 2ª classe, tem-se que \\(1 - 0.46 = 0.54 \\times 100 = 54\\)%. Assim, os passageiros da 2ª classe tem 54% menos chance de sobreviver que os da 1ª classe (nível de referência ); o da 3ª classe tem 84% menos chance de sobreviver que os da 1ª classe. Para saber o nível de referência, basta usar a função levels() da seguinte maneira:\n\nlevels(treino$classe)\n\n[1] \"1\" \"2\" \"3\"\n\n\nQuando o OR é maior que 1, o evento é mais provável no grupo em questão. A interpretação muda:\n\\[\n\\text{Aumento percentual} = \\left(OR - 1 \\right) \\times 100\n\\]\nConsiderando a variável preditora tarifa, cujo OR = 1.003, tem-se \\(1.003 - 1 = 0.003 \\times 100 = 0.3\\). Ou seja, um aumento de 0,3% na sobrevivência. Um valor muito pequeno que se mostrou sem significância estatística (p = 0.2218).\nOs homens têm \\(1 - 0.03 = 0.97 \\times 100 = 97\\)% menos chance de sobreviver do que as mulheres.\nQual a chance das mulheres sobreviverem em relação aos homens?\nPara responder essa pergunta, basta inverter a OR. Assim, as mulheres têm \\(1/0.03 \\approx\\) 33 vezes mais chance de sobreviver que os homens.\n\nA idade (variável numérica) mostra que o aumento da idade diminuiu a sobrevivência; para cada ano a mais de idade a chance de sobrevivência reduz em \\(1 - 0.97 = 0.03 \\times 100 = 3\\)%. Ter irmãos/cônjuges (variável irco) a bordo reduz a chance de sobrevivência em \\(1 - 0.72 = 0.28 \\times 100 = 28\\)%. As demais variáveis não tiveram um efeito significativo. \n\n\n24.5.1.2 Modelo 2 - modelo simples\nNo ajuste realizado no modelo 1, apenas as variáveis classe, sexo, idade e irco foram significativas. Neste novo modelo (mod2), essas variáveis permanecem e as não significativas são removidas.\n\nmod2 &lt;- glm(sobreviveu ~ classe + sexo + idade + irco,\n            data = treino, family = binomial(link = \"logit\"))\nsummary(mod2)\n\n\nCall:\nglm(formula = sobreviveu ~ classe + sexo + idade + irco, family = binomial(link = \"logit\"), \n    data = treino)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.866786   0.415034   9.317  &lt; 2e-16 ***\nclasse2     -0.980705   0.278733  -3.518 0.000434 ***\nclasse3     -2.021346   0.259335  -7.794 6.47e-15 ***\nsexomasc    -3.436506   0.207677 -16.547  &lt; 2e-16 ***\nidade       -0.032155   0.008241  -3.902 9.55e-05 ***\nirco        -0.374735   0.113269  -3.308 0.000938 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.42  on 914  degrees of freedom\nResidual deviance:  715.07  on 909  degrees of freedom\nAIC: 727.07\n\nNumber of Fisher Scoring iterations: 5\n\n\nA saída do summary() mostra que os coeficientes permaneceram apresentando significância estatística.\nRazão de Chances do modelo 2\nUsando a mesma função, usada para o mod1, calcula-se as OR para o mod2:\n\nlibrary(questionr)\nodds.ratio(mod2)\n\n                   OR     2.5 %   97.5 %         p    \n(Intercept) 47.788557 21.606965 110.1338 &lt; 2.2e-16 ***\nclasse2      0.375046  0.215871   0.6447 0.0004341 ***\nclasse3      0.132477  0.078973   0.2186 6.474e-15 ***\nsexomasc     0.032177  0.021173   0.0478 &lt; 2.2e-16 ***\nidade        0.968357  0.952626   0.9839 9.551e-05 ***\nirco         0.687472  0.542726   0.8465 0.0009384 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOs passageiros da 2ª classe, tem, agora, \\(1 - 0.38 = 0.62 \\times 100 = 62\\)% menos chance de sobreviver que os da 1ª classe (nível de referência); o da 3ª classe tem 87% menos chance de sobreviver que os da 1ª classe. Esses valores e os restantes não diferem muito dos do mod1.\nComo o AIC do mod2(727.07) é menor que o do mod1(728.95) é possível dizer que houve uma melhora no ajuste, apesar de a diferença ser muito pequena.\nEntretanto, outros modelos são possíveis.\n\n\n24.5.1.3 Modelo 3\nA partir do modelo completo (mod1), será criado um terceiro modelo, selecionando as variáveis automaticamente, usando a função step(). Usando o argumento direction = \"backward\", a função realiza a eliminação regressiva — ou seja, começa com o modelo completo e vai retirando variáveis uma a uma. Avalia o impacto de remover cada variável individualmente. Remove a variável cuja retirada melhora o critério de ajuste (em geral, AIC) e vai repetindo o processo até que nenhuma remoção adicional melhore o modelo. Considerando ajuste e simplicidade , quanto menor o AIC, melhor o modelo.\n\nmod3 &lt;- step(mod1, direction = \"backward\") \n\nStart:  AIC=728.95\nsobreviveu ~ classe + sexo + idade + irco + pafi + tarifa + porto_embarque\n\n                 Df Deviance     AIC\n- porto_embarque  2   710.27  726.27\n- tarifa          1   710.48  728.48\n&lt;none&gt;                708.95  728.95\n- pafi            1   712.18  730.18\n- irco            1   718.20  736.20\n- idade           1   724.99  742.99\n- classe          2   747.91  763.91\n- sexo            1  1078.51 1096.51\n\nStep:  AIC=726.27\nsobreviveu ~ classe + sexo + idade + irco + pafi + tarifa\n\n         Df Deviance     AIC\n- tarifa  1   712.11  726.11\n&lt;none&gt;        710.27  726.27\n- pafi    1   714.11  728.11\n- irco    1   720.40  734.40\n- idade   1   725.91  739.91\n- classe  2   749.18  761.18\n- sexo    1  1093.47 1107.47\n\nStep:  AIC=726.11\nsobreviveu ~ classe + sexo + idade + irco + pafi\n\n         Df Deviance     AIC\n&lt;none&gt;        712.11  726.11\n- pafi    1   715.07  727.07\n- irco    1   721.21  733.21\n- idade   1   728.02  740.02\n- classe  2   779.41  789.41\n- sexo    1  1098.56 1110.56\n\nsummary(mod3)\n\n\nCall:\nglm(formula = sobreviveu ~ classe + sexo + idade + irco + pafi, \n    family = binomial(link = \"logit\"), data = treino)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.979416   0.423047   9.407  &lt; 2e-16 ***\nclasse2     -0.987641   0.279600  -3.532 0.000412 ***\nclasse3     -2.013771   0.259956  -7.747 9.44e-15 ***\nsexomasc    -3.519814   0.215357 -16.344  &lt; 2e-16 ***\nidade       -0.032431   0.008297  -3.909 9.28e-05 ***\nirco        -0.321086   0.116010  -2.768 0.005645 ** \npafi        -0.186557   0.110554  -1.687 0.091512 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.42  on 914  degrees of freedom\nResidual deviance:  712.11  on 908  degrees of freedom\nAIC: 726.11\n\nNumber of Fisher Scoring iterations: 5\n\n\nEste terceiro modelo chega a um AIC = AIC: 726.11, melhor que os anteriores e incluindo a variável pafi.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#seleção-do-modelo",
    "href": "24-reglogbin.html#seleção-do-modelo",
    "title": "24  Regressão Logística Binária",
    "section": "24.6 Seleção do Modelo",
    "text": "24.6 Seleção do Modelo\nExistem várias maneiras de verificar qual o modelo apresenta um melhor ajuste.\n\n24.6.1 Deviance\nO Deviance mede o quão bem o modelo se ajusta aos dados observados. Ele é baseado na log-verossimilhança (log-likelihood -LL) e funciona como uma medida de erro:\n• Deviance nulo: Representa o erro do modelo sem preditores, apenas com intercepto.\n\\[\nD_{null} = 2 \\times \\left(LL_{\\text{modelo saturado}} - LL_{\\text{modelo nulo}}\\right)\n\\]\nOnde o modelo saturado é um modelo teórico que se ajusta perfeitamente os dados - a log-verossimilhança é máxima possível e o modelo nulo é o modelo que só usa a média da resposta (sem preditores) e serve como base de comparação.\n• Deviance residual: Representa o erro do modelo com preditores.\n\\[\nD_{residual} = 2 \\times \\left(LL_{\\text{modelo saturado}} - LL_{\\text{modelo proposto}}\\right)\n\\]\nSe a null deviance é pequena, o modelo nulo já explica bem os dados; Se a deviance residual é muito menor que a null deviance, o modelo proposto melhora o ajuste. A diferença entre elas pode ser testada com um teste qui-quadrado.\nComo os modelos são aninhados (um é uma versão mais simples do outro), pode-se compará-los, usando a função anova(). Assumindo que o \\(mod2 \\subset mod3 \\subset mod1\\), ou seja, mod2 tem menos preditores que mod3, que tem menos que mod1. A hipótese nula deste teste é que os modelos são equivalentes.\n\nanova(mod2, mod3, mod1, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sobreviveu ~ classe + sexo + idade + irco\nModel 2: sobreviveu ~ classe + sexo + idade + irco + pafi\nModel 3: sobreviveu ~ classe + sexo + idade + irco + pafi + tarifa + porto_embarque\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1       909     715.07                       \n2       908     712.11  1   2.9591   0.0854 .\n3       905     708.95  3   3.1586   0.3678  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO argumento poderia ser test =\"LRT\". Ambos testes calculam a diferença entre as deviances dos modelos e comparam essa diferença com uma distribuição qui-quadrado para obter o valor p.\nA Tabela de Deviance mostra a inclusão das variável pafi no mod3, melhora o modelo, mas não de forma estatisticamente significativa ao nível de 5% (p &gt; 0.05), embora seja marginalmente significativa ao nível de 10% (p &lt; 0.10).\nA inclusão de Parch e Fare no mod1 não melhora significativamente o modelo (p &gt; 0.05).\nConcluindo: Os modelos têm, a grosso modo, Deviances muito próximos e a diferença não é significativa (valor p &gt; 0,05 no teste de razão de verossimilhança), e pode-se aceitar que eles têm desempenho equivalente. Entretanto, o mod2 (model 1 na tabela de Deviance) é o mais simples, mais parcimonioso. Portanto, seria o mais adequado para seguir a análise.\nPode-se, também, usar um critério empírico que compara o Deviance residual com os graus de liberdade:\n\\[\n\\frac{D_{residual}}{n-k}\n\\]\nOnde n = número de observações e k = número de parâmetros, incluindo o intercepto.\nConsiderando o mod2, temos 4 variáveis explicativas (classe + sexo + idade + irco), mas cada uma pode gerar mais um parâmetro, dependendo da codificação (por ex.: classe com 3 níveis \\(\\to\\) 2 parâmetros; sexo com dois níveis \\(\\to\\) 1 parâmetro). Além disso, o modelo inclui um intercepto, então o total de parametros k é:\n\nclasse: 2 parâmetros (niveis 2 e 3)\n\nsexo: 1 parâmetro (masculino)\n\nidade: 1 parâmetro (numérica)\n\nirco: 1 parâmetro (numérica)\n\nIntercepto: 1\n\n\\[\n\\frac{715.07}{889-6} = \\frac{790.88}{883} \\approx 0.810\n\\]\nEsse valor \\(&lt;1\\) indica que o modelo está ajustado de forma adequada — ou seja, o erro médio por grau de liberdade é baixo. Os outros modelos também têm valoes \\(&lt;1\\). Como visto anteriormente, eles não são muito diferentes em relação ao ajuste.\n\n\n24.6.2 AIC (Akaike Information Criterion)\nO AIC é uma medida estatística de ajuste que penaliza o modelo logístico pelo número de variáveis preditivas. Um modelo com valor mínimo de AIC é considerado um modelo bem ajustado. Desta forma o AIC serve para comparar modelos;\n\nAIC(mod1)\n\n[1] 728.9485\n\nAIC(mod2)\n\n[1] 727.0662\n\nAIC(mod3)\n\n[1] 726.1071\n\n\nExiste uma melhora do AIC do mod1 para o mod2 e deste para o mod3. O teste estatístico (anova) mostrou que estatisticamente os modelos podem ser considerados equivalentes.\nAssim, até aqui, baseado no princípio da simplicidade com eficácia (parcimônia), segue-se com o mod2. Entretanto, em um modelo mais complexo existe a possibilidade de interação entre variáveis.\n\n\n24.6.3 Modelo 4: Interação\nUma interação ocorre quando a relação entre as variáveis preditora e desfecho depende do valor ou do nível assumido por outra variável preditora.\nNo exemplo em estudo, poderia haver uma interação se, por exemplo, a relação entre classe do passageiro e sobrevivência ao naufrágio dependesse do sexo. Ou seja, passageiros de uma mesma classe no navio podem ter diferentes probabilidades de sobrevivência se forem de sexos diferentes. Diante dessa possibilidade, será ajustado um novo modelo (mod4), incluindo a interação classe * sexo.\n\nmod4 &lt;- glm(\n  sobreviveu ~ classe * sexo + idade + irco,\n  data = treino,\n  family = binomial(link = \"logit\"))\n\nsummary(mod4)\n\n\nCall:\nglm(formula = sobreviveu ~ classe * sexo + idade + irco, family = binomial(link = \"logit\"), \n    data = treino)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       5.600274   0.827754   6.766 1.33e-11 ***\nclasse2          -1.542878   0.860253  -1.794 0.072890 .  \nclasse3          -4.054871   0.757701  -5.352 8.72e-08 ***\nsexomasc         -5.255350   0.754549  -6.965 3.29e-12 ***\nidade            -0.037505   0.009137  -4.105 4.05e-05 ***\nirco             -0.374646   0.116956  -3.203 0.001359 ** \nclasse2:sexomasc  0.246207   0.935090   0.263 0.792322    \nclasse3:sexomasc  2.717634   0.791466   3.434 0.000595 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.42  on 914  degrees of freedom\nResidual deviance:  683.37  on 907  degrees of freedom\nAIC: 699.37\n\nNumber of Fisher Scoring iterations: 6\n\n\nComparando esse novo modelo com o modelo 2 (mod2):\n\nAIC(mod2, mod4)\n\n     df      AIC\nmod2  6 727.0662\nmod4  8 699.3672\n\nanova(mod2, mod4, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sobreviveu ~ classe + sexo + idade + irco\nModel 2: sobreviveu ~ classe * sexo + idade + irco\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       909     715.07                          \n2       907     683.37  2   31.699 1.308e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nconclui-se que houve uma melhora significativa mod2 quando se inclui a interação.\nInterpretação das OR do modelo de interação\nComo foi visto anteriormente, o cálculo das Odds Ratios (OR) e seus respectivos Intervalos de Confiança (IC95%) é fundamental para interpretar o efeito de cada preditor.\n\nlibrary(questionr)\nodds.ratio(mod4)\n\n                         OR      2.5 %    97.5 %         p    \n(Intercept)      2.7050e+02 6.5093e+01 1908.7073 1.327e-11 ***\nclasse2          2.1376e-01 2.9620e-02    1.0416 0.0728904 .  \nclasse3          1.7338e-02 2.7062e-03    0.0613 8.721e-08 ***\nsexomasc         5.2195e-03 8.1811e-04    0.0183 3.287e-12 ***\nidade            9.6319e-01 9.4576e-01    0.9803 4.050e-05 ***\nirco             6.8753e-01 5.3719e-01    0.8499 0.0013586 ** \nclasse2:sexomasc 1.2792e+00 2.2264e-01   10.2411 0.7923217    \nclasse3:sexomasc 1.5144e+01 3.9122e+00  101.1695 0.0005955 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretação dos Efeitos Singulares (Significativos):\n\nIdade: OR = 0.963. Para cada aumento de um ano na idade, as chances de sobrevivência são 0.963 vezes menores (ou reduzem cerca de 3.7%), mantendo todas as outras variáveis constantes.\nclasse3 (vs. classe1, para Mulheres): OR = 0.0173. Para uma mulher na 3ª Classe, as chances de sobrevivência são 98% menores do que para uma mulher na 1ª Classe.\nsexomasc (vs. Feminino, na Classe 1): OR = 0.005. Para o homem na 1ª Classe, as chances de sobrevivência são mínimas em relação a uma mulher na 1ª Classe.\n\nInterpretação da Interação (classe * sexo)\nComo o termo de interação classe3:sexomasc é significativo, isso confirma que o efeito do sexo é diferente na Classe 3 em comparação com a Classe 1.\nO OR final para homens na Classe 3 (vs. Mulheres na Classe 1) é calculado multiplicando os ORs relevantes:\n\\[\n\\text{OR}_{\\text{H, C3 vs. M, C1}} = \\text{OR}_{\\text{C3}} \\times \\text{OR}_{\\text{H}} \\times \\text{OR}_{\\text{C3:H}}\n\\]\nUsando os resultados:\n\\[\n\\text{OR}_{\\text{H, C3 vs. M, C1}} \\approx 0.005 \\times 0.0173 \\times 15.1 \\approx {0.00131}\n\\]\nA OR de interação de 15.1 significa que a desvantagem extremamente alta de ser homem é parcialmente abrandada (multiplicada por 15.1) quando ele está na 3ª Classe. Apesar disso, o efeito total para um homem na 3ª Classe ainda representa uma chance de sobrevivência de quase zero comparada à mulher na 1ª Classe.\nPreditoras Não Significativas\nOs termos irco, classe2, e classe2:sexomasc não são estatisticamente significativos, pois seus IC95% incluem o valor 1.0. A diferença de chance de sobrevivência entre a 1ª Classe e a 2ª Classe (para mulheres) e a interação classe2:sexomasc não são estatisticamente significativas.\nO modelo mod4 é estatisticamente robusto, parcimonioso e possui uma Qualidade do Ajuste aceitável, representando um bom modelo . portanto, melhor modelo é aquele que inclui:\n\nclasse,\nidade,\nsexo,\nirco e\na interação entre classe e sexo.\n\n\n24.6.3.1 Tabela tab_model()\nTambém é possível comparar facilmente os modelos, usando o AIC com a função tab_model(), disponível no pacote sjPlot:\n\n\n\n\nTabela 24.1: Comparação entre o modelo sem interação e o Modelo com interação\n\n\n\n\n\n\n \nModelo sem interação\nModelo com interação\n\n\nPredictors\nOdds Ratios\nOdds Ratios\n\n\n(Intercept)\n47.79 ***\n270.50 ***\n\n\nclasse [2]\n0.38 ***\n0.21 \n\n\nclasse [3]\n0.13 ***\n0.02 ***\n\n\nsexo [masc]\n0.03 ***\n0.01 ***\n\n\nidade\n0.97 ***\n0.96 ***\n\n\nirco\n0.69 ***\n0.69 **\n\n\nclasse [2] × sexo [masc]\n\n1.28 \n\n\nclasse [3] × sexo [masc]\n\n15.14 ***\n\n\nObservations\n915\n915\n\n\nR2 Tjur\n0.490\n0.510\n\n\nAIC\n727.066\n699.367\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\nA Tabela 24.1 mostra a compação dos mod2 (modelo sem interação) e mod4 (modelo com interação). Observa-se que o AIC no modelo com interação é bem menor, indicando que interação melhorou o ajuste.\nO \\(R^2\\) Tjur (também chamado de coeficiente de discriminação) é um dos chamados pseudo-\\(R^2\\) usados em regressão logística. Ele mede o quanto o modelo consegue distinguir entre os grupos da variável resposta binária (por exemplo, quem sobreviveu vs. quem não sobreviveu). Proposto por Tjur (Tjur 2009), é uma alternativa simples e intuitiva aos pseudo-\\(R^2\\) de Cox & Snell, Nagelkerke ou McFadden (França 2023). A definição matemática é a diferença entre a média das probabilidades previstas para os casos com resposta = 1 (positivos), e a média das probabilidades previstas para os casos com resposta = 0 (negativos).\n\\[\n\\mathrm{R}_{Tjur}^{2} =\\overline{p}_{y=1}-\\overline{p}_{y=0}\n\\]\n\n# Probabilidades previstas\nprob &lt;- predict(mod4, type = \"response\")\n\n# Médias condicionais\nmean_p_y1 &lt;- mean(prob[treino$sobreviveu == 1], na.rm = TRUE)\nmean_p_y0 &lt;- mean(prob[treino$sobreviveu == 0], na.rm = TRUE)\n\nprint(c(mean_p_y1, mean_p_y0))\n\n[1] 0.6922983 0.1819453\n\n# R² Tjur\nR2_Tjur &lt;- mean_p_y1 - mean_p_y0\n\nR2_Tjur\n\n[1] 0.510353\n\n\nO modelo prevê em média 0.69 de probabilidade de sobrevivência para quem realmente sobreviveu e 0.18 para quem não sobreviveu, resultando num \\(\\mathrm{R}_{Tjur}^{2} = 0.51\\). Isso significa que o modelo tem uma boa capacidade de discriminar entre sobreviventes e não sobreviventes.\n\nSe o modelo não tem poder discriminativo, as médias das probabilidades previstas para os dois grupos serão muito próximas → \\(\\mathrm{R}_{Tjur}^{2}\\) ≈ 0.\nSe o modelo separa bem os grupos, a média das probabilidades para os positivos será alta e para os negativos será baixa → \\(\\mathrm{R}_{Tjur}^{2}\\) próximo de 1.\nValores típicos em regressão logística raramente chegam perto de 1; um \\(\\mathrm{R}_{Tjur}^{2}\\) de 0.3–0.5 já indica um modelo com boa capacidade discriminativa.\n\nPode-se calcular o \\(\\mathrm{R}_{Tjur}^{2}\\), usando a função r2_tjur() do pacote performance (Team 2025).\n\nlibrary(performance)\nr2_tjur(mod4)\n\nTjur's R2 \n 0.510353 \n\n\nIsto significa, como dito, uma boa capacidade de dicriminação. Entretanto, não confundir com o \\(R^2\\) da regressão linear, pois o \\(\\mathrm{R}_{Tjur}^{2}\\) mede a capacidade discriminação, ou seja, quão bem o modelo separa os dois grupos da resposta binária. Enquanto o \\(R^2\\) (Coeficiente de determinação), mede a proporção da variância da variável dependente que é explicada pelo modelo.\n\n\n\n24.6.4 Multicolinearidade\nA multicolinearidade ocorre quando duas ou mais variáveis preditoras estão altamente correlacionadas, o que infla o erro padrão dos coeficientes, tornando-os instáveis. Para verificar a multicolinearidade, pode-se usar o Fator de Inflação da Variância (VIF) através da função check_collinearity() do pacote performance, pois fornece análise mais clara da colinearidade em modelos GLM com interações quando comparada a função vif() do pacote car.\n\nlibrary(performance)\ncheck_collinearity(mod4)\n\nModel has interaction terms. VIFs might be inflated.\n  Try to center the variables used for the interaction, or check\n  multicollinearity among predictors of a model without interaction terms.\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n  Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n idade 1.27 [ 1.18,  1.39]     1.13      0.79     [0.72, 0.84]\n  irco 1.09 [ 1.04,  1.21]     1.05      0.91     [0.83, 0.96]\n\nHigh Correlation\n\n        Term   VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n      classe 31.88 [28.11, 36.18]     5.65      0.03     [0.03, 0.04]\n        sexo 13.99 [12.37, 15.84]     3.74      0.07     [0.06, 0.08]\n classe:sexo 47.81 [42.13, 54.28]     6.91      0.02     [0.02, 0.02]\n\n\nSe o VIF &lt; 5 geralmente é aceitável e o modelo deve ser mantido. Um VIF acima de 10 é uma preocupação significativa e deve ser considerada a remoção de uma das variáveis correlacionadas. Quando o VIF se encontra enter 5 e 10 é uma preocupalçao moderada e a decisão depende do contexto. Pode-se manter o modelo, mas estar ciente da instabilidade potencial.\nA interação classe * sexo gera redundância matemática com os termos principais, inflando os VIFs. Isso não invalida o modelo, mas indica que os coeficientes podem ser instáveis (mudam a amostra for alterada ou houver inclusão/exclusão de variáveis). O modelo ainda é utilizável, mas os coeficientes devem ser interpretados com cautela, especialmente os de classe e da interação classe:sexo.\nConcluindo e levando em consideração o valor do VIF ajustado:\n\nidade e irco \\(\\to\\) seguros, sem colinearidade (VIF ajustado &lt; 5).\n\nclasse, sexo e classe:sexo \\(\\to\\) apresentam colinearidade moderada/alta, mas ainda abaixo do limiar crítico (10).\n\nO modelo continua válido, mas os efeitos principais e de interação devem ser interpretados com cuidado.\n\n\n\n\n\n\nConclusão sobre os modelos\n\n\n\nForam ajustados dois modelos de regressão logística: um modelo simples, contendo apenas os efeitos principais, e um modelo com interação entre as variáveis classe e sexo. O modelo com interação apresentou melhor ajuste (menor AIC e deviance), além de indicar que o efeito do sexo depende da classe, especialmente na terceira classe, onde os homens tiveram probabilidade significativamente menor de sobrevivência. Contudo, a inclusão da interação elevou os valores de VIF, revelando colinearidade moderada a alta entre os termos principais e o termo de interação. Essa situação pode comprometer a estabilidade dos coeficientes e dificultar a interpretação direta dos efeitos. Por esse motivo, optou-se por apresentar ambos os modelos: o modelo simples, mais parcimonioso e estável, para interpretação dos efeitos principais; e o modelo com interação, para evidenciar a dependência entre as variáveis e o ganho em capacidade explicativa, ainda que com maior complexidade.\nO modelo com interação entre classe e sexo apresentou melhor ajuste estatístico, mas elevou os índices de colinearidade, indicando menor estabilidade dos coeficientes. Já o modelo simples mostrou-se mais parcimonioso e estável. Assim, ambos foram considerados: o modelo sem interação para interpretação dos efeitos principais e o modelo com interação para evidenciar dependências entre variáveis.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#avaliação-preditiva-com-os-dados-de-teste",
    "href": "24-reglogbin.html#avaliação-preditiva-com-os-dados-de-teste",
    "title": "24  Regressão Logística Binária",
    "section": "24.7 Avaliação preditiva com os dados de teste",
    "text": "24.7 Avaliação preditiva com os dados de teste\n\n24.7.1 Qualidade do modelo\nNormalmente, o objetivo da construção de um modelo é ser capaz de prever, com a maior acurácia possível, a variável resposta para novos dados. Uma boa maneira de avaliar a acurácia de um modelo é monitorar seu desempenho em novos dados (teste) e contar com que frequência ele prevê o resultado correto.\nSuponha que, se a probabilidade de o paciente sobreviver for inferior a 50%, será considerado que o resultado previsto é não sobrevivência; caso contrário, o resultado previsto é sucesso, o paciente sobreviveu.\nInicialmente, constroi-se uma tabela de contingência comparando o desempenho do modelo, usando os dados observados nos dados teste:\n\n# Criar um vetor de probabilidades previstas no dataframe teste\nprob_mod2 &lt;- predict(mod2, newdata = teste, type = \"response\")\nprob_mod4 &lt;- predict(mod4, newdata = teste, type = \"response\")\n\n# Classificação binária (ponto de corte 0.5) ----\npred_mod2 &lt;- ifelse(prob_mod2 &gt;= 0.5, 1, 0)\npred_mod4 &lt;- ifelse(prob_mod4 &gt;= 0.5, 1, 0)\n\n# Transformar as predições em fatores e colocar rótulos\npred_mod2  &lt;- factor(pred_mod2,\n                     levels = c(0, 1))\npred_mod4  &lt;- factor(pred_mod4,\n                     levels = c(0, 1))\n\n# Comparar o resultado observado com o resultado previsto\ntab2 &lt;- table(observado = teste$sobreviveu,\n              previsto  = pred_mod2)\ntab4 &lt;- table(observado = teste$sobreviveu,\n              previsto  = pred_mod4)\n\n# Mostrar resultados\ntab2\n\n         previsto\nobservado   0   1\n        0 228  22\n        1  31 110\n\ntab4\n\n         previsto\nobservado   0   1\n        0 233  17\n        1  34 107\n\n\nA partir da tabela de contingência (tab2) dos resultados previstos e observados, verifica-se que o modelo 2:\n\npreviu corretamente que 228 passageiros que não sobreviveram.\n\npreviu incorretamente 31 passageiros que não sobreviveram.\n\npreviu incorretamente que 17 passageiros que sobreviveram, e\n\npreviu corretamente 110 passageiros que sobreviveram.\n\nA partir da tabela de contingência (tab4) dos resultados previstos e observados, verifica-se que o modelo 4:\n\npreviu corretamente que 233 passageiros que não sobreviveram.\n\npreviu incorretamente 34 passageiros que não sobreviveram.\n\npreviu incorretamente que 17 passageiros que sobreviveram, e\n\npreviu corretamente 107 passageiros que sobreviveram.\n\nA porcentagem de previsões corretas, também chamada de acurácia, é a soma das previsões corretas (verdadeiros positivos + verdadeiros negativos) dividida pelo número total de previsões:\n\\[\nacurácia = \\frac{VP + VN}{VP + VN + FP + FN}\n\\]\nUsando a tabela de contingência tab2:\n\nacuracia2 &lt;- sum(diag(tab2)) / sum(tab2)\nacuracia2\n\n[1] 0.8644501\n\n\nO modelo tem uma alta acurácia (86%), isso indica um bom desempenho geral. Um modelo é considerado razoavelmente bom se a acurácia do modelo for superior a 70%.\nUsando a tabela de contingência tab4:\n\nacuracia4 &lt;- sum(diag(tab4)) / sum(tab4)\nacuracia4\n\n[1] 0.8695652\n\n\nO modelo tem uma alta acurácia (87%), isso indica um bom desempenho geral.\nAmbos modelos apresentam uma boa acurácia!\nEmbora a acurácia seja a maneira mais intuitiva e fácil de medir o desempenho preditivo de um modelo, ela apresenta algumas desvantagens, principalmente porque há necessidade de escolher um limiar arbitrário (aqui foi escolhido 50%) a partir do qual uma nova observação é classificada como 1 ou 0. Poderia ser escolhido outro limite e os resultados seriam diferentes!\nUsando uma matriz de confusão, através da função confusionMatrix() do pacote caret pode-se calcular a acurácia e outras métricas de avaliação da qualidade (validade) do modelo :\n\nlibrary(caret)\nconf_matrix2 &lt;- confusionMatrix(pred_mod2, teste$sobreviveu,\n                positive = \"1\")\nprint(conf_matrix2)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 228  31\n         1  22 110\n                                          \n               Accuracy : 0.8645          \n                 95% CI : (0.8265, 0.8968)\n    No Information Rate : 0.6394          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7019          \n                                          \n Mcnemar's Test P-Value : 0.2718          \n                                          \n            Sensitivity : 0.7801          \n            Specificity : 0.9120          \n         Pos Pred Value : 0.8333          \n         Neg Pred Value : 0.8803          \n             Prevalence : 0.3606          \n         Detection Rate : 0.2813          \n   Detection Prevalence : 0.3376          \n      Balanced Accuracy : 0.8461          \n                                          \n       'Positive' Class : 1               \n                                          \n\nconf_matrix4 &lt;- confusionMatrix(pred_mod4, teste$sobreviveu,\n                positive = \"1\")\nprint(conf_matrix4)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 233  34\n         1  17 107\n                                          \n               Accuracy : 0.8696          \n                 95% CI : (0.8321, 0.9013)\n    No Information Rate : 0.6394          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.7095          \n                                          \n Mcnemar's Test P-Value : 0.02506         \n                                          \n            Sensitivity : 0.7589          \n            Specificity : 0.9320          \n         Pos Pred Value : 0.8629          \n         Neg Pred Value : 0.8727          \n             Prevalence : 0.3606          \n         Detection Rate : 0.2737          \n   Detection Prevalence : 0.3171          \n      Balanced Accuracy : 0.8454          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nA Sensibilidade (Recall) é a proporção de verdadeiros positivos (VP) identificados corretamente. . É obtida pela fórmula:\n\\[\nsensibilidade = \\frac{VP}{VP + FN}\n\\]\nO mod2 identifica corretamete 70,2% e o mod4 75,9% dos verdadeiros positivos. Isto significa que apresentam 29,8% e 24,1% de falsos negativos. São valores que dependendo do contexto podem exigir atenção, principalmente na área da saúde. Entre os modelos, o mod4 gera menos falsos negativos, houve um pequeno avanço. A análise deve ser feita em conjunto com outras métricas.\nA Especificidade é a proporção de verdadeiros negativos(VN). A fórmula para calcular a especificidade é:\n\\[\nespecificidade = \\frac{VN}{FP + VN}\n\\]\nO mod2 reconhece 91.2% dos verdadeiros negativos e o mod4, 93,2%. Ambos tem uma baixa taxa de falos positivos.\nO Valor P do Teste de McNemar testa se há viés significativo na classificação dos erros. O valor alto (p = 0.2718), no mod2 sugere que os erros entre classes não são estatisticamente diferentes. Já no mod4, o valor p &lt; 0.05, indica que ele não é neutro nos erros, não erra de forma aleatória, mas sim enviesada. Isso pode ser problemático quando os falsos negativos forem graves (por ex.: área da saúde) e aí o modelo pode ser inadequado. Isso não invalida o mod4, mas mostra que ele precisa ser avaliado com cuidado quanto ao tipo de erro que gera. Avaliar outras métricas junto (sensibilidade, especificidade, AUC).\nO Kappa mede concordância ajustada para a distribuição dos dados. Um valor próximo de 1 indica boa concordância entre previsões e valores reais.\nO escore F1 (não listado diretamente) representa o equilíbrio entre precisão e recall. Pode ser obtido pela equação:\n\\[\nF_{1} = 2 \\times \\frac{VPP \\times sensibilidade}{VPP + sensibilidade}\n\\]\nOu consultando:\n\nconf_matrix2$byClass[7]\n\n       F1 \n0.8058608 \n\nconf_matrix4$byClass[7]\n\n       F1 \n0.8075472 \n\n\nAmbos os modelos têm \\(F_{1}\\approx 0.81\\), o que indica bom equilíbrio entre sensibilidade e precisão. A diferença entre eles é mínima (0.0017), mostrando que em termos de qualidade preditiva global, os dois modelos são praticamente equivalentes.\nA Precisão é a proporção de previsões positivas que estavam corretas, também denominada de valor preditivo positivo (VPP). A precisão do mod2 é igual 83.3% e do mod4 86.3%, ambas são consideradas valores altos.\n\n\n24.7.2 AUC e curva ROC\nOutra forma comum de avaliar a qualidade de um modelo é calcular a AUC (Área Sob a Curva) e traçar a curva ROC (Característica de Operação do Receptor). A curva ROC mostra a relação entre sensibilidade(recall) e 1 - especificidade (FP) para diferentes limiares de classificação. A AUC resume a curva ROC em um único número (quanto mais próximo de 1, melhor o modelo). A curva ROC/AUC pode ser calculada com os dados treino, mas tende a ser otimista, já que o modelo foi ajustado nesses dados. O ideal é usar os dados teste (ou de validação), porque mede o desempenho em dados não vistos, refletindo a capacidade de generalização do modelo.\nIsso pode ser facilmente alcançado graças à função roc() e ggroc() do pacote pROC associado ao ggplot2:\n\nlibrary(pROC)\nlibrary(ggplot2)\n\n# Gerar objetos ROC\nroc_mod2 &lt;- roc(teste$sobreviveu, prob_mod2)\nroc_mod4 &lt;- roc(teste$sobreviveu, prob_mod4)\n\n# Calcular AUCs\nauc_mod2 &lt;- auc(roc_mod2)\nauc_mod4 &lt;- auc(roc_mod4)\n\n# Lista para ggroc\nroc_list &lt;- list(\"Modelo simples\" = roc_mod2,\n                 \"Modelo interação\" = roc_mod4)\n\n# Plot com ggplot2\nggroc(roc_list, legacy.axes = TRUE) +\n  labs(title = \"Curvas ROC\",\n       x = \"1 - Especificidade (Taxa de Falsos Positivos)\",\n       y = \"Sensibilidade (Taxa de Verdadeiros Positivos)\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank()) +\n  annotate(\"text\", x = 0.6, y = 0.2,\n           label = paste0(\"AUC simples: \", round(auc_mod2, 3)),\n           color = \"blue\", size = 4) +\n  annotate(\"text\", x = 0.6, y = 0.1,\n           label = paste0(\"AUC interação: \", round(auc_mod4, 3)),\n           color = \"red\", size = 4) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 24.9: Comparação do modelo simples com o modelo com interação\n\n\n\n\n\nA AUC (Área sob a Curva ROC) indica a qualidade do modelo (veja também a Seção 21.6):\n\n\\(AUC \\approx 0.5\\): O modelo não tem poder preditivo (equivalente ao puro acaso).\n\\(AUC &gt; 0.7\\): Modelo razoável.\n\\(AUC &gt; 0.8\\): Modelo muito bom.\n\\(AUC &gt; 0.9\\): Excelebte desempenho\n\nUsando as métricas de validade, a curva ROC e a AUC, conclui-se que:\nApós a avaliação dos modelos ajustados, observou-se que o modelo com interação entre classe e sexo (mod4) apresentou melhor ajuste estatístico, com menor AIC e deviance, além de capturar nuances importantes entre as variáveis. No entanto, esse modelo também apresentou colinearidade moderada e padrão de erro enviesado, conforme indicado pelo teste de McNemar (p &lt; 0,05), o que pode comprometer a estabilidade dos coeficientes e a neutralidade preditiva. Por outro lado, o modelo simples (mod2) demonstrou desempenho preditivo praticamente equivalente, com AUC ligeiramente superior (0,919 vs. 0,918), menor colinearidade e maior robustez interpretativa. Diante disso, optou-se por priorizar o modelo simples para fins de interpretação e comunicação dos resultados, mantendo o modelo com interação como referência complementar para análise de padrões específicos\n\n\n24.7.3 Comparar duas curvas ROC (treino vs teste)\n\nlibrary(pROC)\nlibrary(ggplot2)\n\n# Probabilidades previstas no treino e teste\nprob_treino &lt;- predict(mod4, newdata = treino, type = \"response\")\nprob_teste  &lt;- predict(mod4, newdata = teste, type = \"response\")\n\n# Curvas ROC\nroc_treino &lt;- roc(treino$sobreviveu, prob_treino)\nroc_teste  &lt;- roc(teste$sobreviveu, prob_teste)\n\n# AUC e IC95% para treino\nauc_treino &lt;- auc(roc_treino)\nci_treino  &lt;- ci.auc(roc_treino)\n\n# AUC e IC95% para teste\nauc_teste &lt;- auc(roc_teste)\nci_teste  &lt;- ci.auc(roc_teste)\n\n# Criar textos para anotação\ntexto_treino &lt;- paste0(\"Treino: AUC = \", round(auc_treino, 3),\n                       \" (IC95%: \", round(ci_treino[1], 3), \"-\",\n                       round(ci_treino[3], 3), \")\")\n\ntexto_teste &lt;- paste0(\"Teste: AUC = \", round(auc_teste, 3),\n                      \" (IC95%: \", round(ci_teste[1], 3), \"-\",\n                      round(ci_teste[3], 3), \")\")\n\n# Plotar ambas curvas ROC com ggroc\nggroc(list(Treino = roc_treino, Teste = roc_teste), aes = c(\"color\")) +\n  theme_minimal() +\n  labs(title = \"Curvas ROC - Treino vs Teste\",\n       y = \"Sensibilidade\",\n       x = \"1 - Especificidade\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  annotate(\"text\", x = 0.6, y = 0.2, label = texto_treino,\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 0.6, y = 0.1, label = texto_teste,\n           hjust = 0, size = 4, color = \"red\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 24.10: Comparação do modelo simples com o modelo com interação\n\n\n\n\n\nAlém de observar visualmente, pode-se realizar um teste estatístico, o Teste de Delong (DeLong, DeLong, e Clarke-Pearson 1988), para verificar se a diferença entre as AUCs das curvas tem significância estatística:\n\nlibrary(pROC)\n\n# Probabilidades previstas no treino e teste\nprob_treino &lt;- predict(mod4, newdata = treino, type = \"response\")\nprob_teste  &lt;- predict(mod4, newdata = teste, type = \"response\")\n\n# Curvas ROC\nroc_treino &lt;- roc(treino$sobreviveu, prob_treino)\nroc_teste  &lt;- roc(teste$sobreviveu, prob_teste)\n\n# Comparação estatística com teste de DeLong\nteste_delong &lt;- roc.test(roc_treino, roc_teste, method = \"delong\")\n\n# Resultado\nteste_delong\n\n\n    DeLong's test for two ROC curves\n\ndata:  roc_treino and roc_teste\nD = -1.4695, df = 883.58, p-value = 0.1421\nalternative hypothesis: true difference in AUC is not equal to 0\nsample estimates:\nAUC of roc1 AUC of roc2 \n  0.8884194   0.9179291 \n\n\nA saída do teste retorna um valor p = 0.1421 (&lt; 0.05) indicando que a diferença entre os AUCs é estatisticamente não significativa. Os modelo apresentam AUCs semelhantes em ambos dados.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#relato-dos-resultados",
    "href": "24-reglogbin.html#relato-dos-resultados",
    "title": "24  Regressão Logística Binária",
    "section": "24.8 Relato dos resultados",
    "text": "24.8 Relato dos resultados\n\nPara avaliar a capacidade de generalização do modelo, foram comparadas as curvas ROC obtidas nos conjuntos de treino e teste. O modelo apresentou AUC de 0,888 no treino (IC95%: 0,864–0,913) e AUC de 0,918 no teste (IC95%: 0,887–0,949), indicando desempenho superior nos dados não vistos. A sobreposição dos intervalos de confiança sugere que a diferença pode não ser estatisticamente significativa, mas reforça a robustez do modelo. O Teste de Delong confirma que a diferença não é estatisticamente significativa (p = 0.1421), A curva ROC do teste posicionou-se mais próxima do canto superior esquerdo, evidenciando maior capacidade discriminativa e ausência de sobreajuste (overfitting). Esses resultados demonstram que o modelo é confiável e apresenta boa performance preditiva fora da amostra de treinamento.\n\nPode-se também relatar os resultados de uma regressão logística binária usando uma tabela (Tabela 24.2) gerada, por exemplo, pela função tbl_regression() do pacote gtsummary ?sec-gtsummary.\n\nlibrary(gtsummary)\nlibrary(purrr)  # para partial, opcional\n\ntbl_regression(\n  mod4,\n  exponentiate = TRUE,\n  label = list(\n    classe ~ \"Classe\",\n    sexo   ~ \"Sexo\",\n    idade  ~ \"Idade\",\n    `classe:sexo` ~ \"Interação Classe × Sexo\"),\n  estimate_fun = partial(style_sigfig, digits = 3)) %&gt;%\n  modify_header(\n    label ~ \"**Variável**\",\n    p.value ~ \"**Valor p**\") \n\n\n\nTabela 24.2: Resultados da Regressão Logística Binária - modelo com interação\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariável\nOR\n95% CI\nValor p\n\n\n\n\nClasse\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n0.214\n0.030, 1.04\n0.073\n\n\n    3\n0.017\n0.003, 0.061\n&lt;0.001\n\n\nSexo\n\n\n\n\n\n\n\n\n    fem\n—\n—\n\n\n\n\n    masc\n0.005\n0.001, 0.018\n&lt;0.001\n\n\nIdade\n0.963\n0.946, 0.980\n&lt;0.001\n\n\nirco\n0.688\n0.537, 0.850\n0.001\n\n\nInteração Classe × Sexo\n\n\n\n\n\n\n\n\n    2 * masc\n1.28\n0.223, 10.2\n0.8\n\n\n    3 * masc\n15.1\n3.91, 101\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#exercícios",
    "href": "24-reglogbin.html#exercícios",
    "title": "24  Regressão Logística Binária",
    "section": "24.9 Exercícios",
    "text": "24.9 Exercícios\n\n\n\n\n\n\nExercício 1\n\n\n\nPara fins ilustrativos, suponha-se agora que se deseja prever a probabilidade de um “novo” passageiro sobreviver. Esse passageiro seja uma mulher de 35 anos, que comprou bilhete de viagem na terceira classe e viaja com o marido.\nSolução\n\n# Criar um novo dataframe para o novo passageiro\nnovo_passageiro1 &lt;- data.frame(\n  idade = 35,\n  sexo = factor(\"fem\", levels = levels(teste$sexo)),\n  classe = factor(3, levels = levels(teste$classe)),\n  irco = 1)\n\n# Predição com erro padrão no modelo simples\npred_mod2 &lt;- predict(mod2, newdata = novo_passageiro1, type = \"link\", se.fit = TRUE)\n\n# Intervalo de confiança no espaço do logit\nic_logit_mod2 &lt;- c(\n  pred_mod2$fit - 1.96 * pred_mod2$se.fit,\n  pred_mod2$fit + 1.96 * pred_mod2$se.fit)\n\n# Transformar para probabilidade\nic_prob_mod2 &lt;- plogis(ic_logit_mod2)\npred_prob_mod2 &lt;- plogis(pred_mod2$fit)\n\n# Predição com erro padrão no modelo com interação\npred_mod4 &lt;- predict(mod4, newdata = novo_passageiro1, type = \"link\", se.fit = TRUE)\n\nic_logit_mod4 &lt;- c(\n  pred_mod4$fit - 1.96 * pred_mod4$se.fit,\n  pred_mod4$fit + 1.96 * pred_mod4$se.fit)\n\nic_prob_mod4 &lt;- plogis(ic_logit_mod4)\npred_prob_mod4 &lt;- plogis(pred_mod4$fit)\n\n# Mostrar resultados, usando a função cat() e sprintf()\n# Modelo simples (mod2)\ncat(sprintf(\"Modelo simples (mod2): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod2, ic_prob_mod2[1], ic_prob_mod2[2]))\n\nModelo simples (mod2): 0.585 IC95%: 0.494 0.672\n\n# Modelo com interação (mod4)\ncat(sprintf(\"Modelo com interação (mod4): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod4, ic_prob_mod4[1], ic_prob_mod4[2]))\n\nModelo com interação (mod4): 0.465 IC95%: 0.365 0.567\n\n\n\n\n\n\n\n\n\n\nExercício 2\n\n\n\nCalcular a probabilidade de sobreviver do esposo dessa passageira. Ele tem 40 anos.\nSolução\n\n# Criar um novo dataframe para o novo passageiro\nnovo_passageiro2 &lt;- data.frame(\n  idade = 40,\n  sexo = factor(\"masc\", levels = levels(teste$sexo)),\n  classe = factor(3, levels = levels(teste$classe)),\n  irco = 1)\n\n# Predição com erro padrão no modelo simples\npred_mod2 &lt;- predict(mod2, newdata = novo_passageiro2, type = \"link\", se.fit = TRUE)\n\n# Intervalo de confiança no espaço do logit\nic_logit_mod2 &lt;- c(\n  pred_mod2$fit - 1.96 * pred_mod2$se.fit,\n  pred_mod2$fit + 1.96 * pred_mod2$se.fit)\n\n# Transformar para probabilidade\nic_prob_mod2 &lt;- plogis(ic_logit_mod2)\npred_prob_mod2 &lt;- plogis(pred_mod2$fit)\n\n# Predição com erro padrão no modelo com interação\npred_mod4 &lt;- predict(mod4, newdata = novo_passageiro2, type = \"link\", se.fit = TRUE)\n\nic_logit_mod4 &lt;- c(\n  pred_mod4$fit - 1.96 * pred_mod4$se.fit,\n  pred_mod4$fit + 1.96 * pred_mod4$se.fit)\n\nic_prob_mod4 &lt;- plogis(ic_logit_mod4)\npred_prob_mod4 &lt;- plogis(pred_mod4$fit)\n\n# Mostrar resultados, usando a função cat() e sprintf()\n# Modelo simples (mod2)\ncat(sprintf(\"Modelo simples (mod2): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod2, ic_prob_mod2[1], ic_prob_mod2[2]))\n\nModelo simples (mod2): 0.037 IC95%: 0.023 0.059\n\n# Modelo com interação (mod4)\ncat(sprintf(\"Modelo com interação (mod4): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod4, ic_prob_mod4[1], ic_prob_mod4[2]))\n\nModelo com interação (mod4): 0.054 IC95%: 0.034 0.085\n\n\n\n\n\n\n\n\n\n\nExercício 3\n\n\n\nSuponha-se que se deseja prever a probabilidade de um “novo” passageiro sobreviver. Esse passageiro é uma mulher de 35 anos, que comprou bilhete de viagem na primeira classe e viaja com o marido\nSolução\n\n# Criar um novo dataframe para o novo passageiro\nnovo_passageiro3 &lt;- data.frame(\n  idade = 35,\n  sexo = factor(\"fem\", levels = levels(teste$sexo)),\n  classe = factor(1, levels = levels(teste$classe)),\n  irco = 1)\n\n# Predição com erro padrão no modelo simples\npred_mod2 &lt;- predict(mod2, newdata = novo_passageiro3, type = \"link\", se.fit = TRUE)\n\n# Intervalo de confiança no espaço do logit\nic_logit_mod2 &lt;- c(\n  pred_mod2$fit - 1.96 * pred_mod2$se.fit,\n  pred_mod2$fit + 1.96 * pred_mod2$se.fit)\n\n# Transformar para probabilidade\nic_prob_mod2 &lt;- plogis(ic_logit_mod2)\npred_prob_mod2 &lt;- plogis(pred_mod2$fit)\n\n# Predição com erro padrão no modelo com interação\npred_mod4 &lt;- predict(mod4, newdata = novo_passageiro3, type = \"link\", se.fit = TRUE)\n\nic_logit_mod4 &lt;- c(\n  pred_mod4$fit - 1.96 * pred_mod4$se.fit,\n  pred_mod4$fit + 1.96 * pred_mod4$se.fit)\n\nic_prob_mod4 &lt;- plogis(ic_logit_mod4)\npred_prob_mod4 &lt;- plogis(pred_mod4$fit)\n\n# Mostrar resultados, usando a função cat() e sprintf()\n# Modelo simples (mod2)\ncat(sprintf(\"Modelo simples (mod2): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod2, ic_prob_mod2[1], ic_prob_mod2[2]))\n\nModelo simples (mod2): 0.914 IC95%: 0.870 0.944\n\n# Modelo com interação (mod4)\ncat(sprintf(\"Modelo com interação (mod4): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod4, ic_prob_mod4[1], ic_prob_mod4[2]))\n\nModelo com interação (mod4): 0.980 IC95%: 0.924 0.995\n\n\n\n\n\n\n\n\n\n\nExercício 4\n\n\n\nQuala a probabilidade de sobreviver do esposo, 40 anos, da mulher do esxercício 3?\nSolução\n\n# Criar um novo dataframe para o novo passageiro\nnovo_passageiro4 &lt;- data.frame(\n  idade = 40,\n  sexo = factor(\"masc\", levels = levels(teste$sexo)),\n  classe = factor(1, levels = levels(teste$classe)),\n  irco = 1)\n\n# Predição com erro padrão no modelo simples\npred_mod2 &lt;- predict(mod2, newdata = novo_passageiro4, type = \"link\", se.fit = TRUE)\n\n# Intervalo de confiança no espaço do logit\nic_logit_mod2 &lt;- c(\n  pred_mod2$fit - 1.96 * pred_mod2$se.fit,\n  pred_mod2$fit + 1.96 * pred_mod2$se.fit)\n\n# Transformar para probabilidade\nic_prob_mod2 &lt;- plogis(ic_logit_mod2)\npred_prob_mod2 &lt;- plogis(pred_mod2$fit)\n\n# Predição com erro padrão no modelo com interação\npred_mod4 &lt;- predict(mod4, newdata = novo_passageiro4, type = \"link\", se.fit = TRUE)\n\nic_logit_mod4 &lt;- c(\n  pred_mod4$fit - 1.96 * pred_mod4$se.fit,\n  pred_mod4$fit + 1.96 * pred_mod4$se.fit)\n\nic_prob_mod4 &lt;- plogis(ic_logit_mod4)\npred_prob_mod4 &lt;- plogis(pred_mod4$fit)\n\n# Mostrar resultados, usando a função cat() e sprintf()\n# Modelo simples (mod2)\ncat(sprintf(\"Modelo simples (mod2): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod2, ic_prob_mod2[1], ic_prob_mod2[2]))\n\nModelo simples (mod2): 0.226 IC95%: 0.164 0.303\n\n# Modelo com interação (mod4)\ncat(sprintf(\"Modelo com interação (mod4): %.3f IC95%%: %.3f %.3f\\n\", \n            pred_prob_mod4, ic_prob_mod4[1], ic_prob_mod4[2]))\n\nModelo com interação (mod4): 0.178 IC95%: 0.119 0.257\n\n\n\n\n\n\n\n\n\n\nComentário sobre os exercícios\n\n\n\nFoi verificado que os modelos são estatisticamente válidos. Isso dá segurança para confiar nas estimativas geradas. Mesmo que os modelos mod2 (simples) e mod4 (com interação) tenham dado estimativas diferentes de sobrevivência, isso não quer dizer que essas diferenças sejam realmente significativas. Quando se observa os intervalos de confiança de 95% na Tabela 24.3, ve-se que eles se sobrepõem em todos os casos. Isso quer dizer que, apesar de um modelo prever uma chance um pouco maior ou menor, essa diferença está dentro da margem de erro — ou seja, pode ter acontecido só por variação natural dos dados. O modelo com interação até se saiu melhor no teste geral (aquele que compara o desempenho dos dois), mas quando se olha para cada passageiro individual, não dá pra afirmar com segurança que um modelo está “mais certo” que o outro. Por isso, é importante considerar não só o número da previsão, mas também o intervalo de confiança que vem junto com ela\n\n\n\n\nTabela 24.3: Previsões com os modelos simples e com interação\n\n\n\nExercícioIdadeSexoClasseNº irmãos/cônjugesModeloEstimativaIC95%135fem3ª1mod20.5850.494–0.67240masc1ª1mod40.4650.365–0.567235fem3ª1mod20.0370.023–0.05940masc1ª1mod40.0540.034, 0.085335fem3ª1mod20.9140.870–0.94440masc1ª1mod40.9800.924–0.995435fem3ª1mod20.2360.164–0.30340masc1ª1mod40.1780.119–0.217\n\n\n\n\n\n\n\nIt’s frightening, but that’s roughly what happened. It’s the voice of numbers!\n\n\n\n\n\n\nAnderson, Benjamim. 2023. «Como interpretar coeficientes de regressão logística (com exemplo)». https://statorials.org/pt/interpretar-coeficientes-de-regressao-logistica/.\n\n\nBarnier, Julien, François Briatte, e Joseph Larmarange. 2025. questionr: Functions to Make Surveys Processing Easier. doi:10.32614/CRAN.package.questionr.\n\n\nCox, David R. 1958. «The regression analysis of binary sequences». Journal of the Royal Statistical Society Series B: Statistical Methodology 20 (2). Oxford University Press: 215–32.\n\n\nDeLong, Elizabeth R, David M DeLong, e Daniel L Clarke-Pearson. 1988. «Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach». Biometrics 44. JSTOR: 837–45.\n\n\nField, Andy, Jeremy Miles, e Zoë Field. 2012. «Logistic regression». Em Discovering statistics using R, 320. Sage Publications, Ltd.\n\n\nFrança, A. 2023. «Regressão logística: Pseudo R²». https://www.blog.psicometriaonline.com.br/regressao-logistica-pseudo-r2/.\n\n\nPrabhakaran, Selva. 2016. «Missing Value Treatment». datascienceplus. https://datascienceplus.com/missing-value-treatment/.\n\n\nSubramanian, Jyothi, e Richard Simon. 2013. «Overfitting in prediction models–is it a problem only in high dimensions?» Contemporary clinical trials 36 (2). Elsevier: 636–41.\n\n\nTeam, Easystats. 2025. «R2 Tjur — performance package reference». https://easystats.github.io/performance/reference/r2_tjur.html.\n\n\nTjur, Tue. 2009. «Coefficients of determination in logistic regression models—A new proposal: The coefficient of discrimination». The American Statistician 63 (4). Taylor & Francis: 366–72.\n\n\nWikipédia. 2025. «RMS Titanic — Wikipédia, a enciclopédia livre». https://pt.wikipedia.org/w/index.php?title=RMS_Titanic&oldid=69869298.",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "24-reglogbin.html#footnotes",
    "href": "24-reglogbin.html#footnotes",
    "title": "24  Regressão Logística Binária",
    "section": "",
    "text": "Lembre-se que o diretório mostrado no comando é o do autor, você deverá usar o do seu computador↩︎\nDeve-se fazer isso, pois algumas vezes, o R não interpreta espaços vazios como NA.↩︎",
    "crumbs": [
      "Parte IX - Bioestatística Aplicada à Epidemiologia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regressão Logística Binária</span>"
    ]
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referências",
    "section": "",
    "text": "Altman, Douglas G. 1991a. “Comparing Groups: Categorical\nData.” In Practical Statistics for Medical Research,\n244–47. London: Chapman & Hall/CRC.\n\n\n———. 1991b. “Comparing Groups: Continuos Data.” In\nPractical Statistics for Medical Research, 194–97. London:\nChapman & Hall/CRC.\n\n\n———. 1991c. “One Way Analysis of Variance.” In\nPractical Statistics for Medical Research, 206–9. London:\nChapman & Hall/CRC.\n\n\n———. 1994. “The Scandal of Poor Medical Research.”\nBmj. British Medical Journal Publishing Group.\n\n\n———. 2005. “Why We Need Confidence Intervals.” World J.\nSurg 29: 554–56.\n\n\nAltman, Douglas G, and J Martin Bland. 1994a. “Diagnostic Tests 2:\nPredictive Values.” Bmj 309 (6947). British Medical\nJournal Publishing Group: 102.\n\n\n———. 1994b. “Diagnostic Tests 3: Receiver Operating Characteristic\nPlots.” BMJ: British Medical Journal 309 (6948). BMJ\nPublishing Group: 188.\n\n\n———. 1994c. “Diagnostic Tests. 1: Sensitivity and\nSpecificity.” BMJ: British Medical Journal 308 (6943).\nBMJ Publishing Group: 1552.\n\n\nAltman, Douglas G, and Martin J Gardner. 1988. “Statistics in\nMedicine: Calculating Confidence Intervals for Regression and\nCorrelation.” British Medical Journal (Clinical Research\nEd.) 296 (6631). BMJ Publishing Group: 1238.\n\n\nAnderson, Benjamim. 2023. “Como Interpretar Coeficientes de\nRegressão Logística (Com Exemplo).” https://statorials.org/pt/interpretar-coeficientes-de-regressao-logistica/.\n\n\nAnunciação, Luis. 2021. “Conceitos e Análises Estatisticas Com r e\nJasp.” ANOVA de Medidas Repetidas. https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html.\n\n\nAragon, Tomas J. 2020. “Epitools: Epidemiology Tools.” https://CRAN.R-project.org/package=epitools.\n\n\nArango, Hector Gustavo. 2009. “Organização Dos Dados Em\nTabelas.” In Bioestatística: Teórica e Computacional, 3ª\nedição, 32–57. Guanabara Koogan.\n\n\nArmitage, Peter, Geoffrey Berry, and John Nigel Scott Matthews. 2002.\n“Checking the Model.” In Statistical Methods in Medical\nResearch, Fourth Edition, 356–75. Blackwell Science.\n\n\n———. 2008. Statistical Methods in Medical Research. John Wiley\n& Sons.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A\nForward-Pipe Operator for r.\n\n\nBarnier, Julien, François Briatte, and Joseph Larmarange. 2025.\nQuestionr: Functions to Make Surveys Processing Easier. doi:10.32614/CRAN.package.questionr.\n\n\nBarton, Belinda, and Jennifer Peat. 2014. “Analysis of\nVariance.” In Medical Statistics: A Guide to SPSS, Data\nAnalysis and Critical Appraisal, Second Edition, 112–16. John Wiley\n& Sons Ltd.\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression\nDiagnostics: Identifying Influential Data and Sources of\nCollinearity. Wiley Series in Probability and Statistics. New York,\nNY: John Wiley & Sons.\n\n\nBender, Ralf. 2001. “Calculating Confidence Intervals for the\nNumber Needed to Treat.” Controlled Clinical Trials 22\n(2). Elsevier: 102–10.\n\n\nBen-Shachar, Mattan S, Daniel Lüdecke, and Dominique Makowski. 2020.\n“Effectsize: Estimation of Effect Size Indices and Standardized\nParameters.” Journal of Open Source Software 5 (56):\n2815.\n\n\nBland, J Martin, and Douglas G Altman. 1994. “Statistic Notes:\nRegression Towards the Mean.” BMJ 308 (6942). British\nMedical Journal Publishing Group: 1499.\n\n\n———. 2004. “The Logrank Test.” BMJ 328 (7447).\nBritish Medical Journal Publishing Group: 1073.\n\n\nBland, Martin. 2015. In An Introduction to Mrdical Statistics,\nFourth Edition, 145–47. Oxford University Press.\n\n\nBlog, The Jumping Rivers. 2024. “Positron Vs Rstudio – Is It Time\nto Switch?” R-Bloggers. https://www.r-bloggers.com/2024/12/positron-vs-rstudio-is-it-time-to-switch/.\n\n\nBorges, Leonardo Silva Roever. 2016. “Diagnostic Accuracy Measures\nin Cardiovascular Research.” Int J Cardiovasc Sci 29\n(3): 218–22.\n\n\nBowers, David. 2008. “First Things First-the Nature of\nData.” In Medical Statistics from Scratch, Second\nEdition, 3–13. John Wiley; Sons.\n\n\n“Breve História Dos Censos.” 2014. Instituto Nacional\nde Estatistica. Statistics Portugal. https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria.\n\n\nBurattini, Marcelo N. 2004. “Raciocínio Médico e\nInferência.” In Métodos Quantitativos Em Medicina,\nedited by Eduardo Massad, Paulo SP Silveira, Renèe X de Menezes, and\nOthers, 208–23. São Paulo, SP: Editora Manole.\n\n\nCallegari-Jacques, Sidia M. 2003a. “Amostras.” In\nBioestatistica: Principios e Aplicações, 146–47. Artmed\nEditora.\n\n\n———. 2003b. “Distribuição Amostral Das Médias.” In\nBioestatistica: Principios e Aplicações, 47–53. Artmed Editora.\n\n\nCaraguel, Charles GB, and Raphael Vanderstichel. 2013. “The\nTwo-Step Fagan’s Nomogram: Ad Hoc Interpretation of a Diagnostic Test\nResult Without Calculation.” BMJ Evidence-Based Medicine\n18 (4). Royal Society of Medicine: 125–28.\n\n\nCelentano, David D, and Moyses Szklo. 2019. “Cohort\nStudies.” In Gordis Epidemiology, 6th Edition, 179.\nElsevier.\n\n\nChang, Winston. 2021. “Cookbook for r.” Cookbook for R.\nhttp://www.cookbook-r.com.\n\n\nChristensen, Erik. 2007. “Methodology of Superiority Vs.\nEquivalence Trials and Non-Inferiority Trials.” Journal of\nHepatology 46 (5). Elsevier: 947–54.\n\n\nClark, Taane G, Michael J Bradburn, Sharon B Love, and Douglas G Altman.\n2003. “Survival Analysis Part i: Basic Concepts and First\nAnalyses.” British Journal of Cancer 89 (2). Nature\nPublishing Group: 232–38.\n\n\nCohen, Jacob. 1960. “A Coefficient of Agreement for Nominal\nScales.” Educational and Psychological Measurement 20\n(1). Sage Publications: 37–46.\n\n\n———. 1988b. Statistical Power Analysis for the Behavioral\nSciences. Lawrence Erlbaum Associates.\n\n\n———. 1988a. Statistical Power Analysis for the Behavioral\nSciences. 2nd Edition. Routledge.\n\n\nComtois, Dominic. 2022. “Summarytools: Tools to Quickly and Neatly\nSummarize Data.” CRAN R Project.\nhttps://github.com/dcomtois/summarytools.\n\n\nCook, R. D., and S. Weisberg. 1986. Residuals and Influence in\nRegression. Monographs on Statistics and Applied Probability.\nChapman; Hall. https://books.google.com.br/books?id=aMDpswEACAAJ.\n\n\nCoutinho, Mario. 1998. “Principios de Epidemiologia Clínica\nAplicada a Cardiologia.” Arquivos Brasileiros de\nCardiologia 71. SciELO Brasil: 109–16.\n\n\nCox, David R. 1958. “The Regression Analysis of Binary\nSequences.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 20 (2). Oxford University Press: 215–32.\n\n\nDag, Osman, Anil Dolgun, and Naime Meric Konar. 2018.\n“Onewaytests: An r Package for One-Way Tests in Independent Groups\nDesigns.” R Journal 10 (1): 175–99.\n\n\nDamasio, Bruno. 2021. “Teste Post Hoc: O Que é e Qual\nUtilizar?” Blog Psicometria Online. https://www.blog.psicometriaonline.com.br/o-que-e-um-teste-post-hoc/.\n\n\nDamiani, Athos, Beatriz Milz, Caio Lente, and et al. 2015.\n“Ciência de Dados Em r.” R6 Consultoria. https://livro.curso-r.com/index.html.\n\n\nDamiani, Athos, Beatriz Milz, Caio Lente, and Outros. 2022. “O\nPacote Forcats.” Ciência de Dados Em R. R6 Consultoria.\nhttps://livro.curso-r.com/7-6-forcats.html#o-que-s%C3%A3o-fatores.\n\n\nDaniel, Wayne W., and Chad L. Cross. 2013a. “Grouped Data: The\nFrequency Distribuition.” In Biostatistics: A Foundation for\nAnalysis in the Health Sciences, Tenth Edition, 22--23. Wiley.\n\n\n———. 2013b. “The Chi-Square Distribution and Analysis of\nFrequencies.” In Practical Statistics for Medical\nResearch, 604–19. Hoboken, NJ: John Wiley & Sons, Inc.\n\n\nDavies, Huw Talfryn Oakley, Iain Kinloch Crombie, and Manouche Tavakoli.\n1998. “When Can Odds Ratios Mislead?” BMJ 316\n(7136). British Medical Journal Publishing Group: 989–91.\n\n\nDe Winter, Joost CF, Samuel D Gosling, and Jeff Potter. 2016.\n“Comparing the Pearson and Spearman Correlation Coefficients\nAcross Distributions and Sample Sizes: A Tutorial Using Simulations and\nEmpirical Data.” Psychological Methods 21 (3). American\nPsychological Association: 273.\n\n\nDebnath, Lokenath, and Kanadpriya Basu. 2015. “A Short History of\nProbability Theory and Its Applications.” International\nJournal of Mathematical Education in Science and Technology 46 (1).\nTaylor & Francis: 13–39.\n\n\nDeeks, Jonathan J, and Douglas G Altman. 2004. “Diagnostic Tests\n4: Likelihood Ratios.” Bmj 329 (7458). British Medical\nJournal Publishing Group: 168–69.\n\n\nDeLong, Elizabeth R, David M DeLong, and Daniel L Clarke-Pearson. 1988.\n“Comparing the Areas Under Two or More Correlated Receiver\nOperating Characteristic Curves: A Nonparametric Approach.”\nBiometrics 44. JSTOR: 837–45.\n\n\nDunn, Olive Jean. 1964. “Multiple Comparisons Using Rank\nSums.” Technometrics 6 (3). Taylor & Francis:\n241–52.\n\n\nDurbin, James, and Geoffrey S Watson. 1950. “Testing for Serial\nCorrelation in Least Squares Regression: i.” Biometrika\n37 (3/4). JSTOR: 409–28.\n\n\nEditors, History.com. n.d. “Florence\nNightingale.”\nhttps://www.history.com/topics/womens-history/florence-nightingale-1.\n\n\nElander, Gunnel, and Göran Hermerén. 1995. “Placebo Effect and\nRandomized Clinical Trials.” Theoretical Medicine 16\n(2). Springer: 171–82.\n\n\nEliasziw, Michael, and Allan Donner. 1991. “Application of the\nMcNemar Test to Non-Independent Matched Pair Data.”\nStatistics in Medicine 10 (12). Wiley Online Library: 1981–91.\n\n\nErnster, Virginia L. 1994. “Nested Case-Control Studies.”\nPreventive Medicine 23 (5). Elsevier: 587–90.\n\n\nFagan, TJ. 1975. “Nomogram for Bayes’s Theorem.” New\nEngland Journal of Medicine 293: 257.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007.\n“G* Power 3: A Flexible Statistical Power Analysis Program for the\nSocial, Behavioral, and Biomedical Sciences.” Behavior\nResearch Methods 39 (2). Springer: 175–91.\n\n\nFerreira, Daniel F, and Billy P Helms. 2011. “Aproximação Normal\nDa Distribuição f.” Rev. Bras. Biom. 29 (2): 222–28.\n\n\nFeychting, Maria, Bill Osterlund, and Anders Ahlbom. 1998.\n“Reduced Cancer Incidence Among the Blind.”\nEpidemiology 9 (5). LWW: 490–94.\n\n\nField, Andy, Jeremy Miles, and Zoë Field. 2012a. “Comparing\nSeveral Means: ANOVA (GML 1).” In Discovering Statistics\nUsing r, 399–400. Sage Publications, Ltd.\n\n\n———. 2012b. “Everithing You Ever Wanted to Know about Statistics\n(Well, Sort Of).” In Discovering Statistics Using r, 38.\nSage Publications, Ltd.\n\n\n———. 2012c. “Exploring Data with Graphs.” In\nDiscovering Statistics Using r, 117. Sage Publications, Ltd.\n\n\n———. 2012d. “Influential Cases.” In Discovering\nStatistics Using r, 269–71. Sage Publications, Ltd.\n\n\n———. 2012e. “Logistic Regression.” In Discovering\nStatistics Using r, 320. Sage Publications, Ltd.\n\n\n———. 2012f. “Regression.” In Discovering Statistics\nUsing r, 266–76. Sage Publications, Ltd.\n\n\nFisher, Lloyd D., and Gerald Van Belle. 1993. “Poisson Random\nVariables.” In Biostatistics: A Methodology for the Health\nSciences, 211–18. New York, NY: John Wiley & Sons.\n\n\nFletcher, Robert H, Suzanne W Fletcher, and Grant S Fletcher. 2014a.\n“Acaso.” In Epidemiologia Clínica: Elementos\nEssenciais, Quinta Edição, 108–9. Artmed Editora.\n\n\n———. 2014b. “Prognóstico.” In Epidemiologia Clínica:\nElementos Essenciais, 108–9. Artmed Editora.\n\n\n———. 2014c. “Risco: Da Doença à Exposição.” In\nEpidemiologia Clínica: Elementos Essenciais, 88. Artmed\nEditora.\n\n\n———. 2014d. “Tratamento.” In Epidemiologia Clínica:\nElementos Essenciais, 143. Artmed Editora.\n\n\nFox, John, and Sanford Weisberg. 2019. An r Companion to Applied\nRegression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nFrança, A. 2023. “Regressão Logística: Pseudo R².” https://www.blog.psicometriaonline.com.br/regressao-logistica-pseudo-r2/.\n\n\nGeeksforGeeks. 2025. “Case When Statement in r Dplyr Package Using\nCase_when() Function.” GeeksforGeeks. GeeksforGeeks. https://www.geeksforgeeks.org/r-language/case-when-statement-in-r-dplyr-package-using-case_when-function/.\n\n\nGeorge, Darren, and Paul Mallery. 2020. “Descriptive\nStatistics.” In IBM SPSS Statistics 26 Step by Step: A Simple\nGuide and Reference, 114–20. New York, NY: Taylor & Francis\nGroup.\n\n\nGhasemi, Asghar, and Saleh Zahediasl. 2012. “Normality Tests for\nStatistical Analysis: A Guide for Non-Statisticians.”\nInternational Journal of Endocrinology and Metabolism 10 (2).\nBrieflands: 486.\n\n\nGirden, Ellen R. 1992. “Two-Factor Studies with Repeated Measures\non Both Factors.” In ANOVA: Repeated Measures, 84:31–40.\nQASS Series. Sage.\n\n\nGohel, David, and Panagiotis Skintzos. 2025. Flextable: Functions\nfor Tabular Reporting. doi:10.32614/CRAN.package.flextable.\n\n\nGonzalez, José Carlos Soage. 2021. “Normal Distribution in\nr.” R CODER. https://r-coder.com/.\n\n\nGreene Jr, John W, and Joseph C Touchstone. 1963. “Urinary Estriol\nas an Index of Placental Function. A Study of 279 Cases.”\nObstetrical & Gynecological Survey 18 (3). LWW: 356–59.\n\n\nGrimes, David A, and Kenneth F Schulz. 2002a. “An Overview of\nClinical Research: The Lay of the Land.” The Lancet 359\n(9300). Elsevier: 57–61.\n\n\n———. 2002b. “Cohort Studies: Marching Towards Outcomes.”\nThe Lancet 359 (9303). Elsevier: 341–45.\n\n\n———. 2002c. “Descriptive Studies: What They Can and Cannot\nDo.” The Lancet 359 (9301). Elsevier: 145–49.\n\n\n———. 2005. “Compared to What? Finding Controls for Case-Control\nStudies.” The Lancet 365 (9468). Elsevier: 1429–33.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with Lubridate.” Journal of Statistical\nSoftware 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nGross, Michael. 1976. “Oswego County Revisited.” Public\nHealth Reports 91 (2). SAGE Publications: 168.\n\n\nGuyatt, Gordon, Roman Jaeschke, Nancy Heddle, et al. 1995. “Basic\nStatistics for Clinicians: 1. Hypothesis Testing.” CMAJ:\nCanadian Medical Association Journal 152 (1). Canadian Medical\nAssociation: 27.\n\n\nGuyatt, Gordon, Drummond Rennie, et al. 2015. “Diagnostic\nTests.” In User’s Guides to Medical Literature: A Manual for\nEvidence-Based Clinical Practice, 3rd Edition, 607–31. New York:\nJAMA.\n\n\nHald, Anders. 2007. “Biography of Fisher.” In A History\nof Parametric Statistics Inference from Bernoulli to\nFisher,1713-1935, 159–63. John Wiley & Sons.\n\n\nHalkin, A, J Reichman, M Schwaber, O Paltiel, and M Brezis. 1998.\n“Likelihood Ratios: Getting Diagnostic Testing into\nPerspective.” QJM: Monthly Journal of the Association of\nPhysicians 91 (4): 247–58.\n\n\nHalunga, Andreea G., Chris D. Orme, and Takashi Yamagata. 2017. “A\nHeteroskedasticity Robust Breusch–Pagan Test for Contemporaneous\nCorrelation in Dynamic Panel Data Models.” Journal of\nEconometrics 198 (2): 209–30. doi:https://doi.org/10.1016/j.jeconom.2016.12.005.\n\n\nHealth Improvement, Office for, and Disparities. 2020. “Crossover\nRandomised Controlled Trial: Comparative Studies.” Office for\nHealth Improvement and Disparities. UK Health improvement. https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies.\n\n\nHennekens, Charles H, Julie E Buring, et al. 1996. “Lack of Effect\nof Long-Term Supplementation with Beta Carotene on the Incidence of\nMalignant Neoplasms and Cardiovascular Disease.” New England\nJournal of Medicine 334 (18). Mass Medical Soc: 1145–49.\n\n\nHoaglin, David C., and Roy E. Welsch. 1978. “The Hat Matrix in\nRegression and ANOVA.” The American Statistician 32:\n17–22. https://api.semanticscholar.org/CorpusID:120982185.\n\n\nHoltz, Yan. 2025. “R Color Brewer’s Palettes.” – the R\nGraph Gallery. https://r-graph-gallery.com/38-rcolorbrewers-palettes.html.\n\n\nHope, Ryan M. 2022. Rmisc: Ryan Miscellaneous. doi:10.32614/CRAN.package.Rmisc.\n\n\nHothorn, Torsten, Kurt Hornik, Mark A Van De Wiel, and Achim Zeileis.\n2006. “A Lego System for Conditional Inference.” The\nAmerican Statistician 60 (3). Taylor & Francis: 257–63.\n\n\nHulley, Stephen B, Steven R Cummings, Warren S Browner, Deborah G Grady,\nand Thomas B Newman. 2015. “Elaborando a Questão de Pesquisa e\nDesenvolvendo o Plano de Estudo.” In Delineando a Pesquisa\nClinica, Quarta Edição, 15–24. Artmed Editora.\n\n\nHuynh, Huynh, and Leonard S Feldt. 1976. “Estimation of the Box\nCorrection for Degrees of Freedom from Sample Data in Randomized Block\nand Split-Plot Designs.” Journal of Educational\nStatistics 1 (1). Sage Publications: 69–82.\n\n\nHvitfeldt, Emil. 2024. “Use Any Color Palette with\nPaletteer.” The R Graph Gallery. https://r-graph-gallery.com/package/paletteer.html.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, et al. 2020. “Gt:\nEasily Create Presentation-Ready Display Tables.” The R\nFoundation. https://CRAN.R-project.org/package=gt.\n\n\nJain, Sandeep. 2022. “A Guide to Dnorm, Pnorm, Rnorm, and Qnorm in\nr.” GeeksforGeeks. https://www.geeksforgeeks.org/.\n\n\nJoanes, DN, and CA Gill. 1998. “Comparing Measures of Sample\nSkewness and Kurtosis.” Journal of the Royal Statistical\nSociety 47 (1): 183–89.\n\n\nKabisch, Maria, Christian Ruckes, Monika Seibert-Grafe, and Maria\nBlettner. 2011. “Randomized Controlled Trials: Part 17 of a Series\non Evaluation of Scientific Publications.” Deutsches\nÄrzteblatt International 108 (39). Deutscher\nArzte-Verlag GmbH: 663.\n\n\nKanji, Gopal K. 2006. “The Kruskal–Wallis Test.” In 100\nStatiscal Tests, 3rd Edition, 220. London: Sage publications.\n\n\nKannel, William B, and Daniel L McGee. 1979. “Diabetes and\nCardiovascular Risk Factors: The Framingham Study.”\nCirculation 59 (1). Am Heart Assoc: 8–13.\n\n\nKaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation\nfrom Incomplete Observations.” Journal of the American\nStatistical Association 53 (282). Taylor & Francis: 457–81.\n\n\nKaradimitriou, Sofia Maria, and Ellen Marshall. 2020.\n“Kruskal-Wallis in r.” Statistics Support for\nStudents. Loughborough; Coventry Universities. https://www.statstutor.ac.uk/.\n\n\nKassambara, Alboukadel. 2021. “Correlation Test Between Two\nVariables in r.” STHDA - Statistical Tools for\nHigh-Throughput Data Analysis. http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r.\n\n\n———. 2022a. Rstatix: Pipe-Friendly Framework for Basic Statistical\nTests. https://CRAN.R-project.org/package=rstatix.\n\n\n———. 2022b. “Ggpubr:’ggplot2’ Based Publication Ready Plots [r\nPackage Ggpubr Version 0.5.0].” The Comprehensive R Archive\nNetwork. Comprehensive R Archive Network (CRAN). https://cloud.r-project.org/web/packages/ggpubr/index.html.\n\n\nKassambara, Alboukadel, Marcin Kosinski, Przemyslaw Biecek, and Scheipl\nFabian. 2021. “Survminer: Drawing Survival Curves Using\nGgplot2.” URL Https://CRAN. R-Project. Org/Package=\nSurvminer. R Package Version 0.4 9.\n\n\nKelen, Gabor D., Charles B. Brown, and James Ashton. 1988.\n“Statistical Reasoning in Clinical Trials: Hypothesis\nTesting.” Am J Emerg Med 1 (1). W.B. Saunders Company:\n52–61.\n\n\nKendall, Maurice George. 1960. “Studies in the History of\nProbability and Statistics. Where Shall the History of Statistics\nBegin?” Biometrika 47 (3/4). JSTOR: 447–49.\n\n\nKim, Hae-Young. 2019. “Statistical Notes for Clinical Researchers:\nSimple Linear Regression 3–Residual Analysis.” Restorative\nDentistry & Endodontics 44 (1). Korean Academy of Conservative\nDentistry.\n\n\nKirkwood, Betty R, and Jonathan AC Sterne. 2003. “Defining the\nData.” In Essential Medical Statistics, Second Edition,\n9–14. Blackwell Science Company.\n\n\nKohl, Matthias. 2019. “Package ‘MKmisc’.” https://github.com/stamats/MKmisc.\n\n\nKruskal, William. 1980. “The Significance of Fisher: A Review of\nr.a. Fisher: The Life of a Scientist.” Journal of the\nAmerican Statistical Association 75 (372). Taylor & Francis:\n1019–30. doi:10.1080/01621459.1980.10477590.\n\n\nLindenau, Juliana D, and Luciano Santos Pinto Guimaraes. 2012.\n“Calculating the Effect Size in SPSS.” Revista\nHCPA 32 (3): 363–81. https://seer.ufrgs.br/hcpa.\n\n\nMadi, José Mauro, Ricardo da Silva de Souza, Breno Fauth de Araujo,\nPetrônio Fagundes Oliveira Filho, et al. 2010. “Prevalence of\nToxoplasmosis, HIV, Syphilis and Rubella in a Population of Puerperal\nWomen Using Whatman 903 Filter Paper.” The\nBrazilian Journal of Infectious Diseases 14 (1). Elsevier: 24–29.\n\n\nMark, Evan Sergeant, et al. 2022. epiR: Tools for the Analysis of\nEpidemiological Data. https://CRAN.R-project.org/package=epiR.\n\n\nMassad, Eduardo, Paulo Sérgio Panse Silveira, Renèe Xavier de Menezes,\nand Neli Regina Siqueira Ortega. 2004. Métodos Quantitativos Em\nMedicina. São Paulo, SP: Editora Manole.\n\n\nMatsubara, Lídia, and Luis F. C. Araujo. 2019. “Cap. 13 ANOVA de\nMedidas Repetidas.” https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html;\nBookdown. https://bookdown.org/luisfca/docs/anova-de-medidas-repetidas.html.\n\n\nMatthews, Robert, Iain Chalmers, and Peter Rothwell. 2018.\n“Douglas g Altman: Statistician, Researcher, and Driving Force\nBehind Global Initiatives to Improve the Reliability of Health\nResearch.” British Medical Journal Publishing Group.\n\n\nMcCambridge, Jim, John Witton, and Diana R Elbourne. 2014.\n“Systematic Review of the Hawthorne Effect: New Concepts Are\nNeeded to Study Research Participation Effects.” Journal of\nClinical Epidemiology 67 (3). Elsevier: 267–77.\n\n\nMcCombes, Shona. 2019. “Sampling Methods.”\nHttps://Www.scribbr.com/Methodology/Sampling-Methods/.\nscribbr.com Team. https://www.scribbr.com/.\n\n\nMenezes, Renée Xavier de. 2004a. “Análise de Variância.” In\nMétodos Quantitativos Em Medicina, edited by Eduardo Massad,\nRenée Xavier de Menezes, Paulo Sérgio Panse Silveira, and Neli Regina\nSiqueira Ortega, 297–300. Barueri, São Paulo: Editora Manole Ltda.\n\n\n———. 2004b. “Introdução à Probabilidade.” In Métodos\nQuantitativos Em Medicina, edited by Eduardo Massad, Renée Xavier\nde Menezes, Paulo Sérgio Panse Silveira, and Neli Regina Siqueira\nOrtega, 151–87. Barueri, São Paulo: Editora Manole Ltda.\n\n\nMenezes, Renée Xavier de, and Marcelo Nascimento Burattini. 2004a.\n“Testes de Hipótese e Intervalos de Confiança.” In\nMétodos Quantitativos Em Medicina, edited by Eduardo Massad,\nRenée Xavier de Menezes, Paulo Sérgio Panse Silveira, and Neli Regina\nSiqueira Ortega, 225–41. Barueri, São Paulo: Editora Manole Ltda.\n\n\n———. 2004b. “Testes de Hipótese e Intervalos de Confiança.”\nIn Métodos Quantitativos Em Medicina, edited by Eduardo Massad,\nRenée Xavier de Menezes, Paulo Sérgio Panse Silveira, and Neli Regina\nSiqueira Ortega, 225–41. Barueri, São Paulo: Editora Manole Ltda.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel,\nFriedrich Leisch, Chih-Chung Chang, Chih-Chen Lin, and Maintainer David\nMeyer. 2019. “Package ‘E1071’.” The R\nJournal, 1–67.\n\n\nMicrosoft Copilot. 2025. “Assistente de Inteligência Artificial\nUtilizado Para Apoio Estatístico e Redação Técnica.” https://copilot.microsoft.com.\n\n\nMontori, Victor M, and Gordon H Guyatt. 2001. “Intention-to-Treat\nPrinciple.” CMAJ 165 (10). Can Med Assoc: 1339–41.\n\n\nMoore, David S. 2000. “Topics in Inferency.” In The\nBasic Practice of Statistics, 417. W.H. Freeman.\n\n\nMotulsky, Harvey. 2010. “The Theory of Confidence\nIntervals.” In Intuitive Biostatistics: A Nonmathematical\nGuide to Statistical Thinking, Second Edition, 96–102. New York,\nNY: Oxford University Press.\n\n\nMuller, Keith E, and Curtis N Barton. 1989. “Approximate Power for\nRepeated-Measures ANOVA Lacking Sphericity.” Journal of the\nAmerican Statistical Association 84 (406). Taylor & Francis:\n549–55.\n\n\nMyszkowski, Nils. 2020. “Nhstplot Package.”\nRDocumentation. https://rdocumentation.org/packages/nhstplot/versions/1.1.0.\n\n\nNavarro, Danielle. 2024. “Use of Flextable.” Notes from\na Data Witch. https://blog.djnavarro.net/posts/2024-07-04_flextable/.\n\n\nNewman, Thomas B, Warren S Browner, Steven R Cummings, and Stephen B\nHulley. 2015. “Delineando Estudos de Caso-Controle.” In\nDelineando a Pesquisa Clinica, Quarta Edição, 111. Artmed\nEditora.\n\n\nOliveira Filho, Petronio Fagundes de. 2022a. “Natureza Dos\nDados.” In Epidemiologia e Bioestatística–Fundamentos Para a\nLeitura Crítica, 2ª edição, 3–6. Editora Rubio.\n\n\n———. 2022b. “População e Amostra.” In Epidemiologia e\nBioestatística: Fundamentos Para a Leitura Crítica, Segunda Edição,\n109–12. Rio de Janeiro: Editora Rubio.\n\n\n———. 2022c. “Testes Diagnósticos.” In Epidemiologia e\nBioestatística: Fundamentos Para a Leitura Crítica, Segunda Edição,\n89–105. Rio de Janeiro: Editora Rubio.\n\n\nOoms, Jeroen. 2022. Writexl: Export Data Frames to Excel ’Xlsx’\nFormat. https://CRAN.R-project.org/package=writexl.\n\n\nPagano, Marcello, and Kimberlee Gavreau. 2000. “The Central Limit\nTheorem.” In Principles of Biostatistics, Second\nEdition, 197–98. Pacific Grove, CA: Duxbury.\n\n\nPagano, Marcello, and Gauvreau Kimberly. 2000a. “Comparison of Two\nMeans.” In Principles of Biostatistics, Second Edition,\n262–72. CRC Press.\n\n\n———. 2000b. “Theoretical Probability Distributions.” In\nPrinciples of Biostatistics, Second Edition, 162. CRC Press.\n\n\nPeat, Jennifer, and Belinda Barton. 2014a. “Continuous Variables:\nAnalysis of Variance.” In Medical Statistics : A Guide to\nSPSS, Data Analysis, and Critical Appraisal, 114. New York, NY:\nJohn Wiley & Sons.\n\n\n———. 2014b. “Descriptive Statistics.” In Medical\nStatistics : A Guide to SPSS, Data Analysis, and Critical\nAppraisal, 24–51. New York, NY: John Wiley & Sons.\n\n\n———. 2014c. “Survival Analyses.” In Medical Statistics\n: A Guide to SPSS, Data Analysis, and Critical Appraisal, 352–53.\nNew York, NY: John Wiley & Sons.\n\n\nPediatrics, American Academy of, and American College of Obstetricians.\n2006. “The Apgar Score.” Pediatrics 117 (4).\nAmerican Academy of Pediatrics: 1444–47.\n\n\nPeixoto, Rodrigo de Oliveira, Tarcizo Afonso Nunes, and Carlos Augusto\nGomes. 2011. “Indices Diagnósticos Da Ultrassonografia Abdominal\nNa Apendicite Aguda: Influência Do Genero e Constituição Física, Tempo\nEvolutivo Da Doença e Experiencia Do Radiologista.” Revista\nDo Colégio Brasileiro de Cirurgiões 38. SciELO Brasil: 105–11.\n\n\nPereira Lima, Amanda, Felipe José Vieira, Gabriela P.de Moraes Oliveira,\net al. 2016. “Perfil Clinico-Epidemiologico Da Apendicite Aguda:\nAnalise Retrospectiva de 638 Casos.” Revista Do Colegio\nBrasileiro de Cirurgiões 43. SciELO Brasil: 248–53.\n\n\nPhysicians’ Health Study Research Group*, Steering Committee of the.\n1989. “Final Report on the Aspirin Component of the Ongoing\nPhysicians’ Health Study.” New England Journal of\nMedicine 321 (3). Mass Medical Soc: 129–35.\n\n\nPinheiro, José C., and Douglas M. Bates. 2000. “Fitting Linear\nMixed-Effects Models.” In Mixed-Effects Models in s and\ns-PLUS, 133–99. New York, NY: Springer New York.\n\n\nPrabhakaran, Selva. 2016. “Missing Value Treatment.”\nDatascienceplus. https://datascienceplus.com/missing-value-treatment/.\n\n\nQiu, W, J Chavarro, R Lazarus, B Rosner, and J Ma. 2015.\n“powerSurvEpi: Power and Sample Size Calculation for Survival\nAnalysis of Epidemiological Studies.” R Package Version\n0.0 9.\n\n\nR Core Team. 2022a. “The r Project for Statistical Computing |\nCRAN Mirrors.” Disponível em:\nhttps://cran.r-project.org/mirrors.html.\n\n\n———. 2022b. “The r Project for Statistical Computing | What Is\nr?” Disponível em: https://www.r-project.org/about.html.\n\n\nRazali, Nornadiah Mohd, Yap Bee Wah, et al. 2011. “Power\nComparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and\nAnderson-Darling Tests.” Journal of Statistical Modeling and\nAnalytics 2 (1): 21–33.\n\n\nRibeiro Mendes, Fabio. 2012. “O Que é Um Trabalho\nCientífico.” In Iniciacão Cientifica, 17–26. Autonomia\nEditora.\n\n\nRinker, Tyler W., and Dason Kurkiewicz. 2018. Pacman: Package\nManagement for r. Buffalo, New York. http://github.com/trinker/pacman.\n\n\nRobertson, Edmund, and John O’Connor. 2022. “Jacob (Jacques)\nBernoulli.” Maths History. School of Mathematics;\nStatistics, University of St Andrews. https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/.\n\n\nRobin, Xavier, Natacha Turck, Alexandre Hainard, et al. 2011.\n“pROC: An Open-Source Package for r and s+ to Analyze and Compare\nROC Curves.” BMC Bioinformatics 12 (1): 1–8.\n\n\nRosner, Bernard. 2011. “Hypothesis Testing: Categorical\nData.” In Fundamentals of Biostatistics, Seventh\nEdition, 377. Boston: Cengage.\n\n\nSalgado-Neto, Geraldo, and Aquilea Salgado. 2011. “Sir Francis\nGalton e Os Extremos Superiores Da Curva Normal.” Revista de\nCiências Humanas 45 (1): 223–39.\n\n\nSalsburg, David. 2009. “Uma Senhora Toma Chá...” In Uma\nSenhora Toma Chá..., 17–23. Zahar.\n\n\nSchirmer, Janine, and Outros. 2000. “Fatores de Risco\nReprodutivo.” In Assistência Pré-Natal: Manual Técnico,\n3a Edição, 25–26. Ministério da Saúde. https://bvsms.saude.gov.br/bvs/publicacoes/cd04_11.pdf.\n\n\nSchober, Patrick, Christa Boer, and Lothar A Schwarte. 2018.\n“Correlation Coefficients: Appropriate Use and\nInterpretation.” Anesthesia & Analgesia 126 (5).\nWolters Kluwer: 1763–68.\n\n\nSchuh, Claudia Maria. 2008. “Efeitos Da\nExposição Ao Fumo Durante a\nGestação Nas Medidas\nAntropométricas Dos Recém-Nascidos.”\nMaster’s thesis, Universidade Federal do Rio Grande do Sul.\n\n\nSchulz, Kenneth F, and David A Grimes. 2002. “Blinding in\nRandomised Trials: Hiding Who Got What.” The Lancet 359\n(9307). Elsevier: 696–700.\n\n\nSedgwick, Philip. 2013. “Correlation Versus Linear\nRegression.” BMJ 346. British Medical Journal Publishing\nGroup.\n\n\nSignorell, Andri et al. 2022. DescTools: Tools for Descriptive\nStatistics. https://cran.r-project.org/package=DescTools.\n\n\nSingmann, Henrik, Ben Bolker, Jake Westfall, Frederik Aust, and Mattan\nS. Ben-Shachar. 2025. Afex: Analysis of Factorial Experiments.\ndoi:10.32614/CRAN.package.afex.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery,\nand Joseph Larmarange. 2021. “Reproducible Summary Tables with the\nGtsummary Package.” The R Journal 13:\n570–80. doi:10.32614/RJ-2021-053.\n\n\nSoftware, Posit. 2025. “Frequently Asked Questions.”\nPositron. Posit Software, PBC. https://positron.posit.co/faqs.html.\n\n\nStanley, Kenneth. 2007. “Design of Randomized Controlled\nTrials.” Circulation 115 (9). Am Heart Assoc: 1164–69.\n\n\nSternbach, George L. 2000. “The Glasgow Coma Scale.”\nThe Journal of Emergency Medicine 19 (1). Elsevier: 67–71.\n\n\nStolley, Paul D, and Tamar Lasky. 2000a. “Lung Cancer: New Methods\nof Studying Disease.” In Investigating Disease Patterns,\n51–79. Scientific American Library.\n\n\n———. 2000b. “The Beginnings of Epidemiology.” In\nInvestigating Disease Patterns, 23–49. Scientific American\nLibrary.\n\n\nStraus, Sharon E, Paul Glasziou, et al. 2019. “Diagnosis and\nScreening.” In Evidence-Based Medicine: How to Practice and\nTeach EBM, Fifth Edition, 185–218. Edinburgh: Elsevier.\n\n\nSubramanian, Jyothi, and Richard Simon. 2013. “Overfitting in\nPrediction Models–Is It a Problem Only in High Dimensions?”\nContemporary Clinical Trials 36 (2). Elsevier: 636–41.\n\n\nSzklo, Moyses, and F Javier Nieto. 2019b. “Measuring Associations\nBetween Exposures and Outcomes.” In Epidemiology: Beyond the\nBasics, Fourth Edition, 88–94. Burlington, MA: Jones & Bartlett\nLearning.\n\n\n———. 2019a. “Measuring Associations Between Exposures and\nOutcomes.” In Epidemiology: Beyond the Basics, Fourth\nEdition, 84–102. Burlington, MA: Jones & Bartlett Learning.\n\n\n———. 2019c. “Measuring Associations Between Exposures and\nOutcomes.” In Epidemiology: Beyond the Basics, Fourth\nEdition, 97–98. Burlington, MA: Jones & Bartlett Learning.\n\n\n———. 2019d. “Measuring Disease Occurrence.” In\nEpidemiology: Beyond the Basics, Fourth Edition, 80–81.\nBurlington, MA: Jones & Bartlett Learning.\n\n\nTeam, Easystats. 2025. “R2 Tjur — Performance Package\nReference.” https://easystats.github.io/performance/reference/r2_tjur.html.\n\n\nTeam, R Core. 2022. “Write.table: Data Output/CSV Files.”\nDataCamp. https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table.\n\n\nTherneau, Terry et al. 2015. “A Package for Survival Analysis in\nr.” R Package Version 2 (7).\n\n\nTherneau, Terry M. 2024. A Package for Survival Analysis in r.\nhttps://CRAN.R-project.org/package=survival.\n\n\nTjur, Tue. 2009. “Coefficients of Determination in Logistic\nRegression Models—a New Proposal: The Coefficient of\nDiscrimination.” The American Statistician 63 (4).\nTaylor & Francis: 366–72.\n\n\nTomczak, Maciej, and Ewa Tomczak. 2014. “The Need to Report Effect\nSize Estimates Revisited. An Overview of Some Recommended Measures of\nEffect Size.” Trends in Sport Sciences 1 (21): 19–25.\n\n\nVerzani, John. 2004. Using r for Introductory Statistics.\nChapman; Hall/CRC.\n\n\nViana, Kelly de Jesus, José Augusto de Aguiar Carrazedo Taddei, Monize\nCocetti, and Sarah Warkentin. 2013. “Birth Weight in Brazilian\nChildren Under Two Years of Age.” Cadernos de\nSaúde Pública 29. SciELO Brasil: 349–56.\n\n\nWarnes, Gregory R., Ben Bolker, et al. 2022. “Gmodels: Various r\nProgramming Tools for Model Fitting.” CRAN R Project. https://rdrr.io/cran/gmodels/.\n\n\nWatson, Peter. 2021. “Rules of Thumb on Magnitudes of Effect\nSizes.” MRC Cognition and Brain Sciences Unit. Cambridge\nUniversity. https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize.\n\n\nWhitney, Lance et al. 2020. “R Programming Language Continues to\nGrow in Popularity.” TechRepublic. https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1).\nTaylor & Francis: 3–28.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (10): 11–23.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, et al.\n2019. “Welcome to the Tidyverse.” Journal of Open\nSource Software 4 (43): 1686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et al.\n2015. “Dplyr: A Grammar of Data Manipulation.” R\nPackage Version 0.4 3: 156.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. Tidyr: Tidy Messy\nData. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. “15 Factors|r for\nData Science.” Welcome | R for Data Science. O’Reilly.\nhttps://r4ds.had.co.nz/factors.html.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nGgplot2: Elegant Graphics for Data Analysis (3e). http://www.new.pmean.com/ggplot2-book/.\n\n\nWikipédia. 2025. “RMS Titanic — Wikipédia, a\nEnciclopédia Livre.” https://pt.wikipedia.org/w/index.php?title=RMS_Titanic&oldid=69869298.\n\n\nYap, Bee Wah, and Chiaw Hock Sim. 2011. “Comparisons of Various\nTypes of Normality Tests.” Journal of Statistical Computation\nand Simulation 81 (12). Taylor & Francis: 2141–55.\n\n\nYouden, William J. 1950. “Index for Rating Diagnostic\nTests.” Cancer 3 (1). Wiley Online Library: 32–35.\n\n\nZar, Jerrold H. 2014a. “Nonparametric Analysis of\nVariance.” In Biostatistical Analysis, 226–30.\nEdinburgh: Pearson.\n\n\n———. 2014b. “Paired-Sample Hypotheses.” In\nBiostatistical Analysis, 189–98. Edinburgh: Pearson.\n\n\n———. 2014c. “Populations and Samples.” In\nBiostatistical Analysis, 18–22. Edinburgh: Pearson.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007.\n“Residual-Based Shadings for Visualizing (Conditional)\nIndependence.” Journal of Computational and Graphical\nStatistics 16 (3). Taylor & Francis: 507–25.\n\n\nZimmerman, Donald W. 2004. “A Note on Preliminary Tests of\nEquality of Variances.” Br J Math Stat Psychol 57 (1).\nWiley Online Library: 173–81.\n\n\nZuur, Alain F, Elena N Ieno, and Erik HWG Meesters. 2009. “Getting\nData into r.” In A Beginner’s Guide to r, 29–56.\nSpringer.",
    "crumbs": [
      "Referências"
    ]
  }
]